\documentclass{article}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{relsize}

\setlength{\parindent}{0pt}

\def\noval{}

\DeclarePairedDelimiter{\br}{(}{)}
\DeclarePairedDelimiter{\brr}{[}{]}
\DeclarePairedDelimiter{\brs}{\{}{\}}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\makeatletter
\let\oldbr\br
\def\br{\@ifstar{\oldbr}{\oldbr*}}
\makeatother

\makeatletter
\let\oldbrr\brr
\def\brr{\@ifstar{\oldbrr}{\oldbrr*}}
\makeatother

\makeatletter
\let\oldbrs\brs
\def\brs{\@ifstar{\oldbrs}{\oldbrs*}}
\makeatother

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
\makeatother

\makeatletter
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\makeatletter
\let\oldlog\log
\def\log{\@ifstar\@log\@@log}
\def\@log#1{\oldlog\br{#1}}
\def\@@log#1{\oldlog#1}
\makeatother


\makeatletter
\newcommand{\set}[2]{
    \def\@delim{\;\@ifstar{\middle|}{:}\;}
    \ifx\noval#2\noval%
        \left\{#1\right\}
    \else
        \left\{#1\@delim#2\right\}
    \fi
}
\makeatother

\newcommand{\E}[2][]{%
    \ifx\noval#1\noval%
        \mathbb{E}\brr{#2}
    \else
        \mathbb{E}_{#1}\brr{#2}
    \fi
}
\newcommand{\var}[2][]{%
    \ifx\noval#1\noval%
        \text{Var}\br{#2}
    \else
        \text{Var}_{#1}\br{#2}
    \fi
}
\newcommand{\cov}[2][]{%
    \ifx\noval#1\noval%
        \text{Cov}\br{#2}
    \else
        \text{Cov}_{#1}\br{#2}
    \fi
}

\newcommand{\est}[1]{\hat{#1}_m}
\newcommand{\bias}[1]{\text{bias}\br{#1}}


\newcommand{\arrow}{$\rightarrow\;$}

\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\text{T}}
\renewcommand{\v}{\bm}
\renewcommand{\c}[2]{\left(#1\;\middle|\;#2\right)}
\renewcommand{\det}[1]{\text{det}\br{#1}}
\newcommand{\ml}[1]{\mathlarger{#1}\,}
\newcommand{\dkl}[2]{D_{\text{KL}}(#1\;\Vert\;#2)}
\renewcommand{\S}{\mathbb{S}}
\newcommand{\func}[3]{#1:#2\rightarrow#3}
\newcommand{\p}[1]{p_\text{#1}}
\renewcommand{\k}[2]{#1^{(#2)}}

\begin{document}
\section{Definitions}
\subsection{Definition of Machine Learning}
Broad definition of machine learning: \\
“A computer program is said to learn from experience E 
with respect to some class of tasks T and performance measure P, 
if its performance at tasks in T, as measured by P, improves with
experience E.”

Generally, we want to learn some function
\begin{flalign*}
    \func{f}{\R^n}{\R^m}, \, f(x) = y
\end{flalign*}
of high complexity

\subsection{Types of tasks}
Different classes of tasks T require different kinds of functions, \\
Common tasks are:
\begin{itemize}
    \item Classification of $k$ categories: produce function $\func{f}{\R^n}{R^k}$ \\
    Output: probability vector $\norm{\v{y}} = 1$ (can also be one-hot)\\
    Variation: missing inputs in $\v{x}$
    \item Regression: predict numerical value given some input: $\func{f}{\R^n}{R^m}$
    \item Structured output: output is vector with important relationships between elements e.g. output 
    token codes in language processing
    \item Probability density estimation: explicitly learn function $\func{\p{model}}{\R^n}{\R}$ on space examples were drawn from \\
    most above tasks already require implicit capture of structure/parameters of underlying probability distribution
\end{itemize}

\subsection{Types of learning}
Algorithm usually learns / gains experience E by observing examples \\
Tasks usually described in terms of how system should process an example \\
Types of learning can roughly be divided into:
\begin{itemize}
    \item Unsupervised: experience dataset of examples $x_i$, learning useful properties of dataset structure \\
    e.g. probability density $p(\v{x})$ or clustering dataset
    \item Supervised: experience dataset of labeled examples $\br{\v{x}_i, \v{y}_i}$, learn to estimate $p\c{\v{y}}{\v{x}}$
    \item Semi-supervised: some examples have label/target, some not
    \item Multi-instance: collection of examples labeled but not each example
    \item Reinforcement: interaction with environment, feedback loop between s
    ystem and experiences
\end{itemize}

\subsection{What is learned?}
%image: venn diagram
Form of representation of object that should be processed is very important \\
Pieces of information included in representation of object known as features \\
e.g. Image - Representation: Array of Pixels, Features: Individual pixel values \\

General machine learning: discover mapping from representation to output \\
Example: logistic regression: learn parameters $\beta_0, \beta_1$ such that
\begin{flalign*}
    f(x) = \frac{1}{1+\exp(-\beta_0 -\beta_1 x)}
\end{flalign*}
best fits given data\\
%https://en.wikipedia.org/wiki/Logistic_regression

Representation learning: also learn the "optimal" representation of data \\
Example of representation learning: Autoencoders \\
Encoder function: converts data to different representation \\
Decoder function: converts new representation back to original format \\
Task: preserve as much information, while providing additional nice properties e.g. reduced size in new representation \\

Deep learning tackles representation learning by introducing representations that are expressed in terms of other, simpler representations \\
e.g. Image: Pixels \arrow Edges \arrow Contours \arrow Object parts \arrow Object identity \\
Quintessential Example of Deep Learning: Multilayer Perceptron (\arrow later)
%image: representations for image MLP

\subsection{Performance Measure P}
Often difficult to find performance measure that corresponds well to desired behavior of 
system and is suitable for optimization
\begin{itemize}
    \item transcription: how do we value sentences that are not fully correct but close
    \item density estimation: can be impossible to explicitly compute probability for specific points in space
\end{itemize}

Generally we would like to have a function which we can optimize to improve
performance of algorithm on previously unseen data \arrow Loss / Objective / Cost / Error function (mostly synonymous), $J(\theta)$
with $\theta$ model parameters\\
\\

\subsection{Types of Error}
Difference between Machine Learning and simple optimization: we do not only want low error on examples (training error) but also on 
previously unseen data (generalization error) \\
Capacity: ability of model to fit function to be learned \\
Underfitting: model cannot obtain low enough training error, usually capacity too low \\
Overfitting: gap between training and generalization error too large, can be caused by too high capacity for task
\arrow more parameters than examples or "memorization" of training data \\
Best performance with capacity appropriate for true complexity of task and amount of training data \\
%image training, generalization error, capacity

To estimate generalization error, split examples in training set and test set, use only training set for learning \\
To be able to make estimations, have to make i.i.d. assumptions about data generating process:
\begin{itemize}
    \item individual examples independent from each other
    \item training set and test set identically distributed
\end{itemize}
\arrow data generation can be described by distribution over single example $\p{data}(\v{x})$ \\

\section{Machine Learning Basics}
\subsection{Statistics, Information Theory}
\subsubsection*{Empirical distribution}
Puts probability mass $\frac{1}{m}$ on each of the $m$ observed samples $\v{x}_i$
\begin{flalign*}
    \hat{p}(\v{x}) = \frac{1}{m} \sum_{i=1}^{m} \delta(\v{x} - \v{x}_i) &&
\end{flalign*}

\subsubsection*{Information content}
Desired properties of information measure:
\begin{itemize}
    \item Information content of event should depend on its likelihood (low/high likelihood $\leftrightarrow$ high/low information)
    \item Independent events should have additive information: e.g. $I(2\times \text{Heads}) \overset{!}{=} 2\cdot I(\text{Heads})$
\end{itemize}

Define self-information of event $X=x$ as:
\begin{flalign*}
    I(X=x) := -\log{p(X=x)} && 
\end{flalign*}

\subsubsection*{Shannon entropy}
Quantification of uncertainty in probability distribution:
\begin{flalign*}
    H(X) := \E[X \sim p]{I(x)} &&
\end{flalign*}
\arrow Expectation value of self-information

\subsubsection*{Kullback-Leibler (KL) divergence}
Measure of how different distributions $P(X)$ and $Q(X)$ over same variable $X$ are:
\begin{flalign*}
    \dkl{P}{Q} := \E[X \sim P]{\log{\frac{P(x)}{Q(x)}}} &&
\end{flalign*}
Properties:\\
$\dkl{P}{Q} \geq 0, \ \dkl{P}{Q} = 0 \iff P(X) = Q(X)$ \\ 
$\rightarrow$ concept of distance between distributions, however not symmetric so it makes difference which divergence is minimized \\
%image: difference between D_KL(P,Q) and D_KL(Q,P)

\subsubsection*{Cross-Entropy}
\begin{flalign*}
    H(P,Q) = -\E[X\sim p]{\log{Q(x)}} &&
\end{flalign*}
Minimizing cross-entropy with respect to $Q$ equivalent to minimizing KL divergence, since variation of $Q$ does not affect omitted term

\subsection{Maximum Likelihood Estimator}
Estimator: \\
Given i.i.d. data points $\mathbb{X} = \set{x_1, \ldots, x_m}{}$ drawn from data-generating distribution $\p{data}(x)$\\
Point estimator of some parameter $\theta$: $\ \est{\theta} = g(x_1, \ldots, x_m)$ \\ 
\\
$\p{model}(x; \theta)$ family of probability distributions over same space as $\p{data}(x)$, indexed by $\theta$ \\
Maximum likelihood estimator for $\theta$ defined as:
\begin{flalign*}
    & \theta^{(ML)}_{m} := \argmax_\theta \prod_{i=1}^{m} \p{model}(x_i; \theta) &&\\
    \intertext{Since argmax, we can redefine using monotonicity of log to avoid inconvenient product (e.g. underflow) and rescale with $1/m$:}
    & \theta^{(ML)}_{m} := \argmax_\theta \frac{1}{m} \sum_{i=1}^{m} \log{\p{model}(x_i; \theta)} = \argmin_\theta \frac{1}{m} \sum_{i=1}^{m} - \log{\p{model}(x_i; \theta)} \quad (*)&&\\
    &= \argmin_\theta \mathlarger{\int}\br{\frac{1}{m}\sum_{i=1}^{m} \delta\br{x - x_i}} \br{-\log{\p{model}(x; \theta)}} \, dx &&\\
    & = \argmin_\theta \E[x\sim \hat{p}_\text{data}]{-\log{\p{model}(x; \theta)}} &&\\
    & = \argmin_\theta H(\hat{p}_\text{data}, \p{model}(\theta))
    &&&\\
    & \dkl{\hat{p}_\text{data}}{\p{model}(\theta)} = \E[x\sim \hat{p}_\text{data}]{\log{\hat{p}_\text{data}(x)}} +  H(\hat{p}_\text{data}, \p{model}(\theta))&&\\
\end{flalign*}

preferred estimator to use for machine learning because of nice properties:
\begin{itemize}
    \item $\theta^{(ML)}_{m}$ is set of parameters which minimizes cross-entropy / KL divergence (\arrow dissimilarity) between observed distribution of $m$ examples and model distribution
    \item It can also be shown that:
    \item if there exists $\theta$ such that $\p{data}(x) = \p{model}(x;\theta)$ then $\theta^{(ML)}_{m}$ converges to that parameter for $m \to \infty$ (\arrow consistency)
    \item for given large $m$, no other consistent estimator has lower Mean Squared Error
    \begin{flalign*}
        \text{MSE}\left(\est{\theta}\right) = \E[x\sim p_\text{data}]{\br{\est{\theta} - \theta}^2}
    \end{flalign*} 
    than ML (\arrow Cramer-Rao lower bound, high statistical efficiency)
    \item provides a "natural" way of defining cost / loss function
\end{itemize}

ML loss function:
\begin{flalign*}
    &(*) \implies \theta^{(ML)}_{m} := \argmin_\theta \frac{1}{m} \sum_{i=1}^{m} - \log{\p{model}(x_i; \theta)} =: \argmin_\theta J(x_1, \ldots, x_m; \theta)&&\\
    & J_m(\theta) := J(x_1, \ldots, x_m; \theta) = \frac{1}{m} \sum_{i=1}^{m} L(x; \theta)&&
\end{flalign*}
with $L$: per-example loss \\
this type of loss-function: cross-entropy loss / log loss \\
We can generalize to conditional probabilities which appear in supervised learning:
\begin{flalign*}
    \theta^{(ML)}_{m} := \argmin_\theta \frac{1}{m} \sum_{i=1}^{m} - \log{\p{model}\c{x_i}{y_i; \theta}} =: \argmin_\theta J(x_1, y_1, \ldots, x_m, y_m; \theta)
\end{flalign*}


\subsection{Stochastic Gradient Descent}
Mostly our cost function will depend on examples, e.g. const function defined by ML-Estimator \\
To minimize cost function we usually use some form of gradient descent, simplest form:
\begin{flalign*}
    \theta_{k+1} \, \leftarrow \, \theta_k - \epsilon \nabla_\theta J_m(\theta)
\end{flalign*}
This works because the gradient of $J$ with respect to $\theta$ points in the direction, in which $J$ increases the most
step size $\epsilon$ is also called the learning rate

\begin{flalign*}
    \nabla_\theta J_m(\theta) = \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta L(x_i, y_i; \theta) = \E[x,y \sim \hat{p}_\text{data}]{\nabla_\theta L(x, y; \theta)}
\end{flalign*}
Normally we will would to include as many examples as possible, i.e. $m$ large such that:
\begin{flalign*}
    \argmin_\theta J_m(\theta) = \theta^{(ML)}_{m} \approx \theta^* &&
\end{flalign*}
But that causes calculating the gradient to have cost $O(m)$, which makes each gradient step prohibitively long for large training sets \\

We see however that the gradient $\nabla_\theta J_m(\theta)$ is an expectation, and we can define an estimator for the gradient:
\begin{flalign*}
    g := \frac{1}{m^\prime} \sum_{i=1}^{m^\prime} \nabla_theta L(x_i, y_i; \theta)
\end{flalign*}
where $m^\prime \ll M$ and possibly constant and the set of examples used for the estimators is called a minibatch \\
for each step, randomly sample minibatch of examples from test set and compute gradient estimate with those examples \\
with regard to training set size this can be viewed as reducing the computational cost to $O(1)$ \\
minibatch size typically chosen ranges from 1 to few hundred examples

\subsection{Hyperparameters}
Parameters that control the algorithms behavior, however they are not adapted by learning algorithm itself \\
Reasons: parameter difficult to optimize, not appropriate to learn on training set \\
Examples: \\
\begin{itemize}
    \item Learning rate
    \item All parameters that control model capacity \\ \arrow optimization of these parameters on training set would maximize
    capacity, resulting in overfitting
\end{itemize}

\section{Deep Learning}

\subsection{Challenges Motivating Deep Learning}
\begin{itemize}
    \item Curse of Dimensionality: number of possible configurations gets much larger than number of examples, 
    many traditional machine learning algorithms assume output should be approximately that of nearest training point (smoothness prior)\\
    \arrow implicit assumption of smoothness $f(x) \approx f(x + \epsilon)$
    \item Deep learning introduces additional (explicit and implicit) assumptions which can help to reduce generalization error (e.g. concept of receptive field for CNNs)
    \item Idea / Intuition: with Deep Learning we can split complex tasks into subtasks that build on one another and progressively have more and more abstraction
\end{itemize}
    
\subsection{Definition of Deep Learning, Neural Networks}
%graph deep network
Network:\\
Neural networks typically formed by composition of many different functions \\
Can be represented by directed graph (direction of data flow is defined) \\
\arrow network of nodes \\
\textbf{Neural} Networks: \\
architecture loosely inpired by neuroscience \\
interconnected graph nodes playing role analogous to neuron \\
similar to neuron we speak of activation of node: numerical value that the node passes on to following nodes \arrow also  \\
"depth" of graph \arrow origin of term "\textbf{Deep} Learning"  \\
Neural networks and deep learning algorithms are subset of machine learning algorithms with added property of network character and depth

\subsection{Deep Feedforward Networks}
\subsubsection*{Definition}
also known as multilayer perceptrons (MLPs) \\
quintessential deep learning models \\
feedforward: data only flows in one direction, no loops in graph \\
typically have chain structure with vector valued functions:
% typical mlp graph with nodes=vector components
\begin{flalign*}
    y = f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))
\end{flalign*}
we call the individual $f^{(i)}$s layers, with final layer called output layer, intermediate layers hidden layers \\
hidden layer: training examples $(x_i, y_i)$ do not explicitly specify what the intermediate layers should do, it is up to the deep learning algorithm to learn what to do in these layers

\subsubsection*{Hidden Units}
Typically the functions $f^{(k)}$ forming hidden layers layer are an affine vector-transformation combined with a nonlinear activation function:
\begin{flalign*}
    & z^{(k)}_i = W_{ij} a^{(k-1)}_j + b^{(k)}_i, \quad a^{(k)}_i = g^{(k)}(z^{(k)}_i) &&\\
    & f^{(k)}(a^{(k-1)}) = g(W^{(k)} a^{(k-1)} + b^{(k)}) &&\\
    & a^{(k)}: \ \text{activation vector} &&\\
    & g^{(k)}: \ \text{nonlinear activation function (generally element-wise)} &&\\
    & W^{(k)}: \ \text{weight matrix} &&\\
    & b^{(k)}: \ \text{bias vector} &&\\
\end{flalign*}
 
\subsubsection*{Intuition/Reason for use of Nonlinear Activation Functions}
\begin{itemize}
    \item We need some way of breaking linearity, without activation functions:
    \begin{flalign*}
        f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x))) = \k{W}{3} \k{W}{2} \k{W}{1} x + c
    \end{flalign*}
    with $c$ collection of layer biases \\
    \arrow affine transformation again \\
    since affine transformations preserve affine subspaces, if we cannot separate a set of points by a hyperplane in our original space, we wont be able to separate them by a hyperplane after transformation \\
    \arrow simple example this is problematic: we cannot learn XOR without nonlinear activation function: 
    %XOR image
    \item Make it possible to capture repeating patterns: \\
    A layer using an affine transformation with the activation function $g(z_i) = \abs{z_i}$ essentially defines a hyperplane, along which space is folded \\
    Composition of such layers in deep network folds the input space many times, then finally using a simple function on the folded space results in complex symmetrical pattern on the original space
    %folding image 
    \item Loose analogy to biology: real neurons dont have negative activations, most popular activation function rectified linear units (ReLU) simply "cuts" the negative activation:
    \begin{flalign*}
        \text{ReLU}(z_i) := \max(0, z_i)
    \end{flalign*}
    \item Formal justification for use of activation functions: universal approximation theorems
\end{itemize}

\subsubsection*{Universal Approximation Theorems}
prove that functions can be approximated with any desired nonzero amount of error by MLPs with certain activation functions and enough hidden units \\
specifically there also exists a universal approximation theorem for MLPs using the popular ReLU activation function\\
however the theorem only guarantees that functions can be represented, not that they can be learned \\
This ties into another theorem:

\subsection*{No Free Lunch Theorem}
Statement:\\
there is no universally superior machine learning algorithm\\
averaged over all possible data-generating distributions, every algorithm will perform equally bad \\
\arrow understand what data-generating distributions might be relevant to the individual problem and choose machine learning algorithm that performs well for these kinds of distributions

\subsection*{Output Units}
The final layer of a network with $m$ layers should produce the desired output of our algorithm \\
In the maximum likelihood approach we usually see
\begin{flalign*}
    \k{z}{m} = \k{W}{m} \k{a}{m-1} + \k{b}{m}
\end{flalign*}
as a parameter, controlling our model distribution $\p{model}$\\
\\
Gaussian Output:\\
For example, if we want to learn the distribution $p\c{y}{x}$, and our model distribution is a normal distribution (with identity covariance matrix), we would see $\k{z}{m}$ the as the mean:
\begin{flalign*}
    & \hat{y}(x) := \k{z}{m} &&\\
    & \p{model}\c{y}{x} = \mathcal{N}(y; \hat{y}(x), \mathbb{I}) \propto \exp\br{-\frac{1}{2}\br{y-\hat{y}(x)}^2} &&
\end{flalign*}
\\
Bernoulli Output: \\
If we have a classification problem with two classes $y \in \set{0, 1}{}$, we use a Bernoulli-Distribution and let $\k{W}{m}$ be a matrix with only one row:
\begin{flalign*}
    \p{model}\c{y=1}{x} + \p{model}\c{y=0}{x} \overset{!}{=} 1
\end{flalign*}
Define unnormalized distribution and normalize it:
\begin{flalign*}
    &z(x) := \k{z}{m} &&\\
    &\tilde{P}\c{y}{x} := \exp(yz(x)) &&\\
    &\implies \p{model}\c{y}{x} := \frac{\tilde{P}\c{y}{x}}{\sum_{y^\prime = 0}^{1} \tilde{P}\c{y^\prime}{x}} 
    = \frac{\exp(yz(x))}{1+\exp(z(x))}
    =\sigma((2y - 1)z(x))&&\\
    &\text{with sigmoid function:} \ \sigma(x) := \frac{1}{1+ \exp(-x)} = \frac{\exp(x)}{1 + \exp(x)}&&\\ 
\end{flalign*}

\subsection*{Components of a Deep Learning Algorithm}
Nearly all deep learning algorithms can be broken down into the following components:
\begin{itemize}
    \item Specification of a dataset
    \item Model (model distribution $\p{model}$, computation procedure $f(x)$)
    \item Cost function
    \item Optimization procedure
\end{itemize}
By now, we have all the tools / knowledge needed define the first three components for a real world example: handwriting recognition on mnist dataset
\subsection{Architecture of simple MLP for MNIST Dataset}
\subsubsection*{Dataset}
\begin{itemize}
    \item $\approx$ 40000 labeled examples $(x_i, y_i)$
    \item $x_i$: 28x28 grayscale pixel vector
    \item $y_i$: digit the image represents / contains (0-9)
\end{itemize}

\subsubsection*{Model}
We want to output a vector $y(x)$ with $y_i(x) = \p{model}\c{i}{x} , i \in \set{0, \ldots, 9}{}$ representing the possibility of the image depicting integer $i$ \\
\arrow $\p{model}$ should be a Multinoulli distribution:
\begin{flalign*}
    & 0 \overset{!}{\leq} \p{model}\c{i}{x} \overset{!}{\leq} 1 &&\\
    & \sum_{i=0}^{9} \p{model}\c{i}{x} \overset{!}{=} 1 &&
\end{flalign*}

Once again, define unnormalized distribution and normalize:
\end{document}

1. Empirical Distribution, KL Divergence, Cross-Entropy
2. Estimators
4. Maximum Likelihood Estimation
5. Stochastic Gradient Descent
Underfitting / Overfitting 
Deep Learning
1. Challenges Motivating Deep Learning
2. Definition of Deep Learning
3. Deep Feedforward Networks
    1. Hidden Units
    2. Back-Propagation
    3. Model Design
4. Regularization
5. Optimization Algorithms
6. Convolutional Neural Networks
7. Neuroevolution?