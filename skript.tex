\documentclass{article}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{relsize}
\usepackage{float}

\setlength{\parindent}{0pt}

\def\noval{}

\DeclarePairedDelimiter{\br}{(}{)}
\DeclarePairedDelimiter{\brr}{[}{]}
\DeclarePairedDelimiter{\brs}{\{}{\}}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\makeatletter
\let\oldbr\br
\def\br{\@ifstar{\oldbr}{\oldbr*}}
\makeatother

\makeatletter
\let\oldbrr\brr
\def\brr{\@ifstar{\oldbrr}{\oldbrr*}}
\makeatother

\makeatletter
\let\oldbrs\brs
\def\brs{\@ifstar{\oldbrs}{\oldbrs*}}
\makeatother

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
\makeatother

\makeatletter
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\makeatletter
\let\oldlog\log
\def\log{\@ifstar\@log\@@log}
\def\@log#1{\oldlog\br{#1}}
\def\@@log#1{\oldlog#1}
\makeatother


\makeatletter
\newcommand{\set}[2]{
    \def\@delim{\;\@ifstar{\middle|}{:}\;}
    \ifx\noval#2\noval%
        \left\{#1\right\}
    \else
        \left\{#1\@delim#2\right\}
    \fi
}
\makeatother

\newcommand{\E}[2][]{%
    \ifx\noval#1\noval%
        \mathbb{E}\brr{#2}
    \else
        \mathbb{E}_{#1}\brr{#2}
    \fi
}
\newcommand{\var}[2][]{%
    \ifx\noval#1\noval%
        \text{Var}\br{#2}
    \else
        \text{Var}_{#1}\br{#2}
    \fi
}
\newcommand{\cov}[2][]{%
    \ifx\noval#1\noval%
        \text{Cov}\br{#2}
    \else
        \text{Cov}_{#1}\br{#2}
    \fi
}

\newcommand{\est}[1]{\hat{#1}_m}
\newcommand{\bias}[1]{\text{bias}\br{#1}}


\newcommand{\arrow}{$\rightarrow\;$}

\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\text{T}}
\renewcommand{\v}{\bm}
\renewcommand{\c}[2]{\left(#1\;\middle|\;#2\right)}
\renewcommand{\det}[1]{\text{det}\br{#1}}
\newcommand{\ml}[1]{\mathlarger{#1}\,}
\newcommand{\dkl}[2]{D_{\text{KL}}(#1\;\Vert\;#2)}
\renewcommand{\S}{\mathbb{S}}
\newcommand{\func}[3]{#1:#2\rightarrow#3}
\newcommand{\p}[1]{p_\text{#1}}
\renewcommand{\k}[2]{#1^{(#2)}}

\begin{document}
\section{Definitions}
\subsection{Definition of Machine Learning}
Broad definition of machine learning: \\
“A computer program is said to learn from experience E 
with respect to some class of tasks T and performance measure P, 
if its performance at tasks in T, as measured by P, improves with
experience E.”

Generally, we want to learn some function
\begin{flalign*}
    \func{f}{\R^n}{\R^m}, \, f(x) = y
\end{flalign*}
of high complexity

\subsection{Types of tasks}
Different classes of tasks T require different kinds of functions, \\
Common tasks are:
\begin{itemize}
    \item Classification of $k$ categories: produce function $\func{f}{\R^n}{R^k}$ \\
    Output: probability vector $\norm{\v{y}} = 1$ (can also be one-hot)\\
    Variation: missing inputs in $\v{x}$
    \item Regression: predict numerical value given some input: $\func{f}{\R^n}{R^m}$
    \item Structured output: output is vector with important relationships between elements e.g. output 
    token codes in language processing
    \item Probability density estimation: explicitly learn function $\func{\p{model}}{\R^n}{\R}$ on space examples were drawn from \\
    most above tasks already require implicit capture of structure/parameters of underlying probability distribution
\end{itemize}

\subsection{Types of learning}
Algorithm usually learns / gains experience E by observing examples \\
Tasks usually described in terms of how system should process an example \\
Types of learning can roughly be divided into:
\begin{itemize}
    \item Unsupervised: experience dataset of examples $x_i$, learning useful properties of dataset structure \\
    e.g. probability density $p(\v{x})$ or clustering dataset
    \item Supervised: experience dataset of labeled examples $\br{\v{x}_i, \v{y}_i}$, learn to estimate $p\c{\v{y}}{\v{x}}$
    \item Semi-supervised: some examples have label/target, some not
    \item Multi-instance: collection of examples labeled but not each example
    \item Reinforcement: interaction with environment, feedback loop between s
    ystem and experiences
\end{itemize}

\subsection{What is learned?}
%image: venn diagram
Form of representation of object that should be processed is very important \\
Pieces of information included in representation of object known as features \\
e.g. Image - Representation: Array of Pixels, Features: Individual pixel values \\

General machine learning: discover mapping from representation to output \\
Example: logistic regression: learn parameters $\beta_0, \beta_1$ such that
\begin{flalign*}
    f(x) = \frac{1}{1+\exp(-\beta_0 -\beta_1 x)}
\end{flalign*}
best fits given data\\
%https://en.wikipedia.org/wiki/Logistic_regression

Representation learning: also learn the "optimal" representation of data \\
Example of representation learning: Autoencoders \\
Encoder function: converts data to different representation \\
Decoder function: converts new representation back to original format \\
Task: preserve as much information, while providing additional nice properties e.g. reduced size in new representation \\

Deep learning tackles representation learning by introducing representations that are expressed in terms of other, simpler representations \\
e.g. Image: Pixels \arrow Edges \arrow Contours \arrow Object parts \arrow Object identity \\
Quintessential Example of Deep Learning: Multilayer Perceptron (\arrow later)
%image: representations for image MLP

\subsection{Performance Measure P}
Often difficult to find performance measure that corresponds well to desired behavior of 
system and is suitable for optimization
\begin{itemize}
    \item transcription: how do we value sentences that are not fully correct but close
    \item density estimation: can be impossible to explicitly compute probability for specific points in space
\end{itemize}

Generally we would like to have a function which we can optimize to improve
performance of algorithm on previously unseen data \arrow Loss / Objective / Cost / Error function (mostly synonymous), $J(\theta)$
with $\theta$ model parameters\\
\\

\subsection{Types of Error}
Difference between Machine Learning and simple optimization: we do not only want low error on examples (training error) but also on 
previously unseen data (generalization error) \\
Capacity: ability of model to fit function to be learned \\
Underfitting: model cannot obtain low enough training error, usually capacity too low \\
Overfitting: gap between training and generalization error too large, can be caused by too high capacity for task
\arrow more parameters than examples or "memorization" of training data \\
Best performance with capacity appropriate for true complexity of task and amount of training data \\
%image training, generalization error, capacity

To estimate generalization error, split examples in training set and test set, use only training set for learning \\
To be able to make estimations, have to make i.i.d. assumptions about data generating process:
\begin{itemize}
    \item individual examples independent from each other
    \item training set and test set identically distributed
\end{itemize}
\arrow data generation can be described by distribution over single example $\p{data}(\v{x})$ \\

\section{Machine Learning Basics}
\subsection{Statistics, Information Theory}
\subsubsection*{Empirical distribution (Tafel)}
Puts probability mass $\frac{1}{m}$ on each of the $m$ observed samples $\v{x}_i$
\begin{flalign*}
    \hat{p}(\v{x}) = \frac{1}{m} \sum_{i=1}^{m} \delta(\v{x} - \v{x}_i) &&
\end{flalign*}

\subsubsection*{Information content}
Desired properties of information measure:
\begin{itemize}
    \item Information content of event should depend on its likelihood (low/high likelihood $\leftrightarrow$ high/low information)
    \item Independent events should have additive information: e.g. $I(2\times \text{Heads}) \overset{!}{=} 2\cdot I(\text{Heads})$
\end{itemize}

Define self-information of event $X=x$ as:
\begin{flalign*}
    I(X=x) := -\log{p(X=x)} && 
\end{flalign*}

\subsubsection*{Shannon entropy}
Quantification of uncertainty in probability distribution:
\begin{flalign*}
    H(X) := \E[X \sim p]{I(x)} &&
\end{flalign*}
\arrow Expectation value of self-information

\subsubsection*{Kullback-Leibler (KL) divergence}
Measure of how different distributions $P(X)$ and $Q(X)$ over same variable $X$ are:
\begin{flalign*}
    \dkl{P}{Q} := \E[X \sim P]{\log{\frac{P(x)}{Q(x)}}} &&
\end{flalign*}
Properties:\\
$\dkl{P}{Q} \geq 0, \ \dkl{P}{Q} = 0 \iff P(X) = Q(X)$ \\ 
$\rightarrow$ concept of distance between distributions, however not symmetric so it makes difference which divergence is minimized \\
%image: difference between D_KL(P,Q) and D_KL(Q,P)

\subsubsection*{Cross-Entropy}
\begin{flalign*}
    H(P,Q) = -\E[X\sim p]{\log{Q(x)}} &&
\end{flalign*}
Minimizing cross-entropy with respect to $Q$ equivalent to minimizing KL divergence, since variation of $Q$ does not affect omitted term

\subsection{Maximum Likelihood Estimator}
Estimator: \\
Given i.i.d. data points $\mathbb{X} = \set{x_1, \ldots, x_m}{}$ drawn from data-generating distribution $\p{data}(x)$\\
Point estimator of some parameter $\theta$: $\ \est{\theta} = g(x_1, \ldots, x_m)$ \\ 
\\
$\p{model}(x; \theta)$ family of probability distributions over same space as $\p{data}(x)$, indexed by $\theta$ \\
Maximum likelihood estimator for $\theta$ defined as:
\begin{flalign*}
    & \theta^{(ML)}_{m} := \argmax_\theta \prod_{i=1}^{m} \p{model}(x_i; \theta) &&\\
    \intertext{Since argmax, we can redefine using monotonicity of log to avoid inconvenient product (e.g. underflow) and rescale with $1/m$:}
    & \theta^{(ML)}_{m} := \argmax_\theta \frac{1}{m} \sum_{i=1}^{m} \log{\p{model}(x_i; \theta)} = \argmin_\theta \frac{1}{m} \sum_{i=1}^{m} - \log{\p{model}(x_i; \theta)} \quad (*)&&\\
    &= \argmin_\theta \mathlarger{\int}\br{\frac{1}{m}\sum_{i=1}^{m} \delta\br{x - x_i}} \br{-\log{\p{model}(x; \theta)}} \, dx &&\\
    & = \argmin_\theta \E[x\sim \hat{p}_\text{data}]{-\log{\p{model}(x; \theta)}} &&\\
    & = \argmin_\theta H(\hat{p}_\text{data}, \p{model}(\theta))
    &&&\\
    & \dkl{\hat{p}_\text{data}}{\p{model}(\theta)} = \E[x\sim \hat{p}_\text{data}]{\log{\hat{p}_\text{data}(x)}} +  H(\hat{p}_\text{data}, \p{model}(\theta))&&\\
\end{flalign*}

preferred estimator to use for machine learning because of nice properties:
\begin{itemize}
    \item $\theta^{(ML)}_{m}$ is set of parameters which minimizes cross-entropy / KL divergence (\arrow dissimilarity) between observed distribution of $m$ examples and model distribution
    \item It can also be shown that:
    \item if there exists $\theta$ such that $\p{data}(x) = \p{model}(x;\theta)$ then $\theta^{(ML)}_{m}$ converges to that parameter for $m \to \infty$ (\arrow consistency)
    \item for given large $m$, no other consistent estimator has lower Mean Squared Error
    \begin{flalign*}
        \text{MSE}\left(\est{\theta}\right) = \E[x\sim p_\text{data}]{\br{\est{\theta} - \theta}^2}
    \end{flalign*} 
    than ML (\arrow Cramer-Rao lower bound, high statistical efficiency)
    \item provides a "natural" way of defining cost / loss function
\end{itemize}

ML loss function:
\begin{flalign*}
    &(*) \implies \theta^{(ML)}_{m} := \argmin_\theta \frac{1}{m} \sum_{i=1}^{m} - \log{\p{model}(x_i; \theta)} =: \argmin_\theta J(x_1, \ldots, x_m; \theta)&&\\
    & J_m(\theta) := J(x_1, \ldots, x_m; \theta) = \frac{1}{m} \sum_{i=1}^{m} L(x; \theta)&&
\end{flalign*}
with $L$: per-example loss \\
this type of loss-function: cross-entropy loss / log loss \\
We can generalize to conditional probabilities which appear in supervised learning:
\begin{flalign*}
    \theta^{(ML)}_{m} := \argmin_\theta \frac{1}{m} \sum_{i=1}^{m} - \log{\p{model}\c{x_i}{y_i; \theta}} =: \argmin_\theta J(x_1, y_1, \ldots, x_m, y_m; \theta)
\end{flalign*}


\subsection{Stochastic Gradient Descent}
Mostly our cost function will depend on examples, e.g. const function defined by ML-Estimator \\
To minimize cost function we usually use some form of gradient descent, simplest form:
\begin{flalign*}
    \theta_{k+1} \, \leftarrow \, \theta_k - \epsilon \nabla_\theta J_m(\theta)
\end{flalign*}
This works because the gradient of $J$ with respect to $\theta$ points in the direction, in which $J$ increases the most
step size $\epsilon$ is also called the learning rate

\begin{flalign*}
    \nabla_\theta J_m(\theta) = \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta L(x_i, y_i; \theta) = \E[x,y \sim \hat{p}_\text{data}]{\nabla_\theta L(x, y; \theta)}
\end{flalign*}
Normally we will would to include as many examples as possible, i.e. $m$ large such that:
\begin{flalign*}
    \argmin_\theta J_m(\theta) = \theta^{(ML)}_{m} \approx \theta^* &&
\end{flalign*}
But that causes calculating the gradient to have cost $O(m)$, which makes each gradient step prohibitively long for large training sets \\

We see however that the gradient $\nabla_\theta J_m(\theta)$ is an expectation, and we can define an estimator for the gradient:
\begin{flalign*}
    g := \frac{1}{m^\prime} \sum_{i=1}^{m^\prime} \nabla_theta L(x_i, y_i; \theta)
\end{flalign*}
where $m^\prime \ll M$ and possibly constant and the set of examples used for the estimators is called a minibatch \\
for each step, randomly sample minibatch of examples from test set and compute gradient estimate with those examples \\
with regard to training set size this can be viewed as reducing the computational cost to $O(1)$ \\
minibatch size typically chosen ranges from 1 to few hundred examples

\subsection{Hyperparameters}
Parameters that control the algorithms behavior, however they are not adapted by learning algorithm itself \\
Reasons: parameter difficult to optimize, not appropriate to learn on training set \\
Examples: \\
\begin{itemize}
    \item Learning rate
    \item All parameters that control model capacity \\ \arrow optimization of these parameters on training set would maximize
    capacity, resulting in overfitting
\end{itemize}

\section{Deep Learning}

\subsection{Challenges Motivating Deep Learning}
\begin{itemize}
    \item Curse of Dimensionality: number of possible configurations gets much larger than number of examples, 
    many traditional machine learning algorithms assume output should be approximately that of nearest training point (smoothness prior)\\
    \arrow implicit assumption of smoothness $f(x) \approx f(x + \epsilon)$
    \item Deep learning introduces additional (explicit and implicit) assumptions which can help to reduce generalization error (e.g. concept of receptive field for CNNs)
    \item Idea / Intuition: with Deep Learning we can split complex tasks into subtasks that build on one another and progressively have more and more abstraction
\end{itemize}
    
\subsection{Definition of Deep Learning, Neural Networks}
%graph deep network
Network:\\
Neural networks typically formed by composition of many different functions \\
Can be represented by directed graph (direction of data flow is defined) \\
\arrow network of nodes \\
\textbf{Neural} Networks: \\
architecture loosely inpired by neuroscience \\
interconnected graph nodes playing role analogous to neuron \\
similar to neuron we speak of activation of node: numerical value that the node passes on to following nodes \arrow also  \\
"depth" of graph \arrow origin of term "\textbf{Deep} Learning"  \\
Neural networks and deep learning algorithms are subset of machine learning algorithms with added property of network character and depth

\subsection{Deep Feedforward Networks}
\subsubsection*{Definition}
also known as multilayer perceptrons (MLPs) \\
quintessential deep learning models \\
feedforward: data only flows in one direction, no loops in graph \\
typically have chain structure with vector valued functions:
% typical mlp graph with nodes=vector components
\begin{flalign*}
    y = f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))
\end{flalign*}
we call the individual $f^{(i)}$s layers, with final layer called output layer, intermediate layers hidden layers \\
hidden layer: training examples $(x_i, y_i)$ do not explicitly specify what the intermediate layers should do, it is up to the deep learning algorithm to learn what to do in these layers

\subsubsection*{Hidden Units}
Typically the functions $f^{(k)}$ forming hidden layers layer are an affine vector-transformation combined with a nonlinear activation function:
\begin{flalign*}
    & z^{(k)}_i = W_{ij} a^{(k-1)}_j + b^{(k)}_i, \quad a^{(k)}_i = g^{(k)}(z^{(k)}_i) &&\\
    & f^{(k)}(a^{(k-1)}) = \k{g}{k}(W^{(k)} a^{(k-1)} + b^{(k)}) &&\\
    & a^{(k)}: \ \text{activation vector} &&\\
    & g^{(k)}: \ \text{nonlinear activation function (generally element-wise)} &&\\
    & W^{(k)}: \ \text{weight matrix} &&\\
    & b^{(k)}: \ \text{bias vector} &&\\
\end{flalign*}
 
\subsubsection*{Intuition/Reason for use of Nonlinear Activation Functions}
\begin{itemize}
    \item We need some way of breaking linearity, without activation functions:
    \begin{flalign*}
        f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x))) = \k{W}{3} \k{W}{2} \k{W}{1} x + c
    \end{flalign*}
    with $c$ collection of layer biases \\
    \arrow affine transformation again \\
    since affine transformations preserve affine subspaces, if we cannot separate a set of points by a hyperplane in our original space, we wont be able to separate them by a hyperplane after transformation \\
    \arrow simple example this is problematic: we cannot learn XOR without nonlinear activation function: 
    %XOR image
    \item Make it possible to capture repeating patterns: \\
    A layer using an affine transformation with the activation function $g(z_i) = \abs{z_i}$ essentially defines a hyperplane, along which space is folded \\
    Composition of such layers in deep network folds the input space many times, then finally using a simple function on the folded space results in complex symmetrical pattern on the original space
    %folding image 
    \item Loose analogy to biology: real neurons dont have negative activations, most popular activation function rectified linear units (ReLU) simply "cuts" the negative activation:
    \begin{flalign*}
        \text{ReLU}(z_i) := \max(0, z_i)
    \end{flalign*}
    \item Formal justification for use of activation functions: universal approximation theorems
\end{itemize}

\subsubsection*{Universal Approximation Theorems}
prove that functions can be approximated with any desired nonzero amount of error by MLPs with certain activation functions and enough hidden units \\
specifically there also exists a universal approximation theorem for MLPs using the popular ReLU activation function\\
however the theorem only guarantees that functions can be represented, not that they can be learned \\
This ties into another theorem:

\subsection*{No Free Lunch Theorem}
Statement:\\
there is no universally superior machine learning algorithm\\
averaged over all possible data-generating distributions, every algorithm will perform equally bad \\
\arrow understand what data-generating distributions might be relevant to the individual problem and choose machine learning algorithm that performs well for these kinds of distributions

\subsection*{Output Units}
The final layer of a network with $m$ layers should produce the desired output of our algorithm \\
In the maximum likelihood approach we usually see
\begin{flalign*}
    \k{z}{m} = \k{W}{m} \k{a}{m-1} + \k{b}{m}
\end{flalign*}
as a parameter, controlling our model distribution $\p{model}$\\
\\
Gaussian Output:\\
For example, if we want to learn the distribution $p\c{y}{x}$, and our model distribution is a normal distribution (with identity covariance matrix), we would see $\k{z}{m}$ the as the mean:
\begin{flalign*}
    & \hat{y}(x) := \k{z}{m} &&\\
    & \p{model}\c{y}{x} = \mathcal{N}(y; \hat{y}(x), \mathbb{I}) \propto \exp\br{-\frac{1}{2}\br{y-\hat{y}(x)}^2} &&
\end{flalign*}
\\
Bernoulli Output: \\
If we have a classification problem with two classes $y \in \set{0, 1}{}$, we use a Bernoulli-Distribution and let $\k{W}{m}$ be a matrix with only one row:
\begin{flalign*}
    \p{model}\c{y=1}{x} + \p{model}\c{y=0}{x} \overset{!}{=} 1
\end{flalign*}
Define unnormalized distribution and normalize it:
\begin{flalign*}
    &z(x) := \k{z}{m} &&\\
    &\tilde{p}\c{y}{x} := \exp(yz(x)) &&\\
    &\implies \p{model}\c{y}{x} := \frac{\tilde{p}\c{y}{x}}{\sum_{y^\prime = 0}^{1} \tilde{p}\c{y^\prime}{x}} 
    = \frac{\exp(yz(x))}{1+\exp(z(x))}
    =\sigma((2y - 1)z(x))&&\\
    &\text{with sigmoid function:} \ \sigma(x) := \frac{1}{1+ \exp(-x)} = \frac{\exp(x)}{1 + \exp(x)}&&\\ 
\end{flalign*}

\subsection*{Components of a Deep Learning Algorithm}
Nearly all deep learning algorithms can be broken down into the following components:
\begin{itemize}
    \item Specification of a dataset
    \item Model (model distribution $\p{model}$, computation procedure $f(x)$)
    \item Cost function
    \item Optimization procedure
\end{itemize}
By now, we have all the tools / knowledge needed define the first three components for a real world example: handwriting recognition on mnist dataset
\subsection{Architecture of simple MLP for MNIST Dataset}
\subsubsection*{Dataset}
\begin{itemize}
    \item $\approx$ 40000 labeled examples $(x_k, y_k)$
    \item $x_k$: 28x28 grayscale pixel vector
    \item $y_k$: digit the image represents / contains (0-9)
\end{itemize}

\subsubsection*{Model}
We want to output a vector $p(x)$ with $p_i = \p{model}\c{y = i}{x} , i \in \set{0, \ldots, 9}{}$ representing the possibility of the image depicting integer $i$ \\
\arrow $\p{model}$ should be a Multinoulli distribution:
\begin{flalign*}
    & 0 \leq \p{model}\c{y = i}{x} \leq 1 &&\\
    & \sum_{i=0}^{9} \p{model}\c{y = i}{x} = 1 &&
\end{flalign*}

Once again, define unnormalized distribution and normalize:
\begin{flalign*}
    & z(x) := \k{z}{m} = \k{W}{m} \k{a}{m-1} + \k{b}{m}&&\\
    & \tilde{p}\c{y = i}{x} := \exp(z_i(x)) &&\\
    & \implies \p{model}\c{y = i}{x} = \frac{\tilde{p}\c{y=i}{x}}{\sum_{j=0}^{9}\tilde{p}\c{y=j}{x}} = \frac{\exp(z_i(x))}{\sum_{j=0}^{9}\exp(z_j(x))} =: \text{softmax}(z(x))_i &&\\
    & \iff p(x) = \text{softmax}(z(x)) &&\\
\end{flalign*}

Without any particular reason, choose network to have two hidden layers with 16 width each, using ReLU activation \\
\\
Overview of the network:
\begin{table}[H]
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    Layer type              & Input         & Hidden    & Hidden    & Output  \\
    \hline
    Layer width                   & $28 \times 28 = 784$ & $16$      & $16$      & 
    $10$      \\
    \hline
    $\k{W}{k}$ shape & -             & $(16, 784)$      & $(16,16)$      & $(10, 16)$ \\
    \hline
    $\k{b}{k}$ shape & -             & $(16, 1)$      & $(16,1)$      & $(10, 1)$ \\
    \hline
    Activation $\k{g}{k}$ & -             & ReLU      & ReLU      & Softmax \\
    \hline
    $k$                     & $0$           & $1$       & $2$       & $3$       \\
    \hline
    \end{tabular}
\end{table}
Essentially, we see the computation of $p$ in the output layer with softmax as just another type of activation function \\
We can describe the algorithm for propagating the input through the network like so:
\begin{flalign*}
    & \k{a}{0} = x &&\\
    & \text{for} \quad k = 1, \ldots, m = 3 &&\\
    & \qquad \k{z}{k} = \k{W}{k} \k{a}{k-1} + \k{b}{k} &&\\
    & \qquad \k{a}{k} = \k{g}{k}(\k{z}{k}) &&\\
    & p = \k{a}{m} &&
\end{flalign*}

\subsubsection*{Cost function}
Since model uses maximum likelihood approach, cost function and per example loss are automatically defined:
\begin{flalign*}
    & L(x_k, y_k; \theta) = - \log{\p{model}\c{y = y_k}{x_k; \theta}} = -\log*{\text{softmax}(z(x_k))_{y_k}}&&\\
    &J(\theta) = \frac{1}{m} \sum_{k=1}^{m} L(x_k, y_k; \theta)&&
\end{flalign*}
Where $\theta$ describes the set of all learnable parameters of the model i.e. the $\k{W}{k}_{ij}$ and $\k{b}{k}_i$

\section{Optimization}
How can we minimize cost functions of this type?
\subsection{Back-Propagation}
\subsubsection*{General Formulation of Back-Propagation on Graphs}
%image of general dag
Partial derivative of value of one graph node $\k{u}{n}$ with respect to another $\k{u}{j}$ can be computed by recursively applying the chain rule:
\begin{flalign*}
    \frac{\partial\k{u}{n}}{\partial\k{u}{j}} 
    = \mathlarger{\sum}_{i \ : \ \k{u}{j} \in Pa(\k{u}{i})} 
    \frac{\partial\k{u}{n}}{\partial\k{u}{i}} \frac{\partial\k{u}{i}}{\partial\k{u}{j}}
\end{flalign*}
where $Pa(\k{u}{i})$ is the set of all nodes $\k{u}{j}$ such that there is an edge going from $\k{u}{j}$ to $\k{u}{i}$ \\
This essentially collects the derivatives along all possible paths from $\k{u}{j}$ to $\k{u}{n}$ \\
If we have many such derivatives to compute, that is many different $\k{u}{j}$, we can avoid having to use recursion by starting at $\k{u}{n}$ and storing the $\frac{\partial\k{u}{n}}{\partial\k{u}{i}}$ along the way, so that they can be reused

For this to work, it is important for the graph to be directed, so that we know how nodes influence one another
Also, graph shouldnt have any cycles/loops, otherwise, a node would be able to influence itself
\arrow directed, acyclic graphs (DAGs)

\subsection*{Application of Back-Propagation to Simple Feedforward Networks}
In the case of our neural network, the algorithm can be simplified a lot, since we are mainly dealing with matrix-vector and component-wise operations \\
We are interested in $\nabla_\theta J(\theta)$, or equivalently $\nabla_\theta L(x,y; \theta) \ \iff \ \frac{\partial L}{\partial \k{W}{k}_{ij}}, \frac{\partial L}{\partial \k{b}{k}_i}$ \\
To compute these, store the following intermediate values:
\begin{flalign*}
    & k = m : \quad \frac{\partial L}{\partial \k{z}{k}_i} &&\\
    & 1 \leq k < m : \quad 
    \frac{\partial L}{\partial \k{z}{k}_j} 
    = \frac{\partial L}{\partial \k{z}{k+1}_i} \frac{\partial \k{z}{k+1}_i}{\partial \k{z}{k}_j} 
    = \frac{\partial L}{\partial \k{z}{k+1}_i} \frac{\partial}{\partial \k{z}{k}_j} \br{\k{W}{k+1}_{ip} \k{g}{k}(\k{z}{k})_p + \k{b}{k}_i} &&\\
    & \text{component-wise activation for hidden layers} &&\\ 
    &\implies \, \frac{\partial L}{\partial \k{z}{k}_j} = \frac{\partial \k{g}{k}(\k{z}{k}_j)}{\k{z}{k}_j} {\k{W}{k+1}}^T_{ji} \frac{\partial L}{\partial \k{z}{k+1}_i}  &&\\
\end{flalign*}
The intermediate values can then be used as such:
\begin{flalign*}
    & \frac{\partial L}{\partial \k{W}{k}_{ij}} 
    = \frac{\partial L}{\partial \k{z}{k}_n} \frac{\partial \k{z}{k}_n}{\partial \k{W}{k}_{ij}}
    = \frac{\partial L}{\partial \k{z}{k}_n} \frac{\partial}{\partial \k{W}{k}_{ij}} \br{\k{W}{k}_{np} \k{a}{k-1}_p + \k{b}{k}_n}
    = \frac{\partial L}{\partial \k{z}{k}_i} \k{a}{k-1}_j
    &&\\
    & \frac{\partial L}{\partial \k{b}{k}_i}
    = \frac{\partial L}{\partial \k{z}{k}_n} \frac{\partial \k{z}{k}_n}{\partial \k{b}{k}_i}
    = \frac{\partial L}{\partial \k{z}{k}_n} \frac{\partial}{\partial \k{b}{k}_i} \br{\k{W}{k}_{np} \k{a}{k-1}_p + \k{b}{k}_n}
    = \frac{\partial L}{\partial \k{z}{k}_i} &&\\
    &\text{where} \ 1 \leq k \leq m&&
\end{flalign*}
For ReLU, return one-sided derivative at $x=0$, non-formal justification: due to numerical error, our true value probably anyway was not zero for $x=0$
\begin{flalign*}
    \frac{\partial}{\partial x} \text{ReLU}(x) = \begin{cases*}
        0 & for $x \leq 0$ \\
        1 & for $x > 0 $
    \end{cases*}
\end{flalign*}

Computation of $\frac{\partial L}{\partial \k{z}{m}_i}$ for the MNIST-Model:\\
\begin{flalign*}
    & z(x_k) := \k{z}{m} \quad \text{to avoid index-chaos}&&\\
    & L(x_k, y_k; \theta) = - \log{\text{softmax}(z(x_k))_{y_k}} = \log{\sum_{j}\exp\br{z_j(x_k)}} - z_{y_k}(x_k)&&\\
    & \implies \frac{\partial L}{\partial \k{z}{m}_i} = \frac{\exp(z_i(x_k))}{\sum_{j}\exp(z_j(x_k))} - \delta_{i y_k} = \text{softmax}(z(x_k))_i - \delta_{i y_k} &&\\
\end{flalign*}
with these components, we can now compute the gradient estimate for a minibatch of size $m^\prime$:
\begin{flalign*}
    & \nabla_\theta = \frac{1}{m^\prime} \sum_{k=1}^{m^\prime} \nabla_\theta L(x_k, y_k; \theta) &&
\end{flalign*}

Softmax can become numerically unstable for large $\abs{z_i}$ or large differences between the $z_i$: \\
overflow: absolute value so large, gets rounded to $\pm\infty$ \\
overflow: absolute value so small, gets rounded to $0$ \\
Examples where this occurs:
\begin{itemize}
    \item $\sum_{j} \exp{z_j}$ can overflow or underflow for large $\abs{z_i}$
    \item $\frac{\exp(z_i)}{\sum_{j}\exp(z_j)}$ can underflow for large differences between the $z_j$ or many large $z_j$
\end{itemize}

Solution:\\
Due to factoring of exp
\begin{flalign*}
    & \text{softmax}(z) = \text{softmax}(\tilde{z}) \ \text{where} \ \tilde{z}_i := z_i - \max_j(z_j) &&
\end{flalign*}
This guarantees at least one zero in $\tilde{z}$ and otherwise only negative entries \\
\arrow at least one 1 in the sum, otherwise only values $< 1$ \\
This can help to fix:
\begin{itemize}
    \item overflow and underflow of the sum term
    \item underflow of entire sum if many large $z_j$
\end{itemize}

\section{Optimization Algorithms}
Algorithms for minimizing objective functions $J(\theta)$
\subsection{Stochastic Gradient Descent}
Basis of most used optimization algorithms for machine learning \\
\begin{flalign*}
    \theta_{k+1} = \theta_k - \epsilon g
\end{flalign*}
use minibatch to estimate $g \approx \nabla_\theta J(\theta)$ \\
Some problems of this simple algorithm:
\begin{itemize}
    \item $J(\theta)$ for neural networks with many layers often have extremly steep regions with large gradients
    \begin{itemize}
        \item gradient can overflow
        \item fixed-size learning rate $\epsilon$ can cause us to overshoot region of low cost
    \end{itemize}
    \arrow possible solution: clip gradient if larger than specified value
    \item use of minibatches corresponds to random sampling from $\hat{p}_\text{data}$, this introduces source of noise \\
    \arrow gradient usually doesnt become zero, even at minimum \\
    this cannot be solved simply by gradient clipping
\end{itemize}
two adaptations of SGD to better deal with this sort of issue:
\subsection{Momentum}
method designed to accelerate learning, especially for cases of high curvature, noisy or small but consistent gradients
\begin{flalign*}
    & v_k = \alpha v_{k-1} - \epsilon g(\theta_{k-1}) &&\\
    & \theta_k = \theta_{k-1} + v_k &&\\
    & k = 1, 2, 3,  \ldots&&
\end{flalign*}
with $\alpha \in [0,1)$ and initial velocity $v_0$\\
Physical analogy: moving particle with friction, velocity $v = p$ (unit mass) is exponentially decaying average of the gradient $g$
%image: comparison path graident descent, momentum

\subsection{Adam}
derived from phrase "adaptive moments"
moment-based optimization algorithm with adaptive learning rate
\begin{flalign*}
    \intertext{Compute Moment Estimates:}
    & s_k = \rho_1 s_{k-1} + (1-\rho_1)g(\theta_{k-1}) &&\\
    & r_k = \rho_2 r_{k-1} + (1-\rho_2)g(\theta_{k-1}) \odot g(\theta_{k-1}) &&\\
    \intertext{Bias Correction}
    & \hat{s}_{k} = \frac{s_k}{1-\rho_1^k} &&\\
    & \hat{r}_{k} = \frac{r_k}{1-\rho_2^k} &&\\
    & \Delta\theta_k = - \epsilon \frac{\hat{s_k}}{\sqrt{\hat{r}_k} + \delta} \quad \text{element-wise}&&\\
    & \theta_k = \theta_{k-1} + \Delta \theta_k &&\\
    &k = 1,2,3, \ldots&&\\
    &\rho_1, \rho_2 \in [0,1)&&\\
    &\delta \text{Small constant for numerical stabilization}&&
\end{flalign*}


first moment $s$: mean of gradient (exponentially weighted) \\
second moment $r$: measure for variance of gradient, but uncentered, i.e. not distance from mean (exponentially weighted) \\
essentially, we divide the average gradient through a measure of how strong the gradient varies, which stabilizes it \\

%https://en.wikipedia.org/wiki/Moment_(mathematics)
formal definition of moments:
$n$-th moment centered around $c$, ($c=0 \iff$ uncentered):
\begin{flalign*}
    &\mu_n = \int (x-c)^n p(x) \, dx&&
\end{flalign*}

%exponential weighting: https://stats.stackexchange.com/a/286644

%https://en.wikipedia.org/wiki/Stochastic_gradient_descent
Most popular modern machine learning optimizers are based on Adam \\
e.g. TensorFlow and PyTorch as of 2023 almost only include Adam-derived optimizers

\section{Regularization}
Strategies to improve generalization of machine learning algorithms
\subsection{Parameter Norm Constraints}
avoid overfitting and limit capacity of model by introducing a parameter norm penalty $\Omega(\theta)$ \\
typically, for neural networks penalize only weights, bias only control single variable, weight requires observing both variables it connects in variety of conditions, penalizing bias can lead to underfitting \\
\\
strategies:
\begin{itemize}
    \item Implicit constraint: \\
    Minimize regularized objective function:
    \begin{flalign*}
        &\tilde{J}(\theta) = J(\theta) + \alpha \Omega(\theta), \quad \alpha \geq 0&&
    \end{flalign*}
    implicit in sense that we cannot explicitly specify the constraint region
    \item Explicit constraint: \\
    Perform gradient step with $\nabla_\theta J(\theta)$ and project $\theta$ back in constraint region specified by e.g. $\Omega(\theta) < c$ if step left this region \\
    advantage: stabilizing effect, avoids positive feedback loop of large $\theta$ \arrow large gradient step which can lead to exploding $\theta$
\end{itemize}

\subsection{Dataset augmentation}
best way to improve generalization is more training data \\
sometimes possible to generate new training data by transforming existing examples:
e.g. slightly translate, scale or rotate example images \\
\arrow trains network to be invariant to certain translations \\
injecting small amount of random noise into input can also be seen as augmentation

\subsection{Dropout}
% dropout image
trains an ensemble of networks, consisting of all subnetworks that can be formed by removing nonoutput node from underlying base network \\
in feedforward networks based on matrix-vector operations and nonlinear activation function this can be done by simply multiplying value of removed node by zero during feedforward \\
for each example in minibatch, randomly sample binary mask, apply to input and hidden units \\
possibility to sample value 1 (\arrow leave node active) typically 0.8 for input, 0.5 for hidden node \\
run back-propagation as usual \\
when predicting output for new input, output is mean of all outputs in ensemble (\arrow members of ensemble "vote" on what the output should be) \\
can be computationally infeasible due to exponential number of subnetworks in ensemble \\
it was experimentally shown that computing this mean is equivalent to scaling outgoing weights for a node by the probability that it is included and use the base network with scaled weights to make the prediction \\
motivation: capture right expected value of output from that node\\
\\
intuition behind this approach:
\begin{itemize}
    \item can be seen as form of dataset augmentation, constructing new inputs by multiplying by noise
    \item trains nodes to perform well regardless of other hidden units in model, regularizes each hidden node to work well in many different contexts
\end{itemize}
has been shown to be more effective than other standard computationally inexpensive regularizers, e.g. weight penalties \\
can be combined with other forms of regularization for further improvement

\subsection{Early Stopping}
%overfitting graph
When training large models, training error decreases steadily, generalization error starts increasing again after certain amount of time \arrow model starts to overfit \\
idea: stop training at lowest point of in generalization error as estimated by set of examples that is not used for training \\
since test set is defined as set, that does not influence any model decisions at all, we call this set of examples the validation set \\
algorithm:
\begin{itemize}
    \item keep separate set of parameters, $\theta_{min}$ for which the validation set error was minimal so far
    \item run training algorithm for $n$ steps
    \item if current validation set error lower than previous minimum, copy current parameters $\theta_{min} \leftarrow \theta_k$
    \item if validation set error larger than minimum $p$ (for patience) times in a row, exit and return $\theta_{min}$
\end{itemize}

Probably most commonly used form of regularization in deep learning, due to effectiveness and simplicity

\section{Convolutional Neural Networks}
specialized kind of neural network for processing data that has known grid-like topology, e.g. images
\subsection{The Convolution Operation}
General:
\begin{flalign*}
    (f \ast g)(t) := \int f(\tau)g(t - \tau)d\tau = \int f(t - \tau)g(\tau)d\tau
\end{flalign*}

In our case we have real-valued functions defined on 2D grid indices $\in \mathbb{Z}^2$, so for convenience, we define our convolution operation (which is actually a cross-correlation) as:



The functions in our case are real-valued and represent the data of finite-sized 2D arrays. They are therefore defined to have finite support on $\mathbb{Z}^2$:
\begin{flalign*}
    & A(i,j) = \begin{cases*}
        A[i,j] & for $(i,j) \in {0, \ldots , m-1}\times{0, \ldots, n-1}$\\
        0 & otherwise
    \end{cases*}&&
\end{flalign*}
where $A$ is an array of shape $(m, n)$

For convenience, we define this to be our convolution operation (which is formally a cross-correlation):
\begin{flalign*}
    & F(i,j) = \sum_{m=-\infty}^{\infty}\sum_{n=-\infty}^{\infty} I(i+m, j+n)K(m,n) &&\\
    \intertext{if $K$ has shape $(k,l)$:}
    & F(i,j) = \sum_{m=0}^{k-1}\sum_{n=0}^{l-1} I(i+m, j+n)K(m,n) &&
\end{flalign*}
$F$: output/feature map, $I$: input, $K$: kernel
%convolution example

%full, valid, same convolution?

\subsection{Architecture of a convolutional layer}
\begin{enumerate}
    \item Convolutions with several $n$ kernels/filters in parallel produce $n$ feature maps
    \item Application of (element-wise) activation function to each feature map
    \item Pooling
\end{enumerate}
\subsubsection*{Pooling}
pooling function is applied to activations \\
aggregtes/summarizes nearby outputs \\
e.g. max pooling reports maximum activation within rectangular neighborhood

\subsection{Motivations}
\subsection*{Pooling}
Introduction of invariance \\
for some features, we only care if it is present or not \\
% image max pooling
e.g. max pooling (width 3), shifting input to the right only causes half of the pooled values to change \arrow invariance to small translations \\

% image feature pooling
pooling outputs of different kernels/filters can introduce more invariance, e.g. if pooling over filters that detect different orientations of object

\subsection*{Downsampling}
% image pooling with strides
pooling with strides: \\
pooling summarizes responses over neighborhood, using a stride size $> 1$ can help reduce representation size\\
\\
%image convolution with strides
strided convolution: \\
skip over kernel positions to reduce computational cost, equivalent to downsampling output of full convolution function
\begin{flalign*}
    & F(i,j) = \sum_{m}\sum_{n} I(i\cdot s + m, j\cdot s + n) K(m,n) &&
\end{flalign*}
with stride size $s$\\
\\
Downsampling is essential for handling inputs of variable size, e.g. different image sizes \\
MLPs used for classifying the information extracted by the convolution layers require fixed size input\\
This is usually done by varying the stride size such that an output of the expected size is produced

\subsubsection*{Sparse Connectivity}
Not all input nodes are connected to all output nodes for kernels smaller than input
\begin{itemize}
    \item Can lead to dramatic reduction in computational requirements (storage of parameters, arithmetic operations), especially for large input arrays (e.g. large images)
    \item Meaningful features can also be detected with kernels small compared to entire array size (e.g. edges in pictures)
\end{itemize}

\subsubsection*{Parameter sharing}
Kernel parameters are reused for each output node, compared to MLP where each node has its own weights (row in layer weight matrix matrix)
\begin{itemize}
    \item layer has equivariance to translation, i.e. translating the input in certain direction also causes translation of output in same direction \\
    \arrow useful for feature detection, e.g. it makes sense to reuse the same parameters to detect an edge regardless of position in an image
\end{itemize}

\subsection*{Neuroscientific basis}
Research shows that primary visual cortex in mammals is arranged in spatial map \\
has two-dimensional structure, mirroring structure of image in retina \\
these and other findings inspired CNNs and they are perhaps one of the greatest success stories of biologically inspired artificial intelligence


\end{document}