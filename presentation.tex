\documentclass{beamer}

\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{relsize}
\usepackage{float}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{dirtytalk}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{pgffor}
\usepackage[export]{adjustbox}
\usepackage{url}

\setlength{\parindent}{0pt}

\def\noval{}

\DeclarePairedDelimiter{\br}{(}{)}
\DeclarePairedDelimiter{\brr}{[}{]}
\DeclarePairedDelimiter{\brs}{\{}{\}}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\makeatletter
\let\oldbr\br
\def\br{\@ifstar{\oldbr}{\oldbr*}}
\makeatother

\makeatletter
\let\oldbrr\brr
\def\brr{\@ifstar{\oldbrr}{\oldbrr*}}
\makeatother

\makeatletter
\let\oldbrs\brs
\def\brs{\@ifstar{\oldbrs}{\oldbrs*}}
\makeatother

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
\makeatother

\makeatletter
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\makeatletter
\let\oldlog\log
\def\log{\@ifstar\@log\@@log}
\def\@log#1{\oldlog\br{#1}}
\def\@@log#1{\oldlog#1}
\makeatother

\makeatletter
\newcommand{\set}[2]{
    \def\@delim{\;\@ifstar{\middle|}{:}\;}
    \ifx\noval#2\noval%
        \left\{#1\right\}
    \else
        \left\{#1\@delim#2\right\}
    \fi
}
\makeatother

\newcommand{\E}[2][]{%
    \ifx\noval#1\noval%
        \mathbb{E}\brr{#2}
    \else
        \mathbb{E}_{#1}\brr{#2}
    \fi
}
\newcommand{\var}[2][]{%
    \ifx\noval#1\noval%
        \text{Var}\br{#2}
    \else
        \text{Var}_{#1}\br{#2}
    \fi
}
\newcommand{\cov}[2][]{%
    \ifx\noval#1\noval%
        \text{Cov}\br{#2}
    \else
        \text{Cov}_{#1}\br{#2}
    \fi
}

\newcommand{\est}[1]{\hat{#1}_m}
\newcommand{\bias}[1]{\text{bias}\br{#1}}


\newcommand{\arrow}{$\rightarrow\;$}
\newcommand{\smallmin}{\scalebox{0.8}{-}}
\newcommand{\relu}{\text{ReLU}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\text{T}}
\renewcommand{\v}{\bm}
\renewcommand{\c}[2]{\left(#1\;\middle|\;#2\right)}
\renewcommand{\det}[1]{\text{det}\br{#1}}
\newcommand{\ml}[1]{\mathlarger{#1}\,}
\newcommand{\dkl}[2]{D_{\text{KL}}(#1\;\Vert\;#2)}
\renewcommand{\S}{\mathbb{S}}
\newcommand{\func}[3]{#1:#2\rightarrow#3}
\newcommand{\p}[1]{p_\text{#1}}
\renewcommand{\k}[2]{#1^{(#2)}}


\begin{document}
\begin{frame}
    \frametitle{A broad Definition of Machine Learning}
    
        \say{A computer program is said to learn from experience E 
        with respect to some class of tasks T and performance measure P, 
        if its performance at tasks in T, as measured by P, improves with
        experience.} \cite{mitchell97}
    
\end{frame}
\begin{frame}
    \frametitle{Types of tasks T}
    Generally, task is to learn some highly complex function
    \begin{flalign*}
        \func{f}{\R^n}{\R^m}, f(x) = y
    \end{flalign*}
    Common tasks:\\
    \vspace{1em}
    \begin{minipage}{0.4\textwidth}
        \begin{itemize}
            \item Classification
            \item Regression
        \end{itemize}
    \end{minipage}    
    \begin{minipage}{0.55\textwidth}
        \begin{itemize}
            \item Structured prediction
            \item Probability density estimation
        \end{itemize}
    \end{minipage}    
    \vspace{1em}
    
    \hspace{-1em}
    \begin{minipage}[t]{0.59\textwidth}
        \begin{figure}
            \includegraphics[height=2.4cm]{img/density_estimation.png}
            \caption{Density estimate based on samples (red) drawn from a mixed Gaussian (blue) \cite{density_estimation}}
        \end{figure}
    \end{minipage}
    \hspace{0.5em}
    \begin{minipage}[t]{0.39\textwidth}
        \begin{figure}
            \includegraphics[height=2.4cm]{img/structured prediction.pdf}
            \caption{Sequence tagging as an example of structured prediction \cite{structured_prediction}}
        \end{figure}
    \end{minipage}
    \hspace{-2em}

\end{frame}

\begin{frame}
    \frametitle{Types of experience E}
    Learning usually through observing dataset of examples 
    \begin{itemize}
        \item Unsupervised learning: learn useful properties of dataset
        \item Supervised learning: given examples $(x_i, y_i)$, learn $p\c{y}{x}$
        \item Reinforcement learning: learn from interaction with environment
    \end{itemize}
    
    \begin{figure}
        \hspace{-2em}
        $\vcenter{\hbox{\includegraphics[width=0.35\textwidth]{img/unsupervised_learning.png}}}$
        $\vcenter{\hbox{\includegraphics[width=0.24\textwidth]{img/supervised_learning.png}}}$
        $\vcenter{\hbox{\includegraphics[width=0.4\textwidth]{img/reinforcement_learning.jpg}}}$
        \hspace{-2em}
        \caption{Examples for unsupervised, supervised and reinforcement learning \cite{unsupervised}, \cite{supervised} \cite{reinforcement}}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Different Types of Error}
    \begin{figure}[H]
        \includegraphics[width=0.8\textwidth]{img/deep learning/test_generalization_error.pdf}
        \caption{Typical relationship between capacity and error \cite{textbook}}
    \end{figure}
    
    \begin{itemize}
        \item Capacity: ability of model to fit function to be learned
        \item Training error: error on set of training examples
        \item Generalization error: real-world error on new data
    \end{itemize}
    \arrow Goal: low training \textbf{and} generalization error
\end{frame}

\begin{frame}
    \frametitle{Deep Learning and Neural Networks}
    \begin{itemize}
        \item \textbf{Network}: learned function $f(x)$ can be described by network of nodes
        \item \textbf{Neural}: graph nodes play role analogous to biological neuron
        \item \textbf{Deep} Learning: \say{depth} of networks
    \end{itemize}

    \begin{figure}
        \includegraphics[width=0.4\textwidth]{img/dropout/neural_network_visualization.pdf}
        \caption{Simple Deep Neural Network \cite{dropout}\textsuperscript{*}}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Deep Feedforward Networks (MLPs)}
    Typically have layer structure:
    \begin{figure}
        \includegraphics[width=0.8\textwidth]{img/dropout/neural_network_visualization_annotated.pdf}
        \caption{Layer Structure of Deep Feedforward Networks \cite{dropout}\textsuperscript{*}}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{How does Deep Learning Work?}
    \begin{figure}
        \begin{minipage}[c]{0.67\textwidth}
          \includegraphics[width=\textwidth]{img/deep learning/deep_learning_representation.pdf}
        \end{minipage}\hfill
        \begin{minipage}[c]{0.3\textwidth}
          \caption{
            Deep Networks solve difficult tasks by building complex concepts out of simpler concepts \cite{textbook}
          } \label{fig:03-03}
        \end{minipage}
      \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Deep Learning as Extension of Linear Models}
    Hidden layers: kernel $\phi(x; \theta)$, output layer: regression
    \begin{equation*}
        f(x) = \k{f}{N}(\ldots \k{f}{1}(x)) = \k{W}{N}\phi(x; \theta) + \k{b}{N}
    \end{equation*}
    
    \begin{minipage}[b]{0.49\textwidth}
        \begin{figure}
            \includegraphics[width=0.8\textwidth]{img/dropout/network_kernel.pdf}
            \caption{Deep Network as linear layer with kernel \cite{dropout}\textsuperscript{*}}
        \end{figure}
    \end{minipage}
    \begin{minipage}[b]{0.49\textwidth}
        \begin{figure}
            \includegraphics[width=\textwidth]{img/kernel trick.pdf}
            \caption{mapping with $\phi$ makes input linearly separable \cite{kernel}\textsuperscript{*}}
        \end{figure}
    \end{minipage}
    
    \arrow Deep Learning: learn kernel and regression parameters
\end{frame}

\begin{frame}
    \frametitle{Universal Approximation Theorems (UAT)}
    \begin{itemize}
        \item Prove that it is possible to approximate functions arbitrarily well with neural networks
        \item Requirement: non-linearity e.g. $\relu(z) = \max(0, z)$
        \item Shallow network with activation is universal approximator, can however require exponential number of hidden units %exponential in input dimension
    \end{itemize}    

    \begin{figure}
        \includegraphics[width=0.5\textwidth]{img/relu_approximation.pdf}
        \caption{Approximation of $y(x) = x^3+x^2-x-1$ using 6 ReLU nodes}
    \end{figure}


\end{frame}

\begin{frame}
    \frametitle{Shallow Network with ReLU as Universal Approximator}
    
    
    \begin{minipage}[t]{0.4\textwidth}
        \vspace{-0.5cm}
        \begin{flalign*}
            y(x) = & -\relu(-5(x+1.5)) &&\\
            & -\relu(-(x+1)) &&\\
            & -\relu(x+1) &&\\
            &+ \relu(x-0.2) &&\\
            &+ \relu(3(x-0.6)) &&\\
            &+ \relu(3(x-1))&&
        \end{flalign*}

        \begin{flalign*}          
            \k{a}{1} &= \text{ReLU}\br{
            \begin{bsmallmatrix*}
                \smallmin 5 \ \\
                \smallmin 1 \\
                 1 \\
                 1 \\
                 3 \\
                 3 
            \end{bsmallmatrix*} \, x + 
            \begin{bsmallmatrix*}
                \smallmin 7.5 \ \\
                \smallmin 1 \\
                1 \\
                \smallmin 0.2 \\
                \smallmin 1.8 \\
                -3 
            \end{bsmallmatrix*}} &&\\
            &&&\\
            y(x) &= \begin{bsmallmatrix*}
                \smallmin 1,& \smallmin 1,& \smallmin 1,& 1,& 1,& 1
            \end{bsmallmatrix*} \, \k{a}{1} &&
        \end{flalign*}
    \end{minipage}
    \begin{minipage}[t]{0.4\textwidth}
        \begin{figure}
            \includegraphics[width=\textwidth]{img/relu_approximation.pdf}
            \includegraphics[width=0.9\textwidth]{img/relu_approximation_network.pdf}
            \caption{Shallow network and its ouptut}
        \end{figure}
    \end{minipage}
\end{frame}
\begin{frame}
    \frametitle{UAT - Intuition for Advantage of Deeper Networks}
    \begin{itemize}
        \item Stacked hidden layers with activation $g(z) = \abs{z}$ \say{fold} space many times along hyperplanes defined by the $\k{W}{k}, \k{b}{k}$
        \item Function on folded space creates complex repeating pattern on input space
    \end{itemize}
    \begin{figure}
        \includegraphics[width=0.9\textwidth]{img/deep learning/activation_folding.pdf}
        \caption{Two hidden layers with $\abs{z}$-activation fold input space twice \cite{textbook}}
    \end{figure}
    \arrow Important: UAT guarantees ability to approximate, not learn!
\end{frame}

\begin{frame}
    \frametitle{Maximum Likelihood Estimator} 
    \begin{itemize}
        \item Empirical distribution:    
        \begin{equation*}
            p(X = x) = \frac{1}{m} \sum_{i=1}^{m} \delta(x - x_i)
        \end{equation*}
        \item Kullback-Leibler divergence:
        \begin{equation*}
            \dkl{p}{q} := \E[X \sim p]{\log{\frac{p(X=x)}{q(X=x)}}} \geq 0
        \end{equation*}
        \arrow Measure of how different distributions $p$ and $q$ are
    \end{itemize}
    \begin{figure}
        \begin{minipage}[b]{0.35\textwidth}
          \includegraphics[width=\linewidth]{img/deep learning/kl_divergence.pdf}
        \end{minipage}\hfill
        \begin{minipage}[b]{0.6\textwidth}
          \caption{
            Result of minimizing the KL-divergence between a mixed and single Gaussian \cite{textbook}
          }
        \end{minipage}
      \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Maximum Likelihood Estimator}
    
    Prerequisites:
    \begin{itemize}
        \item i.i.d. assumption \arrow view individual examples $x_i \in \mathbb{X}=\set{x_1, \ldots, x_m}{}$ as being drawn from $\p{data}(x)$
        \item $\p{model}(x; \theta)$: family of probability distributions, indexed by $\theta$
        \item $\hat{p}_{\text{data},m}$: empirical distribution defined by $\mathbb{X}$
    \end{itemize}

    \vspace{0.5cm}
    Maximum Likelihood Estimator for $\theta$:
    \begin{align*}
         \k{\theta}{ML}_m 
        :=& \argmax_\theta \prod_{i=1}^{m} \p{model}(x_i; \theta) \\
        =& \argmin_\theta \E[x\sim \hat{p}_{\text{data},m}]{-\log{\p{model}(x; \theta)}} \\
        =& \argmin_\theta \dkl{\hat{p}_{\text{data}, m}}{\p{model}}
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Maximum Likelihood Estimator}
    Properties:
    \begin{itemize}
        \item $\k{\theta}{ML}_m$ minimizes dissimilarity between observed distribution $\hat{p}_{\text{data},m}$ and model distribution $\p{model}$
        \item Fastest convergence to true parameter $\theta^\ast$
    \end{itemize}
    \vspace{0.5cm}
    Generalization to conditional probabilities $p\c{y}{x}$:
    \begin{flalign*}
        \theta^{(ML)}_{m} := \argmin_\theta \frac{1}{m} \sum_{i=1}^{m} - \log{\p{model}\c{x_i}{y_i; \theta}}
    \end{flalign*}

    Connection between MLE and UAT:
    \begin{itemize}
        \item MLE: framework for fitting $\p{model}$ to $\p{data}$ via $\theta$
        \item UAT: guarantees that model can approximate $\p{data}$
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Cost Functions and Gradient Descent}
    
    Cost functions:
    \begin{itemize}
        \item Often sum over per-example loss: $J(\theta) = \frac{1}{m} \sum_{i=1}^
        {m}L(x_i, y_i; \theta)$
        \vspace{0.2cm}
        \item MLE: $L(x, y, \theta) = -\log{\p{model}\c{y}{x; \theta}}$
    \end{itemize}
    
    \vspace{0.4cm}
    
    Gradient descent:
    \begin{itemize}
        \item Iteratively minimize $J$: $\ \theta_{k+1} = \theta_k - \epsilon \nabla_\theta J(\theta_k)$
        \item Problem: computational cost of one step $\propto m$
    \end{itemize}

    \begin{figure}
        \includegraphics[width=0.6\textwidth]{img/gradient_descent.png}
        \caption{Visualization of gradient descent \cite{arnold}}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Stochastic Gradient Descent and Momentum}
    Stochastic Gradient Descent:
    \vspace{-0.2cm}
    \begin{equation*}
        \nabla_\theta J(\theta) \approx g = \frac{1}{m^\prime} \sum_{i=1}^{m^\prime} \nabla_\theta L(x_i, y_i; \theta), \quad m^\prime \ll m
    \end{equation*}\\
    \vspace{-0.2cm}
    \arrow Use randomly drawn minibatch of examples to estimate $\nabla_\theta J$

    \begin{minipage}[b]{0.51\textwidth}
        \vspace{0.4cm}
        Momentum:
        \vspace{-0.3cm}
        \begin{flalign*}
            & v_{k+1} = \alpha v_k - \epsilon \nabla_\theta J(\theta_{k}) &&\\
            & \theta_{k+1} = \theta_k + v_{k+1} &&\\
            & \alpha \in [0,1)&&
        \end{flalign*}
        \vspace{-0.7cm}
        \begin{itemize}
            \item Weighted average stabilizes gradient
            \item Analogy: particle with friction, velocity $v$
        \end{itemize}
    \end{minipage}
    \begin{minipage}[b]{0.47\textwidth}
        \begin{figure}
            \includegraphics[width=0.7\linewidth]{img/deep learning/momentum.pdf}
            \vspace{-0.2cm}
            \hspace{3.4em}
            \caption{Comparison of GD\\ \textbf{\textcolor{red}{with}} and \textbf{without} momentum \cite{textbook}}
            \vspace{-0.7cm}
        \end{figure}
    \end{minipage}
\end{frame}

\begin{frame}
    \frametitle{Backpropagation - General Formulation}
    Algorithm for computing $\frac{\partial F}{\partial X}, \ X \in \set{A, \ldots, F}{}$:
        \begin{itemize}
            \item Start at node $F$, $\frac{\partial F}{\partial F} = 1$
            \item Propagate backwards along graph, \textbf{re-using partial derivatives} of previous nodes, e.g.: $\frac{\partial F}{\partial C} = \frac{\partial F}{\partial E}\frac{\partial E}{\partial C}$
        \end{itemize}
    
    \begin{figure}
        \centering
        \foreach \n in {1,...,6}{\only<\n>{\includegraphics[width=5cm]{img/backprop/backprop_\n.pdf}}}
        \caption{Example of Backpropagation through a graph \cite{dag}\textsuperscript{*}}
    \end{figure}
    \arrow Deep/complex graphs: dramatic speedup compared to recomputing all intermediate $\frac{\partial F}{\partial X}$ for each node
\end{frame}

\begin{frame}
    \frametitle{Backpropagation for MLPs}
    
    \begin{minipage}{0.4\textwidth}
        Feedforward:
        \begin{flalign*}
            & \k{z}{k} = \k{W}{k} \k{a}{k-1} + \k{b}{k} &&\\ 
            & \k{a}{k} = \k{g}{k}(\k{z}{k}) &&
        \end{flalign*}
    \vspace{0.5cm}\\
    Backpropagation:\\
    (let $\br{\frac{\partial u}{\partial v}}_{ij} = \frac{\partial u_j}{\partial v_i}$)    
    \end{minipage}
    \begin{minipage}{0.57\textwidth}
        \begin{figure}
            \includegraphics[width=0.9 \linewidth, right]{img/dropout/backprop_mlp.pdf}
            \caption{Visualization of a MLP \cite{dropout}\textsuperscript{*}}
        \end{figure}
    \end{minipage}
    \vspace{-0.3cm}
    \begin{flalign*}
        & \frac{\partial L}{\partial \k{z}{k}} = \frac{\partial \k{z}{k+1}}{\partial \k{z}{k}}\frac{\partial L}{\partial \k{z}{k+1}} =  \frac{\partial \k{g}{k}(\k{z}{k})}{\partial\k{z}{k}} {\k{W}{k+1}}^T \frac{\partial L}{\partial \k{z}{k+1}} &&\\
        & \frac{\partial L}{\partial \k{W}{k}_{ij}} = \frac{\partial L}{\partial \k{z}{k}_i} \k{a}{k-1}_j, \quad \frac{\partial L}{\partial \k{b}{k}_i}
        = \frac{\partial L}{\partial \k{z}{k}_i} &&
    \end{flalign*}
    \arrow During feedforward    store the $\k{z}{k}, \k{a}{k}$, during backprop $\frac{\partial L}{\partial \k{z}{k}}$
\end{frame}

\begin{frame}
    \frametitle{Regularization Techniques}
    
    \begin{itemize}
        \item Parameter norm constraints (e.g. $\Omega(\theta) = \norm{\theta}_2$):\\
        \vspace{0.15cm}
        implicit: minimize $\tilde{J}(\theta) = J(\theta) + \alpha \Omega(\theta), \quad \alpha \geq 0$ \\
        \vspace{0.15cm}
        explicit: project $\theta$ back after gradient step, s.t. $\Omega(\theta) < c$
        \item Dataset augmentation: generate new examples by transforming existing ones
        \item Early stopping: stop training as soon as generalization error starts increasing again 
    \end{itemize}
    \begin{figure}
        \hspace{-2em}
        $\vcenter{\hbox{\includegraphics[width=0.24\textwidth]{img/deep learning/norm_constraint.pdf}}}$
        $\vcenter{\hbox{\includegraphics[width=0.24\textwidth]{img/deep learning/dataset_augmentation.pdf}}}$
        $\vcenter{\hbox{\includegraphics[width=0.55\textwidth]{img/deep learning/early_stopping.pdf}}}$
        \hspace{-2em}
        \caption{Illustration of parameter norm constraints, dataset augmentation and early stopping \cite{textbook}\textsuperscript{*}}
    \end{figure}
\end{frame}
\begin{frame}
    \frametitle{Dropout}
    Train ensemble of all subnetworks of base network:
    \begin{itemize}
        \item Randomly remove non-output nodes for each training example
        \item MLP: $\k{a}{k}_n = \k{\mu}{k}_n \k{g}{k}(\k{z}{k})_n\,, \ \k{\mu}{k}_n$ Bernoulli-distributed
        \item For non-training inputs return average of ensemble results or
        equivalently scale outgoing weights by $p(\k{\mu}{k}_n = 1)$
    \end{itemize}
    \vspace{-0.5cm}
    \begin{minipage}{0.49\textwidth}
        \vspace{0.3cm}
        Why does it work:
        \begin{itemize}
            \item Regularizes nodes to work well in different contexts
            \item Form of dataset augmentation
            \arrow create new inputs by injecting noise
        \end{itemize}
    \end{minipage}
    \hspace{1em}
    \begin{minipage}{0.4\textwidth}
        \vspace{1cm}
        \begin{figure}
            \includegraphics[width=0.8\linewidth]{img/dropout/dropout.pdf}
            
            \caption{Subnetwork formed by randomly sampling $\mu$ \cite{dropout}\textsuperscript{*}}
        \end{figure}
    \end{minipage}
\end{frame}

\begin{frame}
    \frametitle{Convolutional Neural Networks}
\end{frame}

\begin{thebibliography}{99}
    \bibitem{textbook}
    Goodfellow et al.
    \bibitem{mitchell97}
    Mitchell, T. M. (1997). Machine Learning.
    \bibitem{dropout}
    Dropout: A Simple Way to Prevent Neural Networks from Overfitting
    \bibitem{kernel}
    Pham, Trong-Ton. (2010). MODELE DE GRAPHE ET MODELE DE LANGUE POUR LA RECONNAISSANCE DE SCENES VISUELLES.
    \bibitem{arnold}
    \url{https://ludovicarnold.com/teaching/optimization/gradient-descent/}
    \bibitem{dag}
    \url{https://commons.wikimedia.org/w/index.php?curid=3211391}
    \bibitem{supervised}
    \url{https://corochann.com/mnist-dataset-introduction-532/}
    \bibitem{unsupervised}
    \url{https://cloud.google.com/discover/what-is-unsupervised-learning?hl=en}
    \bibitem{reinforcement}
    \url{https://commons.wikimedia.org/w/index.php?curid=141214663}
    \bibitem{regression}
    \url{https://www.researchgate.net/figure/Linear-regression-of-firm-level-log-average-loan-amount-y-axis-on-log-total-assets_fig4_339863947}
    \bibitem{density_estimation}
    \url{https://commons.wikimedia.org/w/index.php?curid=24309466}
    \bibitem{structured_prediction}
    \url{https://en.wikipedia.org/wiki/Structured_prediction}
\end{thebibliography}
\end{document}