\documentclass{beamer}

\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{relsize}
\usepackage{float}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{dirtytalk}
\usepackage{wrapfig}

\setlength{\parindent}{0pt}

\def\noval{}

\DeclarePairedDelimiter{\br}{(}{)}
\DeclarePairedDelimiter{\brr}{[}{]}
\DeclarePairedDelimiter{\brs}{\{}{\}}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\makeatletter
\let\oldbr\br
\def\br{\@ifstar{\oldbr}{\oldbr*}}
\makeatother

\makeatletter
\let\oldbrr\brr
\def\brr{\@ifstar{\oldbrr}{\oldbrr*}}
\makeatother

\makeatletter
\let\oldbrs\brs
\def\brs{\@ifstar{\oldbrs}{\oldbrs*}}
\makeatother

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
\makeatother

\makeatletter
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\makeatletter
\let\oldlog\log
\def\log{\@ifstar\@log\@@log}
\def\@log#1{\oldlog\br{#1}}
\def\@@log#1{\oldlog#1}
\makeatother

\makeatletter
\newcommand{\set}[2]{
    \def\@delim{\;\@ifstar{\middle|}{:}\;}
    \ifx\noval#2\noval%
        \left\{#1\right\}
    \else
        \left\{#1\@delim#2\right\}
    \fi
}
\makeatother

\newcommand{\E}[2][]{%
    \ifx\noval#1\noval%
        \mathbb{E}\brr{#2}
    \else
        \mathbb{E}_{#1}\brr{#2}
    \fi
}
\newcommand{\var}[2][]{%
    \ifx\noval#1\noval%
        \text{Var}\br{#2}
    \else
        \text{Var}_{#1}\br{#2}
    \fi
}
\newcommand{\cov}[2][]{%
    \ifx\noval#1\noval%
        \text{Cov}\br{#2}
    \else
        \text{Cov}_{#1}\br{#2}
    \fi
}

\newcommand{\est}[1]{\hat{#1}_m}
\newcommand{\bias}[1]{\text{bias}\br{#1}}


\newcommand{\arrow}{$\rightarrow\;$}
\newcommand{\smallmin}{\scalebox{0.8}{-}}
\newcommand{\relu}{\text{ReLU}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\text{T}}
\renewcommand{\v}{\bm}
\renewcommand{\c}[2]{\left(#1\;\middle|\;#2\right)}
\renewcommand{\det}[1]{\text{det}\br{#1}}
\newcommand{\ml}[1]{\mathlarger{#1}\,}
\newcommand{\dkl}[2]{D_{\text{KL}}(#1\;\Vert\;#2)}
\renewcommand{\S}{\mathbb{S}}
\newcommand{\func}[3]{#1:#2\rightarrow#3}
\newcommand{\p}[1]{p_\text{#1}}
\renewcommand{\k}[2]{#1^{(#2)}}


\begin{document}
\begin{frame}
    \frametitle{A broad Definition of Machine Learning}
    
        \say{A computer program is said to learn from experience E 
        with respect to some class of tasks T and performance measure P, 
        if its performance at tasks in T, as measured by P, improves with
        experience.} \cite{mitchell97}
    
\end{frame}
\begin{frame}
    \frametitle{Types of tasks T}
    Generally, task is to learn some highly complex function
    \begin{flalign*}
        \func{f}{\R^n}{\R^m}, f(x) = y
    \end{flalign*}
    Common tasks:
    \begin{itemize}
        \item Classification
        \item Regression
        \item Structured output
        \item Probability density estimation
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Types of experience E}
    Learning usually through observing dataset of examples 
    \begin{itemize}
        \item Unsupervised learning: learn useful properties of dataset
        \item Supervised learning: given examples $(x_i, y_i)$, learn $p\c{y}{x}$
        \item Reinforcement learning: learn from interaction with environment
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Performance Measure P}
    Goal: find a function $J(\theta)$ which estimates performance on unseen data and can be optimized well\\
    \vspace{0.2cm}
    $J(\theta)$: Cost function\\
    \vspace{0.2cm}
    $\theta$: model parameters
\end{frame}

\begin{frame}
    \frametitle{Different Types of Error}
    \begin{figure}[H]
        \includegraphics[width=0.8\textwidth]{img/deep learning/test_generalization_error.pdf}
        \caption{Typical relationship between capacity and error \cite{textbook}}
    \end{figure}
    
    \begin{itemize}
        \item Capacity: ability of model to fit function to be learned
        \item Training error: error on set of training examples
        \item Generalization error: real-world error on new data
    \end{itemize}
    \arrow Goal: low training \textbf{and} generalization error
\end{frame}

\begin{frame}
    \frametitle{Deep Learning and Neural Networks}
    \begin{itemize}
        \item \textbf{Network}: learned function $f(x)$ can be described by network of nodes
        \item \textbf{Neural}: graph nodes play role analogous to biological neuron
        \item \textbf{Deep} Learning: \say{depth} of networks
    \end{itemize}

    \begin{figure}
        \includegraphics[width=0.3\textwidth]{img/dropout/neural_network_visualization.pdf}
        \caption{Simple Deep Neural Network \cite{dropout}\textsuperscript{*}}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Deep Feedforward Networks}
    Typically have layer structure:
    \begin{figure}
        \includegraphics[width=0.8\textwidth]{img/dropout/neural_network_visualization_annotated.pdf}
        \caption{Layer Structure of Deep Feedforward Networks \cite{dropout}\textsuperscript{*}}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{How does Deep Learning Work?}
    \begin{figure}
        \begin{minipage}[c]{0.67\textwidth}
          \includegraphics[width=\textwidth]{img/deep learning/deep_learning_representation.pdf}
        \end{minipage}\hfill
        \begin{minipage}[c]{0.3\textwidth}
          \caption{
            Deep Networks solve difficult tasks by building complex concepts out of simpler concepts \cite{textbook}
          } \label{fig:03-03}
        \end{minipage}
      \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Deep Learning as Extension of Linear Models}
    Hidden layers: kernel $\phi(x; \theta)$, output layer: regression
    \begin{equation*}
        f(x) = \k{f}{N}(\ldots \k{f}{1}(x)) = \k{W}{N}\phi(x; \theta) + \k{b}{N}
    \end{equation*}
    
    \begin{minipage}[b]{0.49\textwidth}
        \begin{figure}
            \includegraphics[width=0.8\textwidth]{img/dropout/network_kernel.pdf}
            \caption{Deep Network as linear layer with kernel \cite{dropout}\textsuperscript{*}}
        \end{figure}
    \end{minipage}
    \begin{minipage}[b]{0.49\textwidth}
        \begin{figure}
            \includegraphics[width=\textwidth]{img/kernel trick.pdf}
            \caption{mapping with $\phi$ makes input linearly separable \cite{kernel}\textsuperscript{*}}
        \end{figure}
    \end{minipage}
    
    \arrow Deep Learning: learn kernel and regression parameters
\end{frame}

\begin{frame}
    \frametitle{Universal Approximation Theorems (UAT)}
    \begin{itemize}
        \item Prove that it is possible to approximate functions arbitrarily well with neural networks
        \item Requirement: non-linearity e.g. $\relu(z) = \max(0, z)$
        \item Shallow network with activation is universal approximator, can however require exponential number of hidden units %exponential in input dimension
    \end{itemize}    

    \begin{figure}
        \includegraphics[width=0.5\textwidth]{img/relu_approximation.pdf}
        \caption{Approximation of $y(x) = x^3+x^2-x-1$ using 6 ReLU nodes}
    \end{figure}


\end{frame}

\begin{frame}
    \frametitle{Shallow Network with ReLU as Universal Approximator}
    
    
    \begin{minipage}[t]{0.4\textwidth}
        \vspace{-0.5cm}
        \begin{flalign*}
            y(x) = & -\relu(-5(x+1.5)) &&\\
            & -\relu(-(x+1)) &&\\
            & -\relu(x+1) &&\\
            &+ \relu(x-0.2) &&\\
            &+ \relu(3(x-0.6)) &&\\
            &+ \relu(3(x-1))&&
        \end{flalign*}

        \begin{flalign*}          
            \k{a}{1} &= \text{ReLU}\br{
            \begin{bsmallmatrix*}
                \smallmin 5 \ \\
                \smallmin 1 \\
                 1 \\
                 1 \\
                 3 \\
                 3 
            \end{bsmallmatrix*} \, x + 
            \begin{bsmallmatrix*}
                \smallmin 7.5 \ \\
                \smallmin 1 \\
                1 \\
                \smallmin 0.2 \\
                \smallmin 1.8 \\
                -3 
            \end{bsmallmatrix*}} &&\\
            &&&\\
            y(x) &= \begin{bsmallmatrix*}
                \smallmin 1,& \smallmin 1,& \smallmin 1,& 1,& 1,& 1
            \end{bsmallmatrix*} \, \k{a}{1} &&
        \end{flalign*}
    \end{minipage}
    \begin{minipage}[t]{0.4\textwidth}
        \begin{figure}
            \includegraphics[width=\textwidth]{img/relu_approximation.pdf}
            \includegraphics[width=0.9\textwidth]{img/relu_approximation_network.pdf}
            \caption{Shallow network and its ouptut}
        \end{figure}
    \end{minipage}
\end{frame}
\begin{frame}
    \frametitle{UAT - Intuition for Advantage of Deeper Networks}
    \begin{itemize}
        \item Stacked hidden layers with activation $g(z) = \abs{z}$ \say{fold} space many times along hyperplanes defined by the $\k{W}{k}, \k{b}{k}$
        \item Function on folded space creates complex repeating pattern on input space
    \end{itemize}
    \begin{figure}
        \includegraphics[width=0.9\textwidth]{img/deep learning/activation_folding.pdf}
        \caption{Two hidden layers with $\abs{z}$-activation fold input space twice \cite{textbook}}
    \end{figure}
    \arrow Important: UAT guarantees ability to approximate, not learn!
\end{frame}

\begin{frame}
    \frametitle{Maximum Likelihood Estimator} 
    \begin{itemize}
        \item Empirical distribution:    
        \begin{equation*}
            p(X = x) = \frac{1}{m} \sum_{i=1}^{m} \delta(x - x_i)
        \end{equation*}
        \item Kullback-Leibler divergence:
        \begin{equation*}
            \dkl{p}{q} := \E[X \sim p]{\log{\frac{p(X=x)}{q(X=x)}}} \geq 0
        \end{equation*}
        \arrow Measure of how different distributions $p$ and $q$ are
    \end{itemize}
    \begin{figure}
        \begin{minipage}[b]{0.35\textwidth}
          \includegraphics[width=\linewidth]{img/deep learning/kl_divergence.pdf}
        \end{minipage}\hfill
        \begin{minipage}[b]{0.6\textwidth}
          \caption{
            Result of minimizing the KL-divergence between a mixed and single Gaussian \cite{textbook}
          }
        \end{minipage}
      \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Maximum Likelihood Estimator}
    
    Prerequisites:
    \begin{itemize}
        \item i.i.d. assumption \arrow view individual examples $x_i \in \mathbb{X}=\set{x_1, \ldots, x_m}{}$ as being drawn from $\p{data}(x)$
        \item $\p{model}(x; \theta)$: family of probability distributions, indexed by $\theta$
        \item $\hat{p}_{\text{data},m}$: empirical distribution defined by $\mathbb{X}$
    \end{itemize}

    \vspace{0.5cm}
    Maximum Likelihood Estimator for $\theta$:
    \begin{align*}
         \k{\theta}{ML}_m 
        :=& \argmax_\theta \prod_{i=1}^{m} \p{model}(x_i; \theta) \\
        =& \argmin_\theta \E[x\sim \hat{p}_{\text{data},m}]{-\log{\p{model}(x; \theta)}} \\
        =& \argmin_\theta \dkl{\hat{p}_{\text{data}, m}}{\p{model}}
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Maximum Likelihood Estimator}
    Properties:
    \begin{itemize}
        \item $\k{\theta}{ML}_m$ minimizes dissimilarity between observed distribution $\hat{p}_{\text{data},m}$ and model distribution $\p{model}$
        \item Fastest convergence to true parameter $\theta^\ast$
    \end{itemize}
    \vspace{0.5cm}
    Generalization to conditional probabilities $p\c{y}{x}$:
    \begin{flalign*}
        \theta^{(ML)}_{m} := \argmin_\theta \frac{1}{m} \sum_{i=1}^{m} - \log{\p{model}\c{x_i}{y_i; \theta}}
    \end{flalign*}

    Connection between MLE and UAT:
    \begin{itemize}
        \item MLE: framework for fitting $\p{model}$ to $\p{data}$ via $\theta$
        \item UAT: guarantees that model can approximate $\p{data}$
    \end{itemize}
\end{frame}


\begin{thebibliography}{9}
    \bibitem{textbook}
    Goodfellow et al.
    \bibitem{mitchell97}
    Mitchell, T. M. (1997). Machine Learning.
    \bibitem{dropout}
    Dropout: A Simple Way to Prevent Neural Networks from Overfitting
    \bibitem{kernel}
    Pham, Trong-Ton. (2010). MODELE DE GRAPHE ET MODELE DE LANGUE POUR LA RECONNAISSANCE DE SCENES VISUELLES.
\end{thebibliography}
\end{document}