\documentclass{beamer}

\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{relsize}
\usepackage{graphicx}
\usepackage{dirtytalk}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{pgffor}
\usepackage[export]{adjustbox}
\usepackage{hyperref}
\usepackage{changepage}
\usepackage{tabularray}

\definecolor{Silver}{rgb}{0.752,0.752,0.752}


\setlength{\parindent}{0pt}

\def\noval{}

\DeclarePairedDelimiter{\br}{(}{)}
\DeclarePairedDelimiter{\brr}{[}{]}
\DeclarePairedDelimiter{\brs}{\{}{\}}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\makeatletter
\let\oldbr\br
\def\br{\@ifstar{\oldbr}{\oldbr*}}
\makeatother

\makeatletter
\let\oldbrr\brr
\def\brr{\@ifstar{\oldbrr}{\oldbrr*}}
\makeatother

\makeatletter
\let\oldbrs\brs
\def\brs{\@ifstar{\oldbrs}{\oldbrs*}}
\makeatother

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
\makeatother

\makeatletter
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\makeatletter
\let\oldlog\log
\def\log{\@ifstar\@log\@@log}
\def\@log#1{\oldlog\br{#1}}
\def\@@log#1{\oldlog#1}
\makeatother

\makeatletter
\newcommand{\set}[2]{
    \def\@delim{\;\@ifstar{\middle|}{:}\;}
    \ifx\noval#2\noval%
        \left\{#1\right\}
    \else
        \left\{#1\@delim#2\right\}
    \fi
}
\makeatother

\newcommand{\E}[2][]{%
    \ifx\noval#1\noval%
        \mathbb{E}\brr{#2}
    \else
        \mathbb{E}_{#1}\brr{#2}
    \fi
}
\newcommand{\var}[2][]{%
    \ifx\noval#1\noval%
        \text{Var}\br{#2}
    \else
        \text{Var}_{#1}\br{#2}
    \fi
}
\newcommand{\cov}[2][]{%
    \ifx\noval#1\noval%
        \text{Cov}\br{#2}
    \else
        \text{Cov}_{#1}\br{#2}
    \fi
}

\newcommand{\est}[1]{\hat{#1}_m}
\newcommand{\bias}[1]{\text{bias}\br{#1}}


\newcommand{\arrow}{$\rightarrow\;$}
\newcommand{\smallmin}{\scalebox{0.8}{-}}
\newcommand{\relu}{\text{ReLU}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\text{T}}
\renewcommand{\v}{\bm}
\renewcommand{\c}[2]{\left(#1\;\middle|\;#2\right)}
\renewcommand{\det}[1]{\text{det}\br{#1}}
\newcommand{\ml}[1]{\mathlarger{#1}\,}
\newcommand{\dkl}[2]{D_{\text{KL}}(#1\;\Vert\;#2)}
\renewcommand{\S}{\mathbb{S}}
\newcommand{\func}[3]{#1:#2\rightarrow#3}
\newcommand{\p}[1]{p_\text{#1}}
\renewcommand{\k}[2]{#1^{(#2)}}
\newcommand{\PLH}{{\mkern-2mu\times\mkern-2mu}}

\begin{document}
\begin{frame}
    \frametitle{A broad Definition of Machine Learning}
    
        \say{A computer program is said to learn from experience E 
        with respect to some class of tasks T and performance measure P, 
        if its performance at tasks in T, as measured by P, improves with
        experience.} \cite{mitchell97}
    
\end{frame}
\begin{frame}
    \frametitle{Types of tasks T}
    Generally, task is to learn some highly complex function
    \begin{flalign*}
        \func{f}{\R^n}{\R^m}, f(x) = y
    \end{flalign*}
    Common tasks:\\
    \vspace{1em}
    \begin{minipage}{0.4\textwidth}
        \begin{itemize}
            \item Classification
            \item Regression
        \end{itemize}
    \end{minipage}    
    \begin{minipage}{0.55\textwidth}
        \begin{itemize}
            \item Structured prediction
            \item Probability density estimation
        \end{itemize}
    \end{minipage}    
    \vspace{1em}
    
    \hspace{-1em}
    \begin{minipage}[t]{0.59\textwidth}
        \begin{figure}
            \includegraphics[height=2.4cm]{img/density_estimation.png}
            \caption{Density estimate based on samples (red) drawn from a mixed Gaussian (blue) \cite{density_estimation}}
        \end{figure}
    \end{minipage}
    \hspace{0.5em}
    \begin{minipage}[t]{0.39\textwidth}
        \begin{figure}
            \includegraphics[height=2.4cm]{img/structured prediction.pdf}
            \caption{Sequence tagging as an example of structured prediction \cite{structured_prediction}}
        \end{figure}
    \end{minipage}
    \hspace{-2em}

\end{frame}

\begin{frame}
    \frametitle{Types of experience E}
    Learning usually through observing dataset of examples 
    \begin{itemize}
        \item Unsupervised learning: learn useful properties of dataset
        \item Supervised learning: given examples $(x_i, y_i)$, learn $p\c{y}{x}$
        \item Reinforcement learning: learn from interaction with environment
    \end{itemize}
    
    \begin{figure}
        \hspace{-2em}
        $\vcenter{\hbox{\includegraphics[width=0.35\textwidth]{img/unsupervised_learning.png}}}$
        $\vcenter{\hbox{\includegraphics[width=0.24\textwidth]{img/supervised_learning.png}}}$
        $\vcenter{\hbox{\includegraphics[width=0.4\textwidth]{img/reinforcement_learning.jpg}}}$
        \hspace{-2em}
        \caption{Examples for unsupervised, supervised and reinforcement learning \cite{unsupervised}, \cite{supervised} \cite{reinforcement}}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Different Types of Error}
    \begin{figure}
        \includegraphics[width=0.8\textwidth]{img/deep learning/test_generalization_error.pdf}
        \caption{Typical relationship between capacity and error \cite{textbook}}
    \end{figure}
    
    \begin{itemize}
        \item Capacity: ability of model to fit function to be learned
        \item Training error: error on set of training examples
        \item Generalization error: real-world error on new data
    \end{itemize}
    \arrow Goal: low training \textbf{and} generalization error
\end{frame}

\begin{frame}
    \frametitle{Deep Learning and Neural Networks}
    \begin{itemize}
        \item \textbf{Network}: learned function $f(x)$ can be described by network of nodes
        \item \textbf{Neural}: graph nodes play role analogous to biological neuron
        \item \textbf{Deep} Learning: \say{depth} of networks
    \end{itemize}

    \begin{figure}
        \includegraphics[width=0.4\textwidth]{img/dropout/neural_network_visualization.pdf}
        \caption{Simple Deep Neural Network \cite{dropout}\textsuperscript{*}}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Deep Feedforward Networks (MLPs)}
    Typically have layer structure:
    \begin{figure}
        \includegraphics[width=0.8\textwidth]{img/dropout/neural_network_visualization_annotated.pdf}
        \caption{Layer Structure of Deep Feedforward Networks \cite{dropout}\textsuperscript{*}}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{How does Deep Learning Work?}
    \begin{figure}
        \begin{minipage}[c]{0.67\textwidth}
          \includegraphics[width=\textwidth]{img/deep learning/deep_learning_representation.pdf}
        \end{minipage}\hfill
        \begin{minipage}[c]{0.3\textwidth}
          \caption{
            Deep Networks solve difficult tasks by building complex concepts out of simpler concepts \cite{textbook}
          } \label{fig:03-03}
        \end{minipage}
      \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Deep Learning as Extension of Linear Models}
    Hidden layers: kernel $\phi(x; \theta)$, output layer: regression
    \begin{equation*}
        f(x) = \k{f}{N}(\ldots \k{f}{1}(x)) = \k{W}{N}\phi(x; \theta) + \k{b}{N}
    \end{equation*}
    
    \begin{minipage}[b]{0.49\textwidth}
        \begin{figure}
            \includegraphics[width=0.8\textwidth]{img/dropout/network_kernel.pdf}
            \caption{Deep Network as linear layer with kernel \cite{dropout}\textsuperscript{*}}
        \end{figure}
    \end{minipage}
    \begin{minipage}[b]{0.49\textwidth}
        \begin{figure}
            \includegraphics[width=\textwidth]{img/kernel trick.pdf}
            \caption{mapping with $\phi$ makes input linearly separable \cite{kernel}\textsuperscript{*}}
        \end{figure}
    \end{minipage}
    
    \arrow Deep Learning: learn kernel and regression parameters
    %kernel: extracts features from x or new representation for x
\end{frame}

\begin{frame}
    \frametitle{Universal Approximation Theorems (UAT)}
    \begin{itemize}
        \item Prove that it is possible to approximate functions arbitrarily well with neural networks
        \item Requirement: non-linearity e.g. $\relu(z) = \max(0, z)$
        \item Shallow network with activation is universal approximator, can however require exponential number of hidden units %exponential in input dimension
    \end{itemize}    

    \begin{figure}
        \includegraphics[width=0.5\textwidth]{img/relu_approximation.pdf}
        \caption{Approximation of $y(x) = x^3+x^2-x-1$ using 6 ReLU nodes}
    \end{figure}


\end{frame}

\begin{frame}
    \frametitle{Shallow Network with ReLU as Universal Approximator}
    
    
    \begin{minipage}[t]{0.4\textwidth}
        \vspace{-0.5cm}
        \begin{flalign*}
            y(x) = & -\relu(-5(x+1.5)) &&\\
            & -\relu(-(x+1)) &&\\
            & -\relu(x+1) &&\\
            &+ \relu(x-0.2) &&\\
            &+ \relu(3(x-0.6)) &&\\
            &+ \relu(3(x-1))&&
        \end{flalign*}

        \begin{flalign*}          
            \k{a}{1} &= \text{ReLU}\br{
            \begin{bsmallmatrix*}
                \smallmin 5 \ \\
                \smallmin 1 \\
                 1 \\
                 1 \\
                 3 \\
                 3 
            \end{bsmallmatrix*} \, x + 
            \begin{bsmallmatrix*}
                \smallmin 7.5 \ \\
                \smallmin 1 \\
                1 \\
                \smallmin 0.2 \\
                \smallmin 1.8 \\
                -3 
            \end{bsmallmatrix*}} &&\\
            &&&\\
            y(x) &= \begin{bsmallmatrix*}
                \smallmin 1,& \smallmin 1,& \smallmin 1,& 1,& 1,& 1
            \end{bsmallmatrix*} \, \k{a}{1} &&
        \end{flalign*}

        
        \href{https://www.youtube.com/watch?v=Ln8pV1AXAgQ}{\textcolor{blue}{Visualization on YouTube}}

    \end{minipage}
    \begin{minipage}[t]{0.4\textwidth}
        \begin{figure}
            \includegraphics[width=\textwidth]{img/relu_approximation.pdf}
            \includegraphics[width=0.9\textwidth]{img/relu_approximation_network.pdf}
            \caption{Shallow network and its ouptut}
        \end{figure}
    \end{minipage}
\end{frame}
\begin{frame}
    \frametitle{UAT - Intuition for Advantage of Deeper Networks}
    \begin{itemize}
        \item Stacked hidden layers with activation $g(z) = \abs{z}$ \say{fold} space many times along hyperplanes defined by the $\k{W}{k}, \k{b}{k}$
        \item Function on folded space creates complex repeating pattern on input space
    \end{itemize}
    \begin{figure}
        \includegraphics[width=0.9\textwidth]{img/deep learning/activation_folding.pdf}
        \caption{Two hidden layers with $\abs{z}$-activation fold input space twice \cite{textbook}}
    \end{figure}
    \arrow Important: UAT guarantees ability to approximate, not learn!
\end{frame}

\begin{frame}
    \frametitle{Maximum Likelihood Estimator} 
    \begin{itemize}
        \item Empirical distribution:    
        \begin{equation*}
            p(X = x) = \frac{1}{m} \sum_{i=1}^{m} \delta(x - x_i)
        \end{equation*}
        \item Kullback-Leibler divergence:
        \begin{equation*}
            \dkl{p}{q} := \E[X \sim p]{\log{\frac{p(X=x)}{q(X=x)}}} \geq 0
        \end{equation*}
        \arrow Measure of how different distributions $p$ and $q$ are
    \end{itemize}
    \begin{figure}
        \begin{minipage}[b]{0.35\textwidth}
          \includegraphics[width=\linewidth]{img/deep learning/kl_divergence.pdf}
        \end{minipage}\hfill
        \begin{minipage}[b]{0.6\textwidth}
          \caption{
            Result of minimizing the KL-divergence between a mixed and single Gaussian \cite{textbook}
          }
        \end{minipage}
      \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Maximum Likelihood Estimator}
    
    Prerequisites:
    \begin{itemize}
        \item i.i.d. assumption \arrow view individual examples $x_i \in \mathbb{X}=\set{x_1, \ldots, x_m}{}$ as being drawn from $\p{data}(x)$
        \item $\p{model}(x; \theta)$: family of probability distributions, indexed by $\theta$
        \item $\hat{p}_{\text{data},m}$: empirical distribution defined by $\mathbb{X}$
    \end{itemize}

    \vspace{0.5cm}
    Maximum Likelihood Estimator for $\theta$:
    \begin{align*}
         \k{\theta}{ML}_m 
        :=& \argmax_\theta \prod_{i=1}^{m} \p{model}(x_i; \theta) \\
        =& \argmin_\theta \E[x\sim \hat{p}_{\text{data},m}]{-\log{\p{model}(x; \theta)}} \\
        =& \argmin_\theta \dkl{\hat{p}_{\text{data}, m}}{\p{model}}
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{Maximum Likelihood Estimator}
    Properties:
    \begin{itemize}
        \item $\k{\theta}{ML}_m$ minimizes dissimilarity between observed distribution $\hat{p}_{\text{data},m}$ and model distribution $\p{model}$
        \item Fastest convergence to true parameter $\theta^\ast$
    \end{itemize}
    \vspace{0.5cm}
    Generalization to conditional probabilities $p\c{y}{x}$:
    \begin{flalign*}
        \theta^{(ML)}_{m} := \argmin_\theta \frac{1}{m} \sum_{i=1}^{m} - \log{\p{model}\c{x_i}{y_i; \theta}}
    \end{flalign*}

    Connection between MLE and UAT:
    \begin{itemize}
        \item MLE: framework for fitting $\p{model}$ to $\p{data}$ via $\theta$
        \item UAT: guarantees that model can approximate $\p{data}$
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Cost Functions and Gradient Descent}
    
    Cost functions:
    \begin{itemize}
        \item Often sum over per-example loss: $J(\theta) = \frac{1}{m} \sum_{i=1}^
        {m}L(x_i, y_i; \theta)$
        \vspace{0.2cm}
        \item MLE: $L(x, y, \theta) = -\log{\p{model}\c{y}{x; \theta}}$
    \end{itemize}
    
    \vspace{0.4cm}
    
    Gradient descent:
    \begin{itemize}
        \item Iteratively minimize $J$: $\ \theta_{k+1} = \theta_k - \epsilon \nabla_\theta J(\theta_k)$
        \item Problem: computational cost of one step $\propto m$
    \end{itemize}

    \begin{figure}
        \includegraphics[width=0.6\textwidth]{img/gradient_descent.png}
        \caption{Visualization of gradient descent \cite{arnold}}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Stochastic Gradient Descent and Momentum}
    Stochastic Gradient Descent (SGD):
    \vspace{-0.2cm}
    \begin{equation*}
        \nabla_\theta J(\theta) \approx g = \frac{1}{m^\prime} \sum_{i=1}^{m^\prime} \nabla_\theta L(x_i, y_i; \theta), \quad m^\prime \ll m
    \end{equation*}\\
    \vspace{-0.2cm}
    \arrow Use randomly drawn minibatch of examples to estimate $\nabla_\theta J$

    \begin{minipage}[b]{0.51\textwidth}
        \vspace{0.4cm}
        Momentum:
        \vspace{-0.3cm}
        \begin{flalign*}
            & v_{k+1} = \alpha v_k - \epsilon \nabla_\theta J(\theta_{k}) &&\\
            & \theta_{k+1} = \theta_k + v_{k+1} &&\\
            & \alpha \in [0,1)&&
        \end{flalign*}
        \vspace{-0.7cm}
        \begin{itemize}
            \item Weighted average stabilizes gradient
            \item Analogy: particle with friction, velocity $v$
        \end{itemize}
    \end{minipage}
    \begin{minipage}[b]{0.47\textwidth}
        \begin{figure}
            \includegraphics[width=0.7\linewidth]{img/deep learning/momentum.pdf}
            \vspace{-0.2cm}
            \hspace{3.4em}
            \caption{Comparison of GD\\ \textbf{\textcolor{red}{with}} and \textbf{without} momentum \cite{textbook}}
            \vspace{-0.7cm}
        \end{figure}
    \end{minipage}
\end{frame}

\begin{frame}
    \frametitle{Backpropagation - General Formulation}
    Algorithm for computing $\frac{\partial F}{\partial X}, \ X \in \set{A, \ldots, F}{}$:
        \begin{itemize}
            \item Start at node $F$, $\frac{\partial F}{\partial F} = 1$
            \item Propagate backwards along graph, \textbf{re-using partial derivatives} of previous nodes, e.g.: $\frac{\partial F}{\partial C} = \frac{\partial F}{\partial E}\frac{\partial E}{\partial C}$
        \end{itemize}
    
    \begin{figure}
        \centering
        \foreach \n in {1,...,6}{\only<\n>{\includegraphics[width=5cm]{img/backprop/backprop_\n.pdf}}}
        \caption{Example of Backpropagation through a graph \cite{dag}\textsuperscript{*}}
    \end{figure}
    \arrow Deep/complex graphs: dramatic speedup compared to recomputing all intermediate $\frac{\partial F}{\partial X}$ for each node
\end{frame}

\begin{frame}
    \frametitle{Backpropagation for MLPs}
    
    \begin{minipage}{0.4\textwidth}
        Feedforward:
        \begin{flalign*}
            & \k{z}{k} = \k{W}{k} \k{a}{k-1} + \k{b}{k} &&\\ 
            & \k{a}{k} = \k{g}{k}(\k{z}{k}) &&
        \end{flalign*}
    \vspace{0.5cm}\\
    Backpropagation:\\
    (let $\br{\frac{\partial u}{\partial v}}_{ij} = \frac{\partial u_j}{\partial v_i}$)    
    \end{minipage}
    \begin{minipage}{0.57\textwidth}
        \begin{figure}
            \includegraphics[width=0.9 \linewidth, right]{img/dropout/backprop_mlp.pdf}
            \caption{Visualization of a MLP \cite{dropout}\textsuperscript{*}}
        \end{figure}
    \end{minipage}
    \vspace{-0.3cm}
    \begin{flalign*}
        & \frac{\partial L}{\partial \k{z}{k}} = \frac{\partial \k{z}{k+1}}{\partial \k{z}{k}}\frac{\partial L}{\partial \k{z}{k+1}} =  \frac{\partial \k{g}{k}(\k{z}{k})}{\partial\k{z}{k}} {\k{W}{k+1}}^T \frac{\partial L}{\partial \k{z}{k+1}} &&\\
        & \frac{\partial L}{\partial \k{W}{k}_{ij}} = \frac{\partial L}{\partial \k{z}{k}_i} \k{a}{k-1}_j, \quad \frac{\partial L}{\partial \k{b}{k}_i}
        = \frac{\partial L}{\partial \k{z}{k}_i} &&
    \end{flalign*}
    \arrow During feedforward    store the $\k{z}{k}, \k{a}{k}$, during backprop $\frac{\partial L}{\partial \k{z}{k}}$
\end{frame}

\begin{frame}
    \frametitle{Regularization Techniques}
    
    \begin{itemize}
        \item Parameter norm constraints (e.g. $\Omega(\theta) = \norm{\theta}_2$):\\
        \vspace{0.15cm}
        implicit: minimize $\tilde{J}(\theta) = J(\theta) + \alpha \Omega(\theta), \quad \alpha \geq 0$ \\
        \vspace{0.15cm}
        explicit: project $\theta$ back after gradient step, s.t. $\Omega(\theta) < c$
        \item Dataset augmentation: generate new examples by transforming existing ones
        \item Early stopping: stop training as soon as generalization error starts increasing again 
    \end{itemize}
    \begin{figure}
        \hspace{-2em}
        $\vcenter{\hbox{\includegraphics[width=0.24\textwidth]{img/deep learning/norm_constraint.pdf}}}$
        $\vcenter{\hbox{\includegraphics[width=0.24\textwidth]{img/deep learning/dataset_augmentation.pdf}}}$
        $\vcenter{\hbox{\includegraphics[width=0.55\textwidth]{img/deep learning/early_stopping.pdf}}}$
        \hspace{-2em}
        \caption{Illustration of parameter norm constraints, dataset augmentation and early stopping \cite{textbook}\textsuperscript{*}}
    \end{figure}
\end{frame}
\begin{frame}
    \frametitle{Dropout}
    Train ensemble of all subnetworks of base network:
    \begin{itemize}
        \item Randomly remove non-output nodes for each training example
        \item MLP: $\k{a}{k}_n = \k{\mu}{k}_n \k{g}{k}(\k{z}{k})_n\,, \ \k{\mu}{k}_n$ Bernoulli-distributed
        \item For non-training inputs return average of ensemble results or
        equivalently scale outgoing weights by $p(\k{\mu}{k}_n = 1)$
    \end{itemize}
    \vspace{-0.5cm}
    \begin{minipage}{0.49\textwidth}
        \vspace{0.3cm}
        Why does it work:
        \begin{itemize}
            \item Regularizes nodes to work well in different contexts
            \item Form of dataset augmentation
            \arrow create new inputs by injecting noise
        \end{itemize}
    \end{minipage}
    \hspace{1em}
    \begin{minipage}{0.4\textwidth}
        \vspace{1cm}
        \begin{figure}
            \includegraphics[width=0.8\linewidth]{img/dropout/dropout.pdf}
            
            \caption{Subnetwork formed by randomly sampling $\mu$ \cite{dropout}\textsuperscript{*}}
        \end{figure}
    \end{minipage}
\end{frame}

\begin{frame}
    \frametitle{Training a MLP to Classify Digits with the MNIST Dataset}

    Model Architecture:
    \begin{itemize}
        \item Input layer: width 784 \arrow images are $28\times28$ pixels
        \item 4 hidden layers, each 100 units wide with ReLU activation
        \item Output layer: multinomial logistic regression, width 10 for digits 0-9
    \end{itemize}
    \arrow $N=5$ non-input layers\\
    \vspace{0.3em}
    
    Multinomial logistic regression:\\
    Affine transformation returns parameters $z_i$ for logistic model:
    \begin{flalign*}
        z :=& \k{z}{N} = \k{W}{N} \phi(x; \theta) + \k{b}{N} \\
        p(c=i) =& \frac{\exp(z_i)}{\sum_{j}\exp(z_j)} =: \text{softmax}(z)_i
    \end{flalign*}

\end{frame}

\begin{frame}
    \frametitle{Training a MLP to Classify Digits with the MNIST Dataset}
    MLE cost function:
    \begin{flalign*}
        L(x, y; \theta) &= -\log{\p{model}\c{y}{x; \theta}} = -\log*{\text{softmax}(z(x))_y} \\
        J(\theta) &= \frac{1}{m} \sum_{k=1}^{m} L(x_k, y_k; \theta)
    \end{flalign*}

    Optimization and Regulation:
    \begin{itemize}
        \item SGD using batchsize 20 with Adam optimizer
        \item Early Stopping
        \item Dropout with $p=$ and $p=$ for keeping input and hidden nodes
    \end{itemize}

    Source code: \url{https://github.com/philipobi/seminar}

\end{frame}

\begin{frame}
    \frametitle{Convolutional Neural Networks (CNNs)}
    Specialized neural networks for inputs with grid-like topology
    
    \begin{minipage}[t]{0.49\textwidth}
        \vspace{1em}
        Define convolution operation for CNNs with 2D-inputs as:
        \begin{equation*}
            F(i,j) = \sum_{m, n} I(i+m, j+n)K(m,n)
        \end{equation*}
        
        Idea: move the kernel $K$ accross the input $I$ like a window into the array
    \end{minipage}
    \begin{minipage}[t]{0.49\textwidth}
        \begin{figure}
            \captionsetup{margin={1cm,-0.5cm}}
            \includegraphics[width=\linewidth]{img/deep learning/2d_conv.pdf}
            \hspace{-4.5em}
            \caption{Visualization of convolution with a $2\PLH2$ kernel \cite{textbook}}
        \end{figure}
    \end{minipage}
\end{frame}

\begin{frame}
    \frametitle{Architecture of a convolutional Layer}
    {\setlength{\leftmargini}{0cm}
    \begin{enumerate}
        \item Compute multiple feature maps $F$ in parallel using different kernels
        \item Apply element-wise activation to the $\k{F}{k}_{ij}$
        \item Summarize the activations using a pooling function
    \end{enumerate}}
    
    \vspace{2em}
    \begin{minipage}[b]{0.52\textwidth}
        \begin{figure}
            \includegraphics[width=0.9\linewidth]{img/conv_layer.png}
            \caption{Composition of a convolutional layer \cite{conv_layer}}
        \end{figure}
    \end{minipage}
    \begin{minipage}[b]{0.45\textwidth}
        \begin{figure}
            \includegraphics[width=0.8\linewidth]{img/deep learning/pooling.pdf}
            \caption{Max-pooling returns maximum activation in area \cite{textbook}}
        \end{figure}
    \end{minipage}
\end{frame}

\begin{frame}
    \frametitle{The Reasoning behind CNNs}
    \hspace{-1em}
    \begin{minipage}{0.6\textwidth}
        Practical advantages over MLPs:
        \begin{itemize}
            \item Sparse connectivity: not all input and output nodes are connected
            \item Parameter sharing: kernel parameters reused for each node in a feature map
            \item Downsampling: fixed-size outputs by adapting stride to input size
        \end{itemize}
    \end{minipage}
    \hspace{0.5em}
    \begin{minipage}{0.35\textwidth}
        \vspace{-3em}
        \begin{figure}
            \includegraphics[width=0.4\linewidth]{img/deep learning/sparse connectivity_1.pdf}
            \includegraphics[width=0.4\linewidth]{img/deep learning/sparse connectivity_2.pdf}
            \hspace{-3em}
            \captionsetup{margin={2em,-5em}}
            \caption{Connectivity comparison: convolutional layer with $3\PLH1$ kernel vs. fully connected \cite{textbook}}
        \end{figure}
    \end{minipage}

    \arrow Decreased computational cost, increased input-flexibility\\
    \vspace{2em}
    
    \begin{minipage}[b]{0.5\textwidth}
        \begin{figure}
            \caption{Max-pooling with stride 2 \cite{textbook}}
        \end{figure}
    \end{minipage}
    \begin{minipage}[b]{0.45\textwidth}
        \begin{figure}
            \includegraphics[width=\linewidth]{img/deep learning/downsampling.pdf}
            \hspace{-3em}
        \end{figure}
    \end{minipage}
    
\end{frame}


\begin{frame}
    \frametitle{The Reasoning behind CNNs}
    \hspace{-1.5em}
    \begin{minipage}{0.7\textwidth}
        Problem-specific arguments:
        {%
        \setlength{\leftmarginii}{1em}
        \begin{itemize}
            \item Feature-extraction often only needs local information
            \item Introduction of invariances by pooling and parameter sharing
            \item Biological analogy: visual cortex
            \begin{itemize}
                \item Two-dimensional structure, mirroring image
                \item Cell activity influenced only by localized receptive field
                \item Deeper brain layers extract increasingly complex features using the same structure
            \end{itemize}
        \end{itemize}}
    \end{minipage}
    \hspace{1em}
    \begin{minipage}{0.2\textwidth}
        \vspace{-2em}
        \begin{figure}
            \includegraphics[width=1.5\linewidth]{img/deep learning/pooling_invariance.pdf}
            \hspace{-10em}
            \captionsetup{margin={1em,-5em}}
            \vspace{-1em}
            \caption{Invariance by pooling feature maps responding to different orientations \cite{textbook}}
        \end{figure}
    \end{minipage}
    \begin{minipage}[b]{0.49\textwidth}
        \begin{figure}
            \caption{Edge detection only needs local information ($K = \brr{- 1, 1}$) \cite{textbook}}
        \end{figure}
    \end{minipage}
    \begin{minipage}[b]{0.49\textwidth}
        \includegraphics[width=0.9\linewidth]{img/deep learning/edge_detection.pdf}
    \end{minipage}
\end{frame}

\begin{thebibliography}{99}
    \bibitem{textbook}
    Goodfellow et al.
    \bibitem{mitchell97}
    Mitchell, T. M. (1997). Machine Learning.
    \bibitem{dropout}
    Dropout: A Simple Way to Prevent Neural Networks from Overfitting
    \bibitem{kernel}
    Pham, Trong-Ton. (2010). MODELE DE GRAPHE ET MODELE DE LANGUE POUR LA RECONNAISSANCE DE SCENES VISUELLES.
    \bibitem{arnold}
    \url{https://ludovicarnold.com/teaching/optimization/gradient-descent/}
    \bibitem{dag}
    \url{https://commons.wikimedia.org/w/index.php?curid=3211391}
    \bibitem{supervised}
    \url{https://corochann.com/mnist-dataset-introduction-532/}
    \bibitem{unsupervised}
    \url{https://cloud.google.com/discover/what-is-unsupervised-learning?hl=en}
    \bibitem{reinforcement}
    \url{https://commons.wikimedia.org/w/index.php?curid=141214663}
    \bibitem{density_estimation}
    \url{https://commons.wikimedia.org/w/index.php?curid=24309466}
    \bibitem{structured_prediction}
    \url{https://en.wikipedia.org/wiki/Structured_prediction}
    \bibitem{conv_layer}
    \url{https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/}
    \bibitem{relu_uat}
    \url{https://www.youtube.com/watch?v=Ln8pV1AXAgQ}
\end{thebibliography}
\end{document}