\documentclass[handout]{beamer}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{relsize}
\usepackage{graphicx}
\usepackage{dirtytalk}
\usepackage{xcolor}
\usepackage{caption}
\usepackage[export]{adjustbox}
\usepackage{hyperref}
\usepackage{pgfpages}
\usepackage{changepage}
\usepackage[
    defernumbers=true,
    backend=biber
    ]{biblatex}
    
\addbibresource{info.bib}
\addbibresource{img.bib}
    
    
\setbeameroption{show notes}
\setbeamertemplate{note page}{%
    \insertnote
}

\setlength{\parindent}{0pt}

\def\noval{}

\DeclarePairedDelimiter{\br}{(}{)}
\DeclarePairedDelimiter{\brr}{[}{]}
\DeclarePairedDelimiter{\brs}{\{}{\}}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\makeatletter
\let\oldbr\br
\def\br{\@ifstar{\oldbr}{\oldbr*}}
\makeatother

\makeatletter
\let\oldbrr\brr
\def\brr{\@ifstar{\oldbrr}{\oldbrr*}}
\makeatother

\makeatletter
\let\oldbrs\brs
\def\brs{\@ifstar{\oldbrs}{\oldbrs*}}
\makeatother

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
\makeatother

\makeatletter
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\makeatletter
\let\oldlog\log
\def\log{\@ifstar\@log\@@log}
\def\@log#1{\oldlog\br{#1}}
\def\@@log#1{\oldlog#1}
\makeatother

\makeatletter
\newcommand{\set}[2]{
    \def\@delim{\;\@ifstar{\middle|}{:}\;}
    \ifx\noval#2\noval%
        \left\{#1\right\}
    \else
        \left\{#1\@delim#2\right\}
    \fi
}
\makeatother

\newcommand{\E}[2][]{%
    \ifx\noval#1\noval%
        \mathbb{E}\brr{#2}
    \else
        \mathbb{E}_{#1}\brr{#2}
    \fi
}



\newcommand{\arrow}{$\rightarrow\;$}
\newcommand{\smallmin}{\scalebox{0.8}{-}}
\newcommand{\relu}{\text{ReLU}}

\newcommand{\R}{\mathbb{R}}

\renewcommand{\c}[2]{\left(#1\;\middle|\;#2\right)}

\newcommand{\ml}[1]{\mathlarger{#1}\,}
\newcommand{\dkl}[2]{D_{\text{KL}}(#1\;\Vert\;#2)}

\newcommand{\func}[3]{#1:#2\rightarrow#3}
\newcommand{\p}[1]{p_\text{#1}}
\renewcommand{\k}[2]{#1^{(#2)}}
\newcommand{\PLH}{{\mkern-2mu\times\mkern-2mu}}

\title{Deep learning with neural networks and programming neural networks in Python}
\author{Philip Obi}
\date{26.11.2024}

\begin{document}

\frame{\titlepage}

\begin{frame}
    \frametitle{A broad Definition of Machine Learning}
    
        \say{A computer program is said to learn from experience E 
        with respect to some class of tasks T and performance measure P, 
        if its performance at tasks in T, as measured by P, improves with
        experience.} \cite{mitchell97}
    
\end{frame}

\note[itemize]{
    \item would like to start with broad definition of machine learning:
}

\begin{frame}
    \frametitle{Types of tasks T}
    Generally, task is to learn some highly complex function
    \begin{flalign*}
        \func{f}{\R^n}{\R^m}, f(x) = y
    \end{flalign*}
    \onslide<2->{Common tasks:\\}
    \vspace{1em}
    \begin{minipage}{0.4\textwidth}
        \begin{itemize}
            \onslide<2->{\item Classification}
            \onslide<3->{\item Regression}
        \end{itemize}
    \end{minipage}    
    \begin{minipage}{0.55\textwidth}
        \begin{itemize}
            \onslide<4->{\item Structured prediction}
            \onslide<5->{\item Probability density estimation}
        \end{itemize}
    \end{minipage}    
    \vspace{1em}
    
    \hspace{-1em}
    \begin{minipage}[t]{0.39\textwidth}
        \begin{figure}
            \onslide<4->{\includegraphics[height=2.4cm]{img/structured prediction.pdf}}
            \onslide<6->{\caption{Sequence tagging as an example of structured prediction \cite{structured_prediction}}}
        \end{figure}
    \end{minipage}
    \hspace{0.5em}
    \begin{minipage}[t]{0.59\textwidth}
        \begin{figure}
            \onslide<5->{\includegraphics[height=2.4cm]{img/density_estimation.png}}
            \onslide<6->{\caption{Density estimate based on samples (red) drawn from a mixed Gaussian (blue) \cite{density_estimation}}}
        \end{figure}
    \end{minipage}
    \hspace{-2em}

\end{frame}

\note[itemize]{
    \item for the tasks:
    \item generally, want to approximate some highly complex function
    \item common tasks include
    \item classification: produce vector of probabilities of input belonging to certain classes
    \item regression: predict numerical value given some input
    \item structured prediction: important relationships between elements input as well as output vector
    \item e.g. tagging a sentence \arrow vector of words
    \item explicitly learn probability function set of inputs
    \item e.g. image: density estimate based on samples drawn from mixed gaussian
    \item most above taskss already implicitly learn structure/parameters of underlying distribution
}

\begin{frame}
    \frametitle{Types of experience E}
    Learning usually through observing dataset of examples 
    \begin{itemize}
        \onslide<2->{\item Unsupervised learning: learn useful properties of dataset}
        \onslide<3->{\item Supervised learning: given examples $(x_i, y_i)$, learn $p\c{y}{x}$}
        \onslide<4->{\item Reinforcement learning: learn from interaction with environment}
    \end{itemize}
    
    \begin{figure}
        \hspace{-2em}
        \onslide<2->{$\vcenter{\hbox{\includegraphics[width=0.35\textwidth]{img/unsupervised_learning.png}}}$}
        \onslide<3->{$\vcenter{\hbox{\includegraphics[width=0.24\textwidth]{img/supervised_learning.png}}}$}
        \onslide<4->{$\vcenter{\hbox{\includegraphics[width=0.4\textwidth]{img/reinforcement_learning.jpg}}}$}
        \hspace{-2em}
        \onslide<5->{\caption{Examples for unsupervised, supervised and reinforcement learning \cite{unsupervised}, \cite{supervised} \cite{reinforcement}}}
    \end{figure}
\end{frame}

\note[itemize]{
    \item in most cases, algorithm gains experience by observing examples
    \item tasks usually described in terms of how to process a example
    \item learning can roughly be divided into:
    \item unsupervised learning: learn properties of dataset, e.g. estimate probability distribution or cluster data
    \item supervised learning: experience dataset of examples with labels
    \item learn to estimate $p\c{y}{x}$, e.g. probability of label 5 given pixel vector 5
    \item reinforcement learning: learn through feedback loop between system and environment
    \item example: parts of full-self-driving
}

\begin{frame}
    \frametitle{Different Types of Error}
    
    \begin{itemize}
        \onslide<2->{\item Training error: error on set of training examples}
        \onslide<3->{\item Generalization error: real-world error on new data}
        \onslide<4->{\item Capacity: ability of model to fit function to be learned}
    \end{itemize}
    
    \hspace{-4em}
    \begin{figure}
        \hspace{-4em}
        \onslide<5->{$\vcenter{\hbox{\includegraphics[height=0.3\textwidth]{img/deep learning/test_generalization_error.pdf}}}$}
        \onslide<6->{$\vcenter{\hbox{\includegraphics[height=0.3\textwidth]{img/deep learning/underfitting.pdf}}}$}
        \onslide<7->{$\vcenter{\hbox{\includegraphics[height=0.3\textwidth]{img/deep learning/overfitting.pdf}}}$}
        \hspace{-4em}
    \end{figure}
    \vspace{-2em}
    \begin{figure}
        \onslide<9->{\caption{(Left:) Typical relationship between capacity and errors. (Right:) Underfitting and overfitting for polynomials of different capacity \cite{textbook}}}
    \end{figure}
    \vspace{-2em}
    \onslide<8->{\arrow Goal: low training \textbf{and} generalization error}
\end{frame}

\note[itemize]{
    \item to measure models performance, we can use different types of error
    \item training error: error the model makes on the set used for training
    \item generalization error: error the model makes on data it hasnt been trained on \arrow error in real world scenario
    \item errors usually change with model capacity
    \item capacity: ability of model to fit many different functions
    \item underfitting: model cant lower training error enough, can be caused by capacity too low for task at hand
    \item overfitting: large gap between generalization and test error
    \item can because by too high capacity, model "memorizes" training set or has too many parameters/degrees of freedom
    \item what differentiates machine learning from simple optimization: goal is low training and more importantly low generalization error
}

\begin{frame}
    \frametitle{Deep Learning and Neural Networks}
    \onslide<2->{Terminology:}
    \begin{itemize}
        \onslide<3->{\item \textbf{Network}: learned function $f(x)$ can be described by network of nodes}
        \onslide<4->{\item \textbf{Neural}: graph nodes play role analogous to biological neuron}
        \onslide<5->{\item \textbf{Deep} Learning: \say{depth} of networks}
    \end{itemize}

    \begin{figure}
        \includegraphics[width=0.4\textwidth]{img/dropout/neural_network_visualization.pdf}
        \caption{Simple Deep Neural Network \cite{dropout}\textsuperscript{*}}
    \end{figure}
\end{frame}

\note[itemize]{
    \item for deep learning and neural networks, first some terminology
    \item called networks because learned function can be described by network of nodes
    \item term neural used because graph nodes have similar functionality to biological neurons in that they are interconnected and propagate information/activations
    \item called deep learning because graphs usually have considerable depth
    \item depth/width: length in/orthogonal to propagation direction
}

\begin{frame}
    \frametitle{Deep Feedforward Networks (MLPs)}
    Typically have layer structure:
    \begin{figure}
        \includegraphics[width=0.8\textwidth]{img/dropout/neural_network_visualization_annotated.pdf}
        \caption{Layer Structure of Deep Feedforward Networks \cite{dropout}\textsuperscript{*}}
    \end{figure}
\end{frame}

\note[itemize]{
    \item Deep feedforward networks, also known as multilayer perceptrons
    \item Quintessential deep learning models
    \item Typically have layer structure
    \item Activations in layer are vector
    \item propagation by matrix-multiplication with weight and addition of bias and application of an activation function
    \item distinguish between input layer, output layer and hidden layers
    \item hidden because we do not explicitly specify what we want this layer to do
}

\begin{frame}
    \frametitle{How does Deep Learning Work?}
    \begin{figure}
        \begin{minipage}[c]{0.67\textwidth}
          \includegraphics[width=\textwidth]{img/deep learning/deep_learning_representation.pdf}
        \end{minipage}\hfill
        \begin{minipage}[c]{0.3\textwidth}
          \caption{
            Deep Networks solve difficult tasks by building complex concepts out of simpler concepts \cite{textbook}
          } \label{fig:03-03}
        \end{minipage}
      \end{figure}
\end{frame}

\note[itemize]{
    \item Several reasons to why deep learning works
    \item Most intuitive: build complex representations of objects / complex tasks out of simpler units
    \item e.g. for recognizing items in image
    \item input represented as pixels
    \item as we progress to deeper layers extract edges, then contours, object parts and finally object identity
}

\begin{frame}
    \frametitle{Deep Learning as Extension of Linear Models}
    Hidden layers: kernel $\phi(x; \theta)$, output layer: regression
    \begin{equation*}
        f(x) = \k{f}{N}(\ldots \k{f}{1}(x)) = \k{W}{N}\phi(x; \theta) + \k{b}{N}
    \end{equation*}
    
    \begin{minipage}[b]{0.49\textwidth}
        \begin{figure}
            \onslide<2->{\includegraphics[width=0.8\textwidth]{img/dropout/network_kernel.pdf}}
            \onslide<5->{\caption{Deep Network as linear layer with kernel \cite{dropout}\textsuperscript{*}}}
        \end{figure}
    \end{minipage}
    \begin{minipage}[b]{0.49\textwidth}
        \begin{figure}
            \onslide<3->{\includegraphics[width=\textwidth]{img/kernel trick.pdf}}
            \onslide<5->{\caption{mapping with $\phi$ makes input linearly separable \cite{kernel}\textsuperscript{*}}}
        \end{figure}
    \end{minipage}
    
    \onslide<4->{\arrow Deep Learning: learn kernel and regression parameters}
    %kernel: extracts features from x or new representation for x
\end{frame}

\note[itemize]{
    \item more formally, deep learning can be seen as extension of linear models
    \item hidden layers form a kernel, output layer performs some kind of regression, e.g. linear regression to separate two input classes
    \item kernel maps from input space, where inputs not linearly separable to high-dimensional space, where problem becomes separable
    \item applying kernel can be seen as extracting features from input or introducing a new representation of the input
    \item deep learning: learn the regression parameters as well as the parameters defining the kernel function
}

\begin{frame}
    \frametitle{Universal Approximation Theorems (UAT)}
    \begin{itemize}
        \onslide<2->{\item Prove that it is possible to approximate functions arbitrarily well with neural networks}
        \onslide<3->{\item Requirement: non-linearity e.g. $\relu(z) = \max(0, z)$}
        \onslide<5->{\item Shallow network with activation is universal approximator, can however require exponential number of hidden units} %exponential in input dimension
    \end{itemize}

    \begin{minipage}[t]{0.49\textwidth}    
        \begin{figure}
            \onslide<4->{\includegraphics[height=0.5\linewidth]{img/relu_plot.pdf}}
            \onslide<7->{\caption{Plot of ReLU$(x)$}}
        \end{figure}
    \end{minipage}
    \begin{minipage}[t]{0.49\textwidth}    
        \begin{figure}
            \onslide<6->{\includegraphics[height=0.5\linewidth]{img/relu_approximation.pdf}}
            \onslide<7->{\caption{Approximation of $y(x) = x^3+x^2-x-1$ using 6 ReLU nodes}}
        \end{figure}
    \end{minipage}
    
\end{frame}

\note[itemize]{
    \item universal approximation theorems also give us arguments for why neural networks are successful at learning complex functions
    \item they prove that it is possible to approximate functions arbitrarily well provided the neural network is large enough
    \item one of the requirements for this to work is non-linearity, which we mostly introduce using activation functions such as ReLU
    \item in fact, one can show that network with one hidden layer and e.g. ReLU activation is universal approximator
    \item Caveat: may require number of hidden units exponential to input size
    \item example: approximation of polynomial with 6 nodes using ReLU activation on interval -2, 2
}

\begin{frame}
    \frametitle{Shallow Network with ReLU as Universal Approximator}
    
    \begin{minipage}[t]{0.4\textwidth}
        \vspace{-0.5cm}
        \onslide<2->{\begin{flalign*}
            y(x) = & -\relu(-5(x+1.5)) &&\\
            & -\relu(-(x+1)) &&\\
            & -\relu(x+1) &&\\
            &+ \relu(x-0.2) &&\\
            &+ \relu(3(x-0.6)) &&\\
            &+ \relu(3(x-1))&&
        \end{flalign*}}%
        \vspace{-1.2em}
        \onslide<3->{\begin{flalign*}          
            \k{a}{1} &= \text{ReLU}\br{
            \begin{bsmallmatrix*}
                \smallmin 5 \ \\
                \smallmin 1 \\
                 1 \\
                 1 \\
                 3 \\
                 3 
            \end{bsmallmatrix*} \, x + 
            \begin{bsmallmatrix*}
                \smallmin 7.5 \ \\
                \smallmin 1 \\
                1 \\
                \smallmin 0.2 \\
                \smallmin 1.8 \\
                -3 
            \end{bsmallmatrix*}} &&\\
            &&&\\
            y(x) &= \begin{bsmallmatrix*}
                \smallmin 1,& \smallmin 1,& \smallmin 1,& 1,& 1,& 1
            \end{bsmallmatrix*} \, \k{a}{1} &&
        \end{flalign*}}

        \onslide<5->{\href{https://www.youtube.com/watch?v=Ln8pV1AXAgQ}{\textcolor{blue}{Visualization on YouTube}}}

    \end{minipage}
    \begin{minipage}[t]{0.4\textwidth}
        \begin{figure}
            \includegraphics[width=\textwidth]{img/relu_approximation.pdf}
            \onslide<4->{\includegraphics[width=0.9\textwidth]{img/relu_approximation_network.pdf}}
            \onslide<6->{\caption{Shallow network and its ouptut}}
        \end{figure}
    \end{minipage}
\end{frame}

\note[itemize]{
    \item claim: this function approximates the polynomial
    \item illustration: see desmos
    \item now that we have seen, how the relus approximate, what is the connection to neural networks?
    \item We can also represent the above like so
    \item this corresponds to neural network with 6 hidden units and linear output
    \item we can also argue in the opposite direction, assume an algorithm learnt these parameters, we see it approximates the given function
    \item also visualization of this on youtube, which inspired me to use this example
}

\begin{frame}
    \frametitle{UAT - Intuition for Advantage of Deeper Networks}
    \begin{itemize}
        \onslide<2->{\item Stacked hidden layers with activation $g(z) = \abs{z}$ \say{fold} space many times along hyperplanes defined by the $\k{W}{k}, \k{b}{k}$}
        \onslide<3->{\item Function on folded space creates complex repeating pattern on input space}
    \end{itemize}
    \begin{figure}
        \onslide<2->{\includegraphics[width=0.9\textwidth]{img/deep learning/activation_folding.pdf}}
        \onslide<5->{\caption{Two hidden layers with $\abs{z}$-activation fold input space twice \cite{textbook}}}
    \end{figure}
    \onslide<4->{\arrow Important: UAT guarantees ability to approximate, not learn!}
\end{frame}

\note[itemize]{
    \item what are advantages of using deeper networks?
    \item geometric intuition for this using absolute value rectification as activation
    \item layer weights and biases define a hyperplane along which the absolute value folds space
    \item using many hidden layers in a row creates complex pattern of folds on input space
    \item computing a function on the folded space leads to a function with complex symmetries being computed on the input space
    \item by using layers we can carve out exponentially many linear regions in the input space, fixing the issue of possibly needing exponentially many hidden units for shallow network
    
    \item one detail is important: uat only guarantees that model has ability to approximate function, does not guarantee that function can be learned
}

\begin{frame}
    \frametitle{Maximum Likelihood Estimator} 
    \begin{itemize}
        \onslide<2->{\item Empirical distribution:    
        \begin{equation*}
            p(X = x) = \frac{1}{m} \sum_{i=1}^{m} \delta(x - x_i)
        \end{equation*}}
        \onslide<3->{\item Kullback-Leibler divergence:
        \begin{equation*}
            \dkl{p}{q} := \E[X \sim p]{\log{\frac{p(X=x)}{q(X=x)}}} \geq 0
        \end{equation*}}
        \onslide<4->{\arrow Measure of how different distributions $p$ and $q$ are}
    \end{itemize}
    \begin{figure}
        \begin{minipage}[b]{0.35\textwidth}
          \onslide<5->{\includegraphics[width=\linewidth]{img/deep learning/kl_divergence.pdf}}
        \end{minipage}\hfill
        \begin{minipage}[b]{0.6\textwidth}
          \onslide<6->{\caption{
            Result of minimizing the KL-divergence between a mixed and single Gaussian \cite{textbook}
          }}
        \end{minipage}
      \end{figure}
\end{frame}

\note[itemize]{
    \item to go more into detail how model learns function, introduce the maximum likelihood estimator
    \item important things we need for this:
    \item empirical distribution: puts probability mass 1/m on every observed example
    \item kullback-leibler divergence
    \item measure how different distributions $p$ and $q$ over same random variable are
    \item positive semidefinite, 0 for both distributions equal
    \item concept of distance between distributions
    \item minimizing over $q$ selects function that places high probability everywhere $p$ has high probability
    \item as seen here where $p$ is mixture of two gaussians and $q$ has to be single gaussian
}

\begin{frame}
    \frametitle{Maximum Likelihood Estimator}
    
    Prerequisites:
    \begin{itemize}
        \onslide<2->{\item i.i.d. assumption \arrow view individual examples $x_i \in \mathbb{X}=\set{x_1, \ldots, x_m}{}$ as being drawn from $\p{data}(x)$}
        \onslide<3->{\item $\p{model}(x; \theta)$: family of probability distributions, indexed by $\theta$}
        \onslide<4->{\item $\hat{p}_{\text{data},m}$: empirical distribution defined by $\mathbb{X}$}
    \end{itemize}

    \vspace{0.5cm}
    \onslide<5->{Maximum Likelihood Estimator for $\theta$:}
    \begin{align*}
        \onslide<5->{\k{\theta}{ML}_m 
        :=& \argmax_\theta \prod_{i=1}^{m} \p{model}(x_i; \theta) \\}
        \onslide<6->{=& \argmin_\theta \frac{1}{m} \sum_{i=1}^{m}-\log{\p{model}(x; \theta)} \\}
        \onslide<7->{=& \argmin_\theta \dkl{\hat{p}_{\text{data}, m}}{\p{model}}}
    \end{align*}
\end{frame}

\note[itemize]{
    \item prerequisites for defining ml-estimator
    \item i.i.d. assumption: view all examples as being indepentent and identically distributed
    \item individual examples can be seen as drawn from data-generating distribution p data
    \item p model family of probability distributions
    \item p data m empirical distribution defined by examples in X, approximation of real p data
    \item define the maximum likelihood estimator for $\theta$ as:
    \item applying logarithm and using definition of kl divergence we get
}

\begin{frame}
    \frametitle{Maximum Likelihood Estimator}
    \begin{minipage}{0.8\textwidth}
        Properties:
        \begin{itemize}
            \onslide<2->{\item $\k{\theta}{ML}_m$ minimizes dissimilarity between observed distribution $\hat{p}_{\text{data},m}$ and model distribution $\p{model}$}
            \onslide<4->{\item Fastest convergence to true parameter $\theta^\ast$}
        \end{itemize}
    \end{minipage}
    \begin{minipage}{0.15\textwidth}
        \begin{figure}
            \onslide<3->{\includegraphics[width=1.5\linewidth]{img/deep learning/kl_divergence.pdf}}
            \vspace{-2em}
            \onslide<9->{\captionsetup{margin={3em,-5em}}
            \caption{\cite{textbook}}}
        \end{figure}
    \end{minipage}
    
    \onslide<5->{Generalization to conditional probabilities $p\c{y}{x}$:
    \vspace{-0.8em}
    \begin{flalign*}
        \theta^{(ML)}_{m} := \argmin_\theta \frac{1}{m} \sum_{i=1}^{m} - \log{\p{model}\c{x_i}{y_i; \theta}}
    \end{flalign*}}
    
    
    \onslide<6->{Connection between MLE and UAT:}
    \begin{itemize}
        \onslide<7->{\item MLE: framework for fitting $\p{model}$ to $\p{data}$ via $\theta$}
        \onslide<8->{\item UAT: guarantees that model can approximate $\p{data}$}
    \end{itemize}
\end{frame}

\note[itemize]{
    \item ml estimator has the following properties
    \item as we saw, minimizes dissimilarity between observed distribution and model distribution by minimizing kl-divergence
    \item If real data-generating distribution is one of this index family with parameter $\theta^*$ then ml estimator shows fastest convergence with increasing m
    \item can generalize concept to conditional probabilities
    \item connection between MLE and UAT:
    \item MLE is framework for optimizing model parameters to fit the probability distribution model learns to the real distribution
    \item UAT guarantees that model has capacity to approximate real distribution
    \item above properties make ML-Estimation preferred parameter estimator in machine learning
}

\begin{frame}
    \frametitle{Cost Functions and Gradient Descent}
    
    Cost functions:
    \begin{itemize}
        \onslide<2->{\item Often sum over per-example loss: $J(\theta) = \frac{1}{m} \sum_{i=1}^
        {m}L(x_i, y_i; \theta)$}
        \onslide<3->{\vspace{0.2cm}
        \item MLE: $L(x, y, \theta) = -\log{\p{model}\c{y}{x; \theta}}$}
    \end{itemize}
    
    \onslide<4->{\vspace{0.4cm}
    
    Gradient descent:}
    \begin{itemize}
        \onslide<5->{\item Iteratively minimize $J$: $\ \theta_{k+1} = \theta_k - \epsilon \nabla_\theta J(\theta_k)$}
        \onslide<7->{\item Problem: computational cost of one step $\propto m$}
    \end{itemize}

    \begin{figure}
        \onslide<6->{\includegraphics[width=0.6\textwidth]{img/gradient_descent.png}}
        \onslide<8->{\caption{Visualization of gradient descent \cite{arnold}}}
    \end{figure}
\end{frame}

\note[itemize]{
    \item for finding the optimal parameters we need some cost function we can minimize
    \item cost functions often sum over per example loss
    \item MLE makes it easy because optimal parameter is already defined via sum of per-example losses
    \item this form of per-example loss known as log-loss
    \item since gradient always points in direction of steepest ascent can, iteratively minimize by taking small steps in opposite direction
    \item problem: computational cost of one iteration proportional to number of examples, which tend to be very large to get good fit
}

\begin{frame}
    \frametitle{Stochastic Gradient Descent and Momentum}
    Stochastic Gradient Descent (SGD):
    \vspace{-0.2cm}
    \onslide<2->{\begin{equation*}
        \nabla_\theta J(\theta) \approx g = \frac{1}{m^\prime} \sum_{i=1}^{m^\prime} \nabla_\theta L(x_i, y_i; \theta), \quad m^\prime \ll m
    \end{equation*}\\}
    \onslide<3->{\vspace{-0.2cm}
    \arrow Use randomly drawn minibatch of examples to estimate $\nabla_\theta J$}

    \begin{minipage}[b]{0.51\textwidth}
        \vspace{0.4cm}
        \onslide<5->{Momentum:}
        \onslide<6->{\vspace{-0.3cm}
        \begin{flalign*}
            & v_{k+1} = \alpha v_k - \epsilon \nabla_\theta J(\theta_{k}) &&\\
            & \theta_{k+1} = \theta_k + v_{k+1} &&\\
            & \alpha \in [0,1)&&
        \end{flalign*}}
        \vspace{-0.7cm}
        \begin{itemize}
            \onslide<7->{\item Weighted average stabilizes gradient}
            \onslide<8->{\item Analogy: particle with friction, velocity $v$}
        \end{itemize}
    \end{minipage}
    \begin{minipage}[b]{0.47\textwidth}
        \begin{figure}
            \onslide<4->{\includegraphics[width=0.7\linewidth]{img/deep learning/momentum.pdf}}
            \vspace{-0.2cm}
            \hspace{3.4em}
            \onslide<9->{\caption{Comparison of GD\\ \textbf{\textcolor{red}{with}} and \textbf{without} momentum \cite{textbook}}}
            \vspace{-0.7cm}
        \end{figure}
    \end{minipage}
\end{frame}

\note[itemize]{
    \item this is where stochastic gradient descent comes in
    \item compute estimate for gradient by using much smaller number of randomly drawn examples
    \item so called minibatch
    \item another problem is that gradient descent can tend to show oscillatory pattern if strong difference in steepness
    \item one solution can be using momentum
    \item computes a exponentially decaying weighted average of gradient this stabilizes the gradient descent
    \item particle moving in potential with friction
}

\begin{frame}
    \frametitle{Backpropagation - General Formulation}
    \onslide<2->{Algorithm for computing $\frac{\partial F}{\partial X}, \ X \in \set{A, \ldots, F}{}$:}
        \begin{itemize}
            \onslide<3->{\item Start at node $F$, $\frac{\partial F}{\partial F} = 1$}
            \onslide<4->{\item Propagate backwards along graph, \textbf{re-using partial derivatives} of previous nodes, e.g.: $\frac{\partial F}{\partial C} = \frac{\partial F}{\partial E}\frac{\partial E}{\partial C}$}
        \end{itemize}
    
    \begin{figure}%
        \centering%
        \only<2-4>{\includegraphics[width=5cm]{img/backprop/backprop_1.pdf}}%
        \only<5>{\includegraphics[width=5cm]{img/backprop/backprop_2.pdf}}%
        \only<6>{\includegraphics[width=5cm]{img/backprop/backprop_3.pdf}}%
        \only<7>{\includegraphics[width=5cm]{img/backprop/backprop_4.pdf}}%
        \only<8>{\includegraphics[width=5cm]{img/backprop/backprop_5.pdf}}%
        \only<9->{\includegraphics[width=5cm]{img/backprop/backprop_6.pdf}}%
        \onslide<11->{\caption{Example of Backpropagation through a graph \cite{dag}\textsuperscript{*}}}
    \end{figure}
    \onslide<10->{\arrow Deep/complex graphs: dramatic speedup compared to recomputing all intermediate $\frac{\partial F}{\partial X}$ for each node}
\end{frame}

\note[itemize]{
    \item To compute the gradient of cost function we make use of backpropagation
    \item Very generally, it is algorithm for computing partial derivatives with respect to nodes in computational graph
    \item efficiently using the chain rule
    \item Start at last node
    \item propagate backwards along graph, re-using partial derivatives of previous nodes
    \item e.g. partial derivative of F with respect to C can reuse partial derivative with respect to E etc.
    \item benefit especially for deep or complex graphs: dramatically speeds up computation compared to recomputing intermediate derivatives
    \item also, memory savings: we can throw away the partial derivatives of earlier nodes if no more reference later and we have processed the value, e.g. remove dFdC after dFdB
}

\begin{frame}
    \frametitle{Backpropagation for MLPs}
    
    \begin{minipage}{0.4\textwidth}
        \onslide<2->{Feedforward:}
        \begin{flalign*}
            \onslide<3->{& \k{z}{k} = \k{W}{k} \k{a}{k-1} + \k{b}{k} &&\\ }
            \onslide<4->{& \k{a}{k} = \k{g}{k}(\k{z}{k}) &&}
        \end{flalign*}
    \vspace{0.5cm}\\
    \onslide<5->{Backpropagation:\\}
    \onslide<10->{(let $\br{\frac{\partial u}{\partial v}}_{ij} = \frac{\partial u_j}{\partial v_i}$)    }
    \end{minipage}
    \begin{minipage}{0.57\textwidth}
        \begin{figure}
            \includegraphics[width=0.9 \linewidth, right]{img/dropout/backprop_mlp.pdf}
            \onslide<12->{\caption{Visualization of a MLP \cite{dropout}\textsuperscript{*}}}
        \end{figure}
    \end{minipage}
    \vspace{-0.3cm}
    \begin{flalign*}
        \onslide<6->{& \frac{\partial L}{\partial \k{z}{k}} = \frac{\partial \k{z}{k+1}}{\partial \k{z}{k}}\frac{\partial L}{\partial \k{z}{k+1}}} \onslide<7->{=  \frac{\partial \k{g}{k}(\k{z}{k})}{\partial\k{z}{k}} {\k{W}{k+1}}^T \frac{\partial L}{\partial \k{z}{k+1}} &&\\}
        \onslide<8->{& \frac{\partial L}{\partial \k{W}{k}_{ij}} = \frac{\partial L}{\partial \k{z}{k}_i} \k{a}{k-1}_j,} \onslide<9->{\quad \frac{\partial L}{\partial \k{b}{k}_i}
        = \frac{\partial L}{\partial \k{z}{k}_i} &&}
    \end{flalign*}
    \onslide<11->{\arrow During feedforward    store the $\k{z}{k}, \k{a}{k}$, during backprop $\frac{\partial L}{\partial \k{z}{k}}$}
\end{frame}

\note[itemize]{
    \item Due to usage of matrices and vectors in MLPs, backpropagation can be formulated more straightforward
    \item main backpropagation step is reusing previous dL/dz
    \item using this, we can then get the derivatives with respect to weights and biases
    \item important: store the z, a along the way during feedforward and the dL/dz during backpropagation
}

\begin{frame}
    \frametitle{Regularization Techniques}
    
    \begin{itemize}
        \onslide<2->{\item Parameter norm constraints (e.g. $\Omega(\theta) = \norm{\theta}_2$):\\}
        \vspace{0.15cm}
        \onslide<3->{implicit: minimize $\tilde{J}(\theta) = J(\theta) + \alpha \Omega(\theta), \quad \alpha \geq 0$ \\}
        \vspace{0.15cm}
        \onslide<4->{explicit: project $\theta$ back after gradient step, s.t. $\Omega(\theta) < c$}
        \onslide<6->{\item Dataset augmentation: generate new examples by transforming existing ones}
        \onslide<8->{\item Early stopping: stop training as soon as generalization error starts increasing again }
    \end{itemize}
    \begin{figure}
        \hspace{-2em}
        \onslide<5->{$\vcenter{\hbox{\includegraphics[width=0.24\textwidth]{img/deep learning/norm_constraint.pdf}}}$}
        \onslide<7->{$\vcenter{\hbox{\includegraphics[width=0.24\textwidth]{img/deep learning/dataset_augmentation.pdf}}}$}
        \onslide<9->{$\vcenter{\hbox{\includegraphics[width=0.55\textwidth]{img/deep learning/early_stopping.pdf}}}$}
        \hspace{-2em}
        \onslide<10->{\caption{Illustration of parameter norm constraints, dataset augmentation and early stopping \cite{textbook}\textsuperscript{*}}}
    \end{figure}
\end{frame}

\note[itemize]{
    \item To avoid overfitting, we use regularization techniques
    \item examples are
    \item parameter norm constraints where Omega is in some way a measure of the capacity of the model
    \item implicit constraint by minimizing regularized cost function with norm penalty
    \item explicit: perform gradient step, if theta violates constraint after, project it back into the constraint area
    \item Constraints restrict area in parameter space that can be chosen by model
    \item Dataset augmentation: best way to improve generalization is use more examples
    \item generate new examples by transforming existing ones, e.g. translating or rotating images etc.
    \item very easy regularization technique: stop training as soon as overfitting starts to occur, i.e. generalization error starts increasing again
    \item Another powerful regularization technique is dropout
}

\begin{frame}
    \frametitle{Dropout}
    Train ensemble of all subnetworks of base network:
    \begin{itemize}
        \onslide<2->{\item Randomly remove non-output nodes for each training example}
        \onslide<4->{\item MLP: $\k{a}{k}_n = \k{\mu}{k}_n \k{g}{k}(\k{z}{k})_n\,, \ \k{\mu}{k}_n$ Bernoulli-distributed}
        \onslide<5->{\item For non-training inputs return average of ensemble results or
        equivalently scale outgoing weights by $p(\k{\mu}{k}_n = 1)$}
    \end{itemize}
    \vspace{-0.5cm}
    \begin{minipage}{0.49\textwidth}
        \vspace{0.3cm}
        \onslide<6->{Why does it work:}
        \begin{itemize}
            \onslide<7->{\item Regularizes nodes to work well in different contexts}
            \onslide<8->{\item Form of dataset augmentation
            \arrow create new inputs by injecting noise}
        \end{itemize}
    \end{minipage}
    \hspace{1em}
    \begin{minipage}{0.4\textwidth}
        \vspace{1cm}
        \begin{figure}
            \onslide<3->{\includegraphics[width=0.8\linewidth]{img/dropout/dropout.pdf}}
            
            \onslide<9->{\caption{Subnetwork formed by randomly sampling $\mu$ \cite{dropout}\textsuperscript{*}}}
        \end{figure}
    \end{minipage}
\end{frame}

\note[itemize]{
    \item Dropout technique trains ensemble of networks, specifically ensemble of all subnetworks of base network
    \item does that by randomly removing non-output nodes from base network for each training example
    \item for MLPs that is as easy as sampling mask from bernoulli-distribution and applying it to the activations during feedforward
    \item during inference, i.e. for processing real world input with network need to return average of all results in the ensemble, can be numerically non-feasible
    \item experimentally shown that this is same as using full base network and scaling down outgoing weights with probability of the node being included
    \item intuitive explanation: make sure that expected input at test time is same as during training and not too high
    \item why does this technique work:
    \item regularizes nodes to work well in different contexts, e.g. if other nodes are randomly missing
    \item can be seen as form of dataset augmentation, essentially creating new inputs by injecting noise into the network in form of the masks 
}

\begin{frame}
    \frametitle{Convolutional Neural Networks (CNNs)}
    Specialized neural networks for inputs with grid-like topology
    
    \begin{minipage}[t]{0.49\textwidth}
        \vspace{1em}
        \onslide<2->{Define convolution operation for CNNs with 2D-inputs as:
        \begin{equation*}
            F(i,j) = \sum_{m, n} I(i+m, j+n)K(m,n)
        \end{equation*}}
        
        \onslide<4->{Idea: move the kernel $K$ accross the input $I$ like a window into the array}
    \end{minipage}
    \begin{minipage}[t]{0.49\textwidth}
        \begin{figure}
            \onslide<3->{\includegraphics[width=\linewidth]{img/deep learning/2d_conv.pdf}}
            \hspace{-4.5em}
            \captionsetup{margin={1cm,-0.5cm}}
            \onslide<5->{\caption{Visualization of convolution with a $2\PLH2$ kernel \cite{textbook}}}
        \end{figure}
    \end{minipage}
\end{frame}

\note[itemize]{
    \item final topic: introduce convolutional neural networks
    \item specialized neural networks for inputs with grid structure, most notably images 
    \item for discrete, finite-sized, 2D inputs such as images, we define the convolution operation as such
    \item this is formally cross-correlation but for most aspects of CNNs this doesnt matter and more convenient than minus in convolution
    \item Example output after convolving I input with 2x2-kernel K
    \item Intuition for operation: move kernel across input like window in array and perform element-wise multiplication and summation at each position
}

\begin{frame}
    \frametitle{Architecture of a Convolutional Layer}
    {\setlength{\leftmargini}{0cm}
    \begin{enumerate}
        \onslide<2->{\item Compute multiple feature maps $F$ in parallel using different kernels}
        \onslide<4->{\item Apply element-wise activation to the $\k{F}{k}_{ij}$}
        \onslide<5->{\item Summarize the activations using a pooling function}
    \end{enumerate}}
    
    \vspace{2em}
    \begin{minipage}[b]{0.52\textwidth}
        \begin{figure}
            \onslide<3->{\includegraphics[width=0.9\linewidth]{img/conv_layer.png}}
            \onslide<7->{\caption{Composition of a convolutional layer \cite{conv_layer}}}
        \end{figure}
    \end{minipage}
    \begin{minipage}[b]{0.45\textwidth}
        \begin{figure}
            \onslide<6->{\includegraphics[width=0.8\linewidth]{img/deep learning/pooling.pdf}}
            \onslide<7->{\caption{Max-pooling returns maximum activation in area \cite{textbook}}}
        \end{figure}
    \end{minipage}
\end{frame}

\note[itemize]{
    \item Convolutional layers mostly have the following architecture
    \item First compute multiple feature maps F in parallel using different kernels
    \item Idea: each kernel will respond to different kind of feature in the input
    \item Then apply element-wise activation to the feature-maps
    \item Finally, pool the activations in the feature maps, performing some kind of summarization of the previously detected features
    \item example: max pooling - for each unit, return the maximum activation in neighbourhood of fixed size
}

\begin{frame}
    \frametitle{The Reasoning behind CNNs}
    \hspace{-1em}
    \begin{minipage}{0.6\textwidth}
        Practical advantages over MLPs:
        \begin{itemize}
            \onslide<2->{\item Sparse connectivity: not all input and output nodes are connected}
            \onslide<4->{\item Parameter sharing: kernel parameters reused for each node in a feature map}
            \onslide<5->{\item Downsampling: fixed-size outputs by adapting stride to input size}
        \end{itemize}
    \end{minipage}
    \hspace{0.5em}
    \begin{minipage}{0.35\textwidth}
        \vspace{-3em}
        \begin{figure}
            \onslide<3->{\includegraphics[width=0.4\linewidth]{img/deep learning/sparse connectivity_1.pdf}
            \includegraphics[width=0.4\linewidth]{img/deep learning/sparse connectivity_2.pdf}}
            \hspace{-3em}
            \captionsetup{margin={2em,-5em}}
            \onslide<8->{\caption{Connectivity comparison: convolutional layer with $3\PLH1$ kernel vs. fully connected \cite{textbook}}}
        \end{figure}
    \end{minipage}

    \onslide<7->{\arrow Decreased computational cost, increased input-flexibility\\
    \vspace{2em}}
    
    \begin{minipage}[b]{0.5\textwidth}
        \begin{figure}
            \onslide<8->{\caption{Max-pooling with stride 2 \cite{textbook}}}
        \end{figure}
    \end{minipage}
    \begin{minipage}[b]{0.45\textwidth}
        \begin{figure}
            \onslide<6->{\includegraphics[width=\linewidth]{img/deep learning/downsampling.pdf}}
            \hspace{-3em}
        \end{figure}
    \end{minipage}
    
\end{frame}

\note[itemize]{
    \item Why use convolutional layers over fully connected layers in MLPs?
    \item firstly, sparse connectivity: not nodes between layers are connected
    \item e.g. kernel of size 3 always connects 3 input nodes to 1 output node, compared to fully connected where all input connected to all output
    \item parameter sharing: the same kernel is used for the entire convolution, compared to mlp where each input node has its own parameters connecting it to output nodes 
    \item Downsampling: we can realize fixed size outputs by adapting stride of our operations to the size of the input, e.g. use bigger stride for larger inputs downsample the output
    \item why these things are advantageous, can be seen especially for image processing:
    \item images are typically several million pixels in size, that would imply order of million weights for first layer alone, arithmetic operations on similar order of magnitude
    \item image dimensions are generally very varied, MLPs can only deal with fixed input size
}

\begin{frame}
    \frametitle{The Reasoning behind CNNs}
    \hspace{-1.5em}
    \begin{minipage}{0.7\textwidth}
        Problem-specific arguments:
        {%
        \setlength{\leftmarginii}{1em}
        \begin{itemize}
            \onslide<2->{\item Feature-extraction often only needs local information}
            \onslide<4->{\item Introduction of invariances by pooling and parameter sharing}
            \onslide<6->{\item Biological analogy: visual cortex}
            \begin{itemize}
                \onslide<7->{\item Two-dimensional structure, mirroring image}
                \onslide<8->{\item Cell activity influenced only by localized receptive field}
                \onslide<9->{\item Deeper brain layers extract increasingly complex features using the same structure}
            \end{itemize}
        \end{itemize}}
    \end{minipage}
    \hspace{1em}
    \begin{minipage}{0.2\textwidth}
        \vspace{-2em}
        \begin{figure}
            \onslide<5->{\includegraphics[width=1.5\linewidth]{img/deep learning/pooling_invariance.pdf}}
            \hspace{-10em}
            \captionsetup{margin={1em,-5em}}
            \vspace{-1em}
            \onslide<10->{\caption{Invariance by pooling feature maps responding to different orientations \cite{textbook}}}
        \end{figure}
    \end{minipage}
    \onslide<3->{\begin{minipage}[b]{0.49\textwidth}
        \begin{figure}
            \caption{Edge detection only needs local information ($K = \brr{- 1, 1}$) \cite{textbook}}
        \end{figure}
    \end{minipage}
    \begin{minipage}[b]{0.49\textwidth}
        \includegraphics[width=0.9\linewidth]{img/deep learning/edge_detection.pdf}
    \end{minipage}}
\end{frame}

\note[itemize]{
    \item there are also problem-specific arguments for use of CNNs for processing grid-like data
    \item features in grid-like data can often be extracted with only knowledge of local structure \arrow analogy to kernel working like a window
    \item for example, edge information can be extracted using small kernel that computes difference in brightess of two neighboring pixels
    \item by pooling an parameter sharing, we can explicitly of implicitly introduce invariances to our model
    \item e.g. pooling across outputs of different kernels that each respond to different orientation of number 5 introduces invariance to rotation of input
    \item also, we do not necessarily care where in grid feature occurs, which gives further argument for using same kernel across entire image
    \item Finally, for image processing we can draw biological analogy with visual cortex in mammals
    \item Similar to feature maps, it has two dimensional structure and images in retina are spatially mapped to simple cells in visual cortex
    \item Cell activity here is only influenced by a locally receptive field, similar to kernel only connecting several input nodes to one output node
    \item Also, it is assumed that further layers in image processing pipeline in brain extract increasingly complex features using the same structure as this first layer
    \item This inspired the creation of CNNs and is probably the most successful example machine learning inspired by neurobiology
}

\begin{frame}
    \frametitle{Training a MLP to Classify Digits with the MNIST Dataset}
    
    \begin{minipage}{0.82\textwidth}
        Model Architecture:
        \begin{itemize}
            \onslide<2->{\item Input layer: width 784 \arrow images are $28\PLH28$px}
            \onslide<4->{\item 4 hidden layers, each width 100, with ReLU activation}
            \onslide<5->{\item Output layer: multinomial logistic regression, width 10 for digits 0-9}
        \end{itemize}
        \onslide<6->{\arrow $N=5$ non-input layers\\}
    \end{minipage}
    \begin{minipage}{0.15\textwidth}
        \begin{figure}
            \onslide<3->{\includegraphics[width=1.3\linewidth]{img/nist_sample.png}}
            \captionsetup{margin={0em,-2em}}
            \onslide<10->{\caption{Sample from the MNIST-Dataset}}
        \end{figure}
    \end{minipage}
    
    \vspace{0.3em}
    
    \onslide<7->{Multinomial logistic regression:\\}
    \onslide<8->{Affine transformation returns parameters $z_i$ for logistic model:}
    \begin{flalign*}
        \onslide<8->{z :=& \k{z}{N} = \k{W}{N} \phi(x; \theta) + \k{b}{N}} \\
        \onslide<9->{p(c=i) =& \frac{\exp(z_i)}{\sum_{j}\exp(z_j)} =: \text{softmax}(z)_i}
    \end{flalign*}
\end{frame}

\note[itemize]{
    \item Finally for the programming part of my seminar topic:
    \item Programmed MLP using numpy to learn to classify digits using MNIST dataset
    \item Chose the following model architecture ...
    \item Multinomial logistic regression compsed of following steps
    \item Hidden layers form kernel extracting features
    \item Followed by affine transformation which returns logistic units, e.g. parameters for the logistic model
    \item the logistic model then defines the probability of the input being of class i, in our case, the image being digit i as
}

\begin{frame}
    \frametitle{Training a MLP to Classify Digits with the MNIST Dataset}
    MLE cost function:
    \begin{flalign*}
        \onslide<2->{L(x, y; \theta) &= -\log{\p{model}\c{y}{x; \theta}}} \onslide<3->{= -\log*{\text{softmax}(z(x))_y} \\}
        \onslide<4->{J(\theta) &= \frac{1}{m} \sum_{k=1}^{m} L(x_k, y_k; \theta)}
    \end{flalign*}

    \onslide<5->{Optimization and Regulation:}
    \begin{itemize}
        \onslide<6->{\item SGD using batchsize 20 with Adam optimizer}
        \onslide<7->{\item Early Stopping}
        \onslide<8->{\item Dropout with $p=$ and $p=$ for keeping input and hidden nodes}
    \end{itemize}

    \vspace{1em}
    \onslide<9->{Source code: \url{https://github.com/philipobi/seminar}}
\end{frame}

\note[itemize]{
    \item this probability is our p model, then the ML per-example loss can be readily defined like so, which immediately gives us the cost function
    \item For optimization used Stochastic gradient descent with bacthsize 20 and the Adam optimizer
    \item In some sense, improvement upon the Momentum based gradient descent mentioned earlier
    \item To regularize early stopping and dropout      
}

\begin{frame}[allowframebreaks]
    \nocite{*}
    \frametitle{Image Sources}
    \printbibliography[notkeyword={info},title={Image Sources}]
\end{frame}

\begin{frame}[allowframebreaks]
    \frametitle{References}
    \printbibliography[notkeyword={img},title={References}]
\end{frame}
    
\end{document}