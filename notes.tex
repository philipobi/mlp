\documentclass{article}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{relsize}

\setlength{\parindent}{0pt}

\def\noval{}

\DeclarePairedDelimiter{\br}{(}{)}
\DeclarePairedDelimiter{\brr}{[}{]}
\DeclarePairedDelimiter{\brs}{\{}{\}}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}


\makeatletter
\let\oldbr\br
\def\br{\@ifstar{\oldbr}{\oldbr*}}
\makeatother

\makeatletter
\let\oldbrr\brr
\def\brr{\@ifstar{\oldbrr}{\oldbrr*}}
\makeatother

\makeatletter
\let\oldbrs\brs
\def\brs{\@ifstar{\oldbrs}{\oldbrs*}}
\makeatother

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
\makeatother

\makeatletter
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\makeatletter
\let\oldlog\log
\def\log{\@ifstar\@log\@@log}
\def\@log#1{\oldlog\br{#1}}
\def\@@log#1{\oldlog#1}
\makeatother


\makeatletter
\newcommand{\set}[2]{
    \def\@delim{\;\@ifstar{\middle|}{:}\;}
    \ifx\noval#2\noval%
        \left\{#1\right\}
    \else
        \left\{#1\@delim#2\right\}
    \fi
}
\makeatother

\newcommand{\E}[2][]{%
    \ifx\noval#1\noval%
        \mathbb{E}\brr{#2}
    \else
        \mathbb{E}_{#1}\brr{#2}
    \fi
}
\newcommand{\var}[2][]{%
    \ifx\noval#1\noval%
        \text{Var}\br{#2}
    \else
        \text{Var}_{#1}\br{#2}
    \fi
}
\newcommand{\cov}[2][]{%
    \ifx\noval#1\noval%
        \text{Cov}\br{#2}
    \else
        \text{Cov}_{#1}\br{#2}
    \fi
}



\newcommand{\arrow}{$\rightarrow\;$}

\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\text{T}}
\renewcommand{\v}{\bm}
\renewcommand{\c}[2]{\left(#1\;\middle|\;#2\right)}
\renewcommand{\det}[1]{\text{det}\br{#1}}
\newcommand{\ml}[1]{\mathlarger{#1}\,}
\newcommand{\dkl}[2]{D_{\text{KL}}(#1\;\Vert\;#2)}
\renewcommand{\S}{\mathbb{S}}
\newcommand{\func}[3]{#1:#2\rightarrow#3}

\begin{document}
\section*{Probability}
\subsubsection*{Random Variables}
Variable $X$ that can take on different values $x$ randomly. 

\subsubsection*{Probability Distributions}
Discrete variables: probability mass function \\
Continous variables: probability density \\
Define $p(x) := p(X=x)$

\subsubsection*{Marginal Probability}
\begin{flalign*}
    p(x) = \int p(x,y) dy &&
\end{flalign*}

\subsubsection*{Conditional Probability}
Probability of some event, given that other event has happened:
\begin{flalign*}
    P\c{Y=y}{X = x} = \frac{P(Y=y, X=x)}{P(X = x)} & &
\end{flalign*}
Chain rule:
\begin{flalign*}
    P(x_1, \ldots, x_n) = \prod_{i=1}^n P\c{x_i}{x_1, \ldots, x_{i-1}} &&
\end{flalign*}

\subsubsection*{Independence, Conditional Independence}
Random variables $X, Y$ independent ($X \perp Y$):
\begin{flalign*}
    \forall x \in X, y \in Y : \quad p(X=x, Y=y) = p(X=x)p(Y=y) &&
\end{flalign*}

Conditionally independent ($X\perp Y | Z$):
\begin{flalign*}
    \forall x \in X, y \in Y, z \in Z : \quad p(X=x,Y=y,Z=z) = p\c{X=x}{Z=z} p\c{Y=y}{Z=z} &&
\end{flalign*}

\subsubsection*{Expectation, Variance and Covariance}
Expectation / expected value of function $f(x)$ w.r.t. $P(X)$ is average value that $f$ takes when $x$ drawn from $P$
\begin{flalign*}
    \E[X \sim p]{f(x)} := \int p(x)f(x)dx &&
\end{flalign*}
Variance is measure of how much $f(x)$ vary as different values $x$ from distribution sampled:
\begin{flalign*}
    \var{f(x)} := \E{\br{f(x) - \E{f(x)}}^2} &&
\end{flalign*}
Low variance $\rightarrow$ values of $f(x)$ cluster near expected value \\
Variance = square of standard deviation \\
\\
\\
Covariance gives sense of how two values linearly related to each other:
\begin{flalign*}
    \cov{f(x), g(y)} := \E{\br{f(x)-\E{f(x)}}\br{g(y)-\E{g(y)}}} &&
\end{flalign*}
Covariance matrix of random vector $\v{x} \in \R^n$:
\begin{flalign*}
    \cov{\v{x}}_{i,j} = \cov{x_i, x_j} &&
\end{flalign*}
\\
Identities:
\begin{flalign*}
    & \mu := \E{x} && \\
    & \sigma^2 := \var{x} = \E{\br{x - \E{x}}^2} = \E{x^2} - 2 \, \E{x \, \E{x}} + \E{\E{x}^2} = \E{x^2} - \E{x}^2 &&
\end{flalign*}

\subsubsection*{Probability Distributions}
Normal / Gaussian distribution:
\begin{flalign*}
    &p(x) = \mathcal{N}(x; \mu, \sigma^2) := \sqrt{\frac{1}{2\pi\sigma}} \exp\br{-\frac{1}{2\sigma^2}(x-\mu)^2} &&\\
    \intertext{with properties:}
    &\E[X \sim \mathcal{N}(x; \mu, \sigma^2)]{x} = \mu &&\\
    &\var[x \sim \mathcal{N}(x; \mu, \sigma^2)]{x} = \sigma^2 &&
\end{flalign*}
\\
$n$-variate normal distribution:
\begin{flalign*}
    \mathcal{N}(\v{x}; \v{\mu}, \v{\Sigma}) %
    = \sqrt{\frac{1}{(2\pi)^n \text{det}(\v{\Sigma})}} \exp\br{-\frac{1}{2} (\v{x} - \v{\mu})^T \v{\Sigma}^{-1}(\v{x}- \v{\mu})} &&
\end{flalign*}
\\
\\
Empirical distribution:
\begin{flalign*}
    \hat{p}(\v{x}) = \frac{1}{m} \sum_{i=1}^{m} \delta(\v{x} - \v{x}_i) &&
    \intertext{Puts probability mass $\frac{1}{m}$ on each of the $m$ observed samples $\v{x}_i$}
\end{flalign*}

\subsubsection*{Bayes Rule}
Obtaining $P\c{X}{Y}$ given $P\c{Y}{X}$:
\begin{flalign*}
    &P\c{X}{Y} = \frac{ P(X) P\c{Y}{X} } { P(Y) } &&\\
    \intertext{We can usually compute:}
    &P(y) = \sum_{x} P\c{y}{x} P(x) &&
\end{flalign*}

\subsubsection*{Functions of random variables}
Given $y := g(x)$ invertible and $x \sim p_x(x)$
\begin{flalign*}
    & \int_{a}^{b} p_x(x) dx \overset{!}{=} \int_{g(a)}^{g(b)} p_y(y) dy = \int_{a}^{b} p_y(g(x)) \frac{dg}{dx} dx &&\\ 
    & \implies p_x(x) = p_y(g(x)) \frac{dg}{dx} &&\\
    \intertext{In higher dimensions:}
    & p_{\v{x}} (\v{x}) = p_{\v{y}} (\v{g}(\v{x})) \abs{\det{\frac{\partial \v{g}}{\partial \v{x}}}} &&
\end{flalign*}

\section*{Information Theory}
\subsubsection*{Information content}
Desired properties of information measure:
\begin{itemize}
    \item Information content of event should depend on its likelihood (low/high likelihood $\leftrightarrow$ high/low information)
    \item Independent events should have additive information: e.g. $I(2\times \text{Heads}) \overset{!}{=} 2\cdot I(\text{Heads})$
\end{itemize}

Define self-information of event $X=x$ as:
\begin{flalign*}
    I(X=x) := -\log{p(X=x)} && 
\end{flalign*}

\subsubsection*{Shannon entropy}
Quantification of uncertainty in probability distribution:
\begin{flalign*}
    H(X) := \E[X \sim p]{I(x)} &&
\end{flalign*}
$\rightarrow$ Expectation value of self-information

\subsubsection*{Kullback-Leibler (KL) divergence}
Measure of how different distributions $P(X)$ and $Q(X)$ over same variable $X$ are:
\begin{flalign*}
    \dkl{P}{Q} := \E[X \sim P]{\log{\frac{P(x)}{Q(x)}}} &&
\end{flalign*}
Properties:\\
$\dkl{P}{Q} \geq 0, \ \dkl{P}{Q} = 0 \iff P(X) = Q(X)$ \\ 
$\rightarrow$ concept of distance between distributions, however not symmetric so it makes difference which divergence is minimized \\
Proof: %https://stats.stackexchange.com/a/335201
\begin{flalign*}
    &\forall a > 0: \quad \log{a} \leq a - 1 &&\\
    & -\dkl{p}{q} = \sum_x p(x) \log{\frac{q(x)}{p(x)}} \leq \sum_x p(x) \br{\frac{q(x)}{p(x)} - 1} = \sum_x q(x) - \sum_x p(x) = 1 - 1 = 0 &&\\
    &\iff \dkl{p}{q} \geq 0
    \intertext{Since $\log{a} < a - 1$ for $a\neq 1$:}
    & -\dkl{p}{q} = \sum_x p(x) \br{\frac{q(x)}{p(x)} - 1} = 0 \iff \forall x: \quad p(x) = q(x) &&
\end{flalign*}

\subsubsection*{Cross-Entropy}
\begin{flalign*}
    H(P,Q) = -\E[X\sim p]{\log{Q(x)}} &&
\end{flalign*}
Minimizing cross-entropy with respect to $Q$ equivalent to minimizing KL divergence, since variation of $Q$ does not affect omitted term

\section*{Numerical Computation}
\subsubsection*{Overflow and Underflow}
Discrete representation of real numbers can lead to accumulation of rounding error resulting in algorithm failure \\
Underflow: number near zero rounded to zero $\rightarrow$ problem: zero division, $\log 0$
Overflow: large numbers approximated to $\pm\infty$ $\rightarrow$ problem: NaN in further arithmetic

\subsubsection*{Poor Conditioning}
Conditioning: how rapidly does function change for small variation in inputs $\rightarrow$ possibly large changes due to rounding error

\subsubsection*{Gradent-Based Optimization}
Problem in machine / deep learning: \\
functions high-dimensional, often many local minima, saddle points surrounded by very flat regions \\
difficult to limit function space used
\begin{flalign*}
    & \text{Jacobian} \ J_{i,j} := \frac{\partial f(\v{x})_i}{\partial x_j} && \\
    &\rightarrow \text{Jacobian of scalar function is transpose of its gradient}&&\\
    & \text{Hessian} \ H_{i,j} := \frac{\partial^2 f(\v{x})}{\partial x_i \partial x_j} \quad &&\\
    &\rightarrow \text{Jacobian of gradient of a scalar function} &&
\end{flalign*}
Interchangeability of partial derivatives $\implies \; \v{H}$ symmetric, can be decomposed in basis of eigenvectors \\
Eigenvalue = second derivative in direction of corresponding eigenvector

\begin{flalign*}
    \v{H} \; \begin{cases*}
        \text{positive definite} &$\implies$ local minimum \\
        \text{negative definite} &$\implies$ local maximum \\
        \text{pos. and neg. eigenvalues} &$\implies$ saddle point \\
        \text{at least one zero eigenvalue} &$\implies$ inconclusive in that direction \\
    \end{cases*} 
\end{flalign*}
Large differences in magnitude of Hessian eigenvalues ($\rightarrow$ curvature) problematic due to overshooting with gradient descent
\begin{flalign*}
    f(\v{x}) \approx f(\v{x}_0) + (\v{x} - \v{x}_0)^T \v{g} + \frac{1}{2} (\v{x} - \v{x}_0)^T \v{H} (\v{x} - \v{x}_0) &&
\end{flalign*}
can be used to estimate optimal step size $\epsilon^*$ for gradient descent step with $\v{x} = \v{x}_0 - \epsilon \v{g}$ \\
\\
Optimization algorithms using only gradient $\rightarrow$ first-order optimization algorithms, using Hessian second-order \\
Convex optimization: most successful field of specialized optimization, problems in deep learning difficult to formulate as convex optimization, if at all only used in subroutines

\subsubsection*{Constrained Optimization}
Limit set of domain of optimization to set of feasible points $\S$ e.g. norm constraint\\
Approaches:
\begin{itemize}
    \item modify gradient descent taking constraint into account (e.g. projection into $\S$)
    \item design unconstrained problem whose solution can be converted into solution of original, e.g. for $\norm{\v{x}} \overset{!}{=} 1$ optimize $g(\theta) = f([\cos(\theta), \sin(\theta)]^T)$
    \item Karush-Kuhn-Tucker (KKT) approach:
    \begin{flalign*}
       & \S := \set{\v{x}}{\forall i, g_i(\v{x})=0 \ \text{and} \ \forall j, h_j(\v{x}) \leq 0} &&\\
       & L(\v{x}, \v{\lambda}. \v{\alpha}) := f(\v{x}) + \sum_{i} \lambda_i g_i(\v{x}) + \sum_j \alpha_j h_j(\v{x}) \qquad \text{(Generalized Lagrangian)} &&\\
       & \text{Constrained minimization $\rightarrow$ unconstrained optimization of $L$:} \ \min_{\v{x}}  \max_{\v{\lambda}} \max_{\v{\alpha}, \alpha_j \geq 0} L(\v{x}, \v{\lambda}, \v{\alpha}) &&\\
       & \text{Constraint violated} \implies \max_{\v{\lambda}} \max_{\v{\alpha}, \alpha_j \geq 0} L(\v{x}, \v{\lambda}, \v{\alpha}) = \infty &&\\
       & \text{Constraints satisfied} \implies \max_{\v{\lambda}} \max_{\v{\alpha}, \alpha_j \geq 0} L(\v{x}, \v{\lambda}, \v{\alpha}) = f(\v{x}) \ \text{with} \ \forall j:  h_j(\v{x}) = 0 \ \text{or} \ \alpha_j = 0&& 
    \end{flalign*}
\end{itemize}
\section*{Machine Learning Basics}
\subsubsection*{Definitions}

Broad definition of machine learning: "computer program is said to learn from experience [...] with respect to some class of tasks $T$ if its performance at tasks in $T$ [...] improves with experience" \\
\\
Form of representation of object is very important \\
Pieces of information included in representation of object known as features \\
e.g. Image - Representation: Array of Pixels, Features: Individual pixel values \\
General machine learning: discover mapping from representation to output \\
Representation learning: also learn the "optimal" representation of data \\
Example of representation learning: Autoencoders \\
Encoder function: converts data to different representation \\
Decoder function: converts new representation back to original format \\
Task: preserve as much information, while providing additional nice properties e.g. reduced size in new representation \\
Deep learning tackles representation learning by introducing representations that are expressed in terms of other, simpler representations \\
e.g. Image: Pixels \arrow Edges \arrow Contours \arrow Object parts \arrow Object identity \\
\\
Tasks usually described in terms of how system should process an example \\
Example: collection (usually vector) of features that has been measured, representing object that system should process \\
\\

\subsubsection*{Common tasks}
\begin{itemize}
    \item Classification of $k$ categories: produce function $\func{f}{\R^n}{R^k}$ \\
    Output: probability vector $\norm{\v{y}} = 1$ (can also be one-hot)\\
    Variation: missing inputs in $\v{x}$
    \item Regression: predict numerical value given some input: $\func{f}{\R^n}{R^m}$
    \item Structured output: output is vector with important relationships between elements e.g. output 
    token codes in language processing
    \item Probability density estimation: explicitly learn function $\func{p_\text{model}}{\R^n}{\R}$ on space examples were drawn from \\
    most above tasks already require implicit capture of structure of probability distribution
\end{itemize}

\subsubsection*{Types of experience / learning}
\begin{itemize}
    \item Unsupervised: experience dataset, learning useful properties of dataset structure \\
    e.g. probability density $p(\v{x})$ or clustering dataset
    \item Supervised: experience dataset of labeled examples $\br{\v{x}, \v{y}}$, learn to estimate $p\c{\v{y}}{\v{x}}$
    \item Semi-supervised: some examples have label/target, some not
    \item Multi-instance: collection of examples labeled but not each example
    \item Reinforcement: interaction with environment, feedback loop between system and experiences
\end{itemize}
Not formally defined terms, for example:
\begin{flalign*}
    &\text{Unsupervised problem can be split into supervised learning problems:} \ p(\v{x}) = \prod_{i} p\c{x_i}{x_1, \ldots, x_{i-1}} &&\\
    &\text{Supervised learning of $p\c{\v{y}}{\v{x}}$ can be replaced by unsupervised learning of $p(\v{x}, \v{y})$ and using:} &&\\
    & p\c{\v{y}}{\v{x}} = \frac{p(\v{x}, \v{y})}{\sum_{\v{y}} p(\v{x}, \v{y})}&&
\end{flalign*}

\subsubsection*{Capacity, Overfitting and Underfitting}
Generalization: capacity to perform well on previously unseen inputs \\
During learning: reduce training error \arrow error on training set \\
Difference between machine learning and simple optimization:
goal is also low generalization error \arrow expected error on new input \\
Estimate generalization error with error on test set, set of examples not used for training \\
\\
to be able to estimate performance on test set without observing it make assumptions about data-generating process - i.i.d. assumptions:
\begin{itemize}
    \item examples independent from each other
    \item training set and test set identically distributed
\end{itemize}
\arrow data generation can be described by distribution over single example $p_\text{data}(\v{x})$ \\
\\
Terminology:\\
Capacity: ability of model to fit function to be learned \\
Underfitting: model cannot obtain low enough training error, usually capacity too low \\
Overfitting: gap between training and generalization error too large, can be caused by too high capacity for task
\arrow more parameters than examples or "memorization" of training data \\
Best performance with capacity appropriate for true complexity of task and amount of training data \\
Learning theory: upper bound for discrepancy between training and generalization error grows with capacity, shrinks with number of training examples

\end{document}