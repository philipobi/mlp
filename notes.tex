\documentclass{article}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{relsize}

\setlength{\parindent}{0pt}

\DeclarePairedDelimiter{\br}{(}{)}
\DeclarePairedDelimiter{\brr}{[}{]}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}

\makeatletter
\let\oldbr\br
\def\br{\@ifstar{\oldbr}{\oldbr*}}
\makeatother

\makeatletter
\let\oldbrr\brr
\def\brr{\@ifstar{\oldbrr}{\oldbrr*}}
\makeatother

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
\makeatother

\makeatletter
\let\oldlog\log
\def\log{\@ifstar\@log\@@log}
\def\@log#1{\oldlog\br{#1}}
\def\@@log#1{\oldlog#1}
\makeatother

\def\noval{}
\newcommand{\E}[2][\noval]{%
    \ifx#1\noval%
        \mathbb{E}\brr{#2}
    \else
        \mathbb{E}_{#1}\brr{#2}
    \fi
}
\newcommand{\var}[2][\noval]{%
    \ifx#1\noval%
        \text{Var}\br{#2}
    \else
        \text{Var}_{#1}\br{#2}
    \fi
}
\newcommand{\cov}[2][\noval]{%
    \ifx#1\noval%
        \text{Cov}\br{#2}
    \else
        \text{Cov}_{#1}\br{#2}
    \fi
}
\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\text{T}}
\renewcommand{\v}{\bm}
\renewcommand{\c}[2]{\left(#1\;\middle|\;#2\right)}
\renewcommand{\det}[1]{\text{det}\br{#1}}
\newcommand{\ml}[1]{\mathlarger{#1}\,}
\newcommand{\dkl}[2]{D_{\text{KL}}(#1\;\Vert\;#2)}

\begin{document}
\section*{Probability}
\subsubsection*{Random Variables}
Variable $X$ that can take on different values $x$ randomly. 

\subsubsection*{Probability Distributions}
Discrete variables: probability mass function \\
Continous variables: probability density \\
Define $p(x) := p(X=x)$

\subsubsection*{Marginal Probability}
\begin{flalign*}
    p(x) = \int p(x,y) dy &&
\end{flalign*}

\subsubsection*{Conditional Probability}
Probability of some event, given that other event has happened:
\begin{flalign*}
    P\c{Y=y}{X = x} = \frac{P(Y=y, X=x)}{P(X = x)} & &
\end{flalign*}
Chain rule:
\begin{flalign*}
    P(x_1, \ldots, x_n) = \prod_{i=1}^n P\c{x_i}{x_1, \ldots, x_{i-1}} &&
\end{flalign*}

\subsubsection*{Independence, Conditional Independence}
Random variables $X, Y$ independent ($X \perp Y$):
\begin{flalign*}
    \forall x \in X, y \in Y : \quad p(X=x, Y=y) = p(X=x)p(Y=y) &&
\end{flalign*}

Conditionally independent ($X\perp Y | Z$):
\begin{flalign*}
    \forall x \in X, y \in Y, z \in Z : \quad p(X=x,Y=y,Z=z) = p\c{X=x}{Z=z} p\c{Y=y}{Z=z} &&
\end{flalign*}

\subsubsection*{Expectation, Variance and Covariance}
Expectation / expected value of function $f(x)$ w.r.t. $P(X)$ is average value that $f$ takes when $x$ drawn from $P$
\begin{flalign*}
    \E[X \sim p]{f(x)} := \int p(x)f(x)dx &&
\end{flalign*}
Variance is measure of how much $f(x)$ vary as different values $x$ from distribution sampled:
\begin{flalign*}
    \var{f(x)} := \E{\br{f(x) - \E{f(x)}}^2} &&
\end{flalign*}
Low variance $\rightarrow$ values of $f(x)$ cluster near expected value \\
Variance = square of standard deviation \\
\\
\\
Covariance gives sense of how two values linearly related to each other:
\begin{flalign*}
    \cov{f(x), g(y)} := \E{\br{f(x)-\E{f(x)}}\br{g(y)-\E{g(y)}}} &&
\end{flalign*}
Covariance matrix of random vector $\v{x} \in \R^n$:
\begin{flalign*}
    \cov{\v{x}}_{i,j} = \cov{x_i, x_j} &&
\end{flalign*}
\\
Identities:
\begin{flalign*}
    & \mu := \E{x} && \\
    & \sigma^2 := \var{x} = \E{\br{x - \E{x}}^2} = \E{x^2} - 2 \, \E{x \, \E{x}} + \E{\E{x}^2} = \E{x^2} - \E{x}^2 &&
\end{flalign*}

\subsubsection*{Probability Distributions}
Normal / Gaussian distribution:
\begin{flalign*}
    &p(x) = \mathcal{N}(x; \mu, \sigma^2) := \sqrt{\frac{1}{2\pi\sigma}} \exp\br{-\frac{1}{2\sigma^2}(x-\mu)^2} &&\\
    \intertext{with properties:}
    &\E[X \sim \mathcal{N}(x; \mu, \sigma^2)]{x} = \mu &&\\
    &\var[x \sim \mathcal{N}(x; \mu, \sigma^2)]{x} = \sigma^2 &&
\end{flalign*}
\\
$n$-variate normal distribution:
\begin{flalign*}
    \mathcal{N}(\v{x}; \v{\mu}, \v{\Sigma}) %
    = \sqrt{\frac{1}{(2\pi)^n \text{det}(\v{\Sigma})}} \exp\br{-\frac{1}{2} (\v{x} - \v{\mu})^T \v{\Sigma}^{-1}(\v{x}- \v{\mu})} &&
\end{flalign*}
\\
\\
Empirical distribution:
\begin{flalign*}
    \hat{p}(\v{x}) = \frac{1}{m} \sum_{i=1}^{m} \delta(\v{x} - \v{x}_i) &&
    \intertext{Puts probability mass $\frac{1}{m}$ on each of the $m$ observed samples $\v{x}_i$}
\end{flalign*}

\subsubsection*{Bayes Rule}
Obtaining $P\c{X}{Y}$ given $P\c{Y}{X}$:
\begin{flalign*}
    &P\c{X}{Y} = \frac{ P(X) P\c{Y}{X} } { P(Y) } &&\\
    \intertext{We can usually compute:}
    &P(y) = \sum_{x} P\c{y}{x} P(x) &&
\end{flalign*}

\subsubsection*{Functions of random variables}
Given $y := g(x)$ invertible and $x \sim p_x(x)$
\begin{flalign*}
    & \int_{a}^{b} p_x(x) dx \overset{!}{=} \int_{g(a)}^{g(b)} p_y(y) dy = \int_{a}^{b} p_y(g(x)) \frac{dg}{dx} dx &&\\ 
    & \implies p_x(x) = p_y(g(x)) \frac{dg}{dx} &&\\
    \intertext{In higher dimensions:}
    & p_{\v{x}} (\v{x}) = p_{\v{y}} (\v{g}(\v{x})) \abs{\det{\frac{\partial \v{g}}{\partial \v{x}}}} &&
\end{flalign*}

\section*{Information Theory}
\subsubsection*{Information content}
Desired properties of information measure:
\begin{itemize}
    \item Information content of event should depend on its likelihood (low/high likelihood $\leftrightarrow$ high/low information)
    \item Independent events should have additive information: e.g. $I(2\times \text{Heads}) \overset{!}{=} 2\cdot I(\text{Heads})$
\end{itemize}

Define self-information of event $X=x$ as:
\begin{flalign*}
    I(X=x) := -\log{p(X=x)} && 
\end{flalign*}

\subsubsection*{Shannon entropy}
Quantification of uncertainty in probability distribution:
\begin{flalign*}
    H(X) := \E[X \sim p]{I(x)} &&
\end{flalign*}
$\rightarrow$ Expectation value of self-information

\subsubsection*{Kullback-Leibler (KL) divergence}
Measure of how different distributions $P(X)$ and $Q(X)$ over same variable $X$ are:
\begin{flalign*}
    \dkl{P}{Q} := \E[X \sim P]{\log{\frac{P(x)}{Q(x)}}} &&
\end{flalign*}
Properties:\\
$\dkl{P}{Q} \geq 0, \ \dkl{P}{Q} = 0 \iff P(X) = Q(X)$ \\ 
$\rightarrow$ concept of distance between distributions, however not symmetric so it makes difference which divergence is minimized \\
Proof: %https://stats.stackexchange.com/a/335201
\begin{flalign*}
    &\forall a > 0: \quad \log{a} \leq a - 1 &&\\
    & -\dkl{p}{q} = \sum_x p(x) \log{\frac{q(x)}{p(x)}} \leq \sum_x p(x) \br{\frac{q(x)}{p(x)} - 1} = \sum_x q(x) - \sum_x p(x) = 1 - 1 = 0 &&\\
    &\iff \dkl{p}{q} \geq 0
    \intertext{Since $\log{a} < a - 1$ for $a\neq 1$:}
    & -\dkl{p}{q} = \sum_x p(x) \br{\frac{q(x)}{p(x)} - 1} = 0 \iff \forall x: \quad p(x) = q(x) &&
\end{flalign*}

\subsubsection*{Cross-Entropy}
\begin{flalign*}
    H(P,Q) = -\E[X\sim p]{\log{Q(x)}} &&
\end{flalign*}
Minimizing cross-entropy with respect to $Q$ equivalent to minimizing KL divergence, since variation of $Q$ does not affect omitted term

\section*{Numerical Computation}
\subsubsection*{Overflow and Underflow}
Discrete representation of real numbers can lead to accumulation of rounding error resulting in algorithm failure \\
Underflow: number near zero rounded to zero $\rightarrow$ problem: zero division, $\log 0$
Overflow: large numbers approximated to $\pm\infty$ $\rightarrow$ problem: NaN in further arithmetic

\subsubsection*{Poor Conditioning}
Conditioning: how rapidly does function change for small variation in inputs $\rightarrow$ possibly large changes due to rounding error

\subsubsection*{Gradent-Based Optimization}
Problem in machine / deep learning: high-dimensional, often many local minima, saddle points surrounded by very flat regions
\begin{flalign*}
    & \text{Jacobian} \ J_{i,j} := \frac{\partial f(\v{x})_i}{\partial x_j} && \\
    &\rightarrow \text{Jacobian of scalar function is transpose of its gradient}&&\\
    & \text{Hessian} \ H_{i,j} := \frac{\partial^2 f(\v{x})}{\partial x_i \partial x_j} \quad &&\\
    &\rightarrow \text{Jacobian of gradient of a scalar function} &&
\end{flalign*}
Interchangeability of partial derivatives $\implies \; \v{H}$ symmetric, can be decomposed in basis of eigenvectors \\
Eigenvalue = second derivative in direction of corresponding eigenvector

\begin{flalign*}
    \v{H} \; \begin{cases*}
        \text{positive definite} &$\implies$ local minimum \\
        \text{negative definite} &$\implies$ local maximum \\
        \text{pos. and neg. eigenvalues} &$\implies$ saddle point \\
        \text{at least one zero eigenvalue} &$\implies$ inconclusive in that direction \\
    \end{cases*} 
\end{flalign*}
Large differences in magnitude of Hessian eigenvalues ($\rightarrow$ curvature) problematic due to overshooting with gradient descent
\begin{flalign*}
    f(\v{x}) \approx f(\v{x}_0) + (\v{x} - \v{x}_0)^T \v{g} + \frac{1}{2} (\v{x} - \v{x}_0)^T \v{H} (\v{x} - \v{x}_0) &&
\end{flalign*}
can be used to estimate optimal step size $\epsilon^*$ for gradient descent step with $\v{x} = \v{x}_0 - \epsilon \v{g}$ \\



Optimization algorithms using only gradent $\rightarrow$ first-order optimization algorithms, using Hessian second-order



\end{document}