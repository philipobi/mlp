\documentclass{article}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{relsize}

\setlength{\parindent}{0pt}

\def\noval{}

\DeclarePairedDelimiter{\br}{(}{)}
\DeclarePairedDelimiter{\brr}{[}{]}
\DeclarePairedDelimiter{\brs}{\{}{\}}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\makeatletter
\let\oldbr\br
\def\br{\@ifstar{\oldbr}{\oldbr*}}
\makeatother

\makeatletter
\let\oldbrr\brr
\def\brr{\@ifstar{\oldbrr}{\oldbrr*}}
\makeatother

\makeatletter
\let\oldbrs\brs
\def\brs{\@ifstar{\oldbrs}{\oldbrs*}}
\makeatother

\makeatletter
\let\oldabs\abs
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
\makeatother

\makeatletter
\let\oldnorm\norm
\def\norm{\@ifstar{\oldnorm}{\oldnorm*}}
\makeatother

\makeatletter
\let\oldlog\log
\def\log{\@ifstar\@log\@@log}
\def\@log#1{\oldlog\br{#1}}
\def\@@log#1{\oldlog#1}
\makeatother


\makeatletter
\newcommand{\set}[2]{
    \def\@delim{\;\@ifstar{\middle|}{:}\;}
    \ifx\noval#2\noval%
        \left\{#1\right\}
    \else
        \left\{#1\@delim#2\right\}
    \fi
}
\makeatother

\newcommand{\E}[2][]{%
    \ifx\noval#1\noval%
        \mathbb{E}\brr{#2}
    \else
        \mathbb{E}_{#1}\brr{#2}
    \fi
}
\newcommand{\var}[2][]{%
    \ifx\noval#1\noval%
        \text{Var}\br{#2}
    \else
        \text{Var}_{#1}\br{#2}
    \fi
}
\newcommand{\cov}[2][]{%
    \ifx\noval#1\noval%
        \text{Cov}\br{#2}
    \else
        \text{Cov}_{#1}\br{#2}
    \fi
}

\newcommand{\est}[1]{\hat{#1}_m}
\newcommand{\bias}[1]{\text{bias}\br{#1}}


\newcommand{\arrow}{$\rightarrow\;$}

\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\text{T}}
\renewcommand{\v}{\bm}
\renewcommand{\c}[2]{\left(#1\;\middle|\;#2\right)}
\renewcommand{\det}[1]{\text{det}\br{#1}}
\newcommand{\ml}[1]{\mathlarger{#1}\,}
\newcommand{\dkl}[2]{D_{\text{KL}}(#1\;\Vert\;#2)}
\renewcommand{\S}{\mathbb{S}}
\newcommand{\func}[3]{#1:#2\rightarrow#3}
\newcommand{\p}[1]{p_\text{#1}}

\begin{document}
\section*{Probability}
\subsubsection*{Random Variables}
Variable $X$ that can take on different values $x$ randomly. 

\subsubsection*{Probability Distributions}
Discrete variables: probability mass function \\
Continous variables: probability density \\
Define $p(x) := p(X=x)$

\subsubsection*{Marginal Probability}
\begin{flalign*}
    p(x) = \int p(x,y) dy &&
\end{flalign*}

\subsubsection*{Conditional Probability}
Probability of some event, given that other event has happened:
\begin{flalign*}
    P\c{Y=y}{X = x} = \frac{P(Y=y, X=x)}{P(X = x)} & &
\end{flalign*}
Chain rule:
\begin{flalign*}
    P(x_1, \ldots, x_n) = \prod_{i=1}^n P\c{x_i}{x_1, \ldots, x_{i-1}} &&
\end{flalign*}

\subsubsection*{Independence, Conditional Independence}
Random variables $X, Y$ independent ($X \perp Y$):
\begin{flalign*}
    \forall x \in X, y \in Y : \quad p(X=x, Y=y) = p(X=x)p(Y=y) &&
\end{flalign*}

Conditionally independent ($X\perp Y | Z$):
\begin{flalign*}
    \forall x \in X, y \in Y, z \in Z : \quad p(X=x,Y=y,Z=z) = p\c{X=x}{Z=z} p\c{Y=y}{Z=z} &&
\end{flalign*}

\subsubsection*{Expectation, Variance and Covariance}
Expectation / expected value of function $f(x)$ w.r.t. $P(X)$ is average value that $f$ takes when $x$ drawn from $P$
\begin{flalign*}
    \E[X \sim p]{f(x)} := \int p(x)f(x)dx &&
\end{flalign*}
Variance is measure of how much $f(x)$ vary as different values $x$ from distribution sampled:
\begin{flalign*}
    \var{f(x)} := \E{\br{f(x) - \E{f(x)}}^2} &&
\end{flalign*}
Low variance $\rightarrow$ values of $f(x)$ cluster near expected value \\
Variance = square of standard deviation \\
\\
\\
Covariance gives sense of how two values linearly related to each other:
\begin{flalign*}
    \cov{f(x), g(y)} := \E{\br{f(x)-\E{f(x)}}\br{g(y)-\E{g(y)}}} &&
\end{flalign*}
Covariance matrix of random vector $\v{x} \in \R^n$:
\begin{flalign*}
    \cov{\v{x}}_{i,j} = \cov{x_i, x_j} &&
\end{flalign*}
\\
Identities:
\begin{flalign*}
    & \mu := \E{x} && \\
    & \sigma^2 := \var{x} = \E{\br{x - \E{x}}^2} = \E{x^2} - 2 \, \E{x \, \E{x}} + \E{\E{x}^2} = \E{x^2} - \E{x}^2 &&
\end{flalign*}

\subsubsection*{Probability Distributions}
Normal / Gaussian distribution:
\begin{flalign*}
    &p(x) = \mathcal{N}(x; \mu, \sigma^2) := \sqrt{\frac{1}{2\pi\sigma}} \exp\br{-\frac{1}{2\sigma^2}(x-\mu)^2} &&\\
    \intertext{with properties:}
    &\E[X \sim \mathcal{N}(x; \mu, \sigma^2)]{x} = \mu &&\\
    &\var[x \sim \mathcal{N}(x; \mu, \sigma^2)]{x} = \sigma^2 &&
\end{flalign*}
\\
$n$-variate normal distribution:
\begin{flalign*}
    \mathcal{N}(\v{x}; \v{\mu}, \v{\Sigma}) %
    = \sqrt{\frac{1}{(2\pi)^n \text{det}(\v{\Sigma})}} \exp\br{-\frac{1}{2} (\v{x} - \v{\mu})^T \v{\Sigma}^{-1}(\v{x}- \v{\mu})} &&
\end{flalign*}
\\
\\
Empirical distribution:
\begin{flalign*}
    \hat{p}(\v{x}) = \frac{1}{m} \sum_{i=1}^{m} \delta(\v{x} - \v{x}_i) &&
    \intertext{Puts probability mass $\frac{1}{m}$ on each of the $m$ observed samples $\v{x}_i$}
\end{flalign*}

\subsubsection*{Bayes Rule}
Obtaining $P\c{X}{Y}$ given $P\c{Y}{X}$:
\begin{flalign*}
    &P\c{X}{Y} = \frac{ P(X) P\c{Y}{X} } { P(Y) } &&\\
    \intertext{We can usually compute:}
    &P(y) = \sum_{x} P\c{y}{x} P(x) &&
\end{flalign*}

\subsubsection*{Functions of random variables}
Given $y := g(x)$ invertible and $x \sim p_x(x)$
\begin{flalign*}
    & \int_{a}^{b} p_x(x) dx \overset{!}{=} \int_{g(a)}^{g(b)} p_y(y) dy = \int_{a}^{b} p_y(g(x)) \frac{dg}{dx} dx &&\\ 
    & \implies p_x(x) = p_y(g(x)) \frac{dg}{dx} &&\\
    \intertext{In higher dimensions:}
    & p_{\v{x}} (\v{x}) = p_{\v{y}} (\v{g}(\v{x})) \abs{\det{\frac{\partial \v{g}}{\partial \v{x}}}} &&
\end{flalign*}

\section*{Information Theory}
\subsubsection*{Information content}
Desired properties of information measure:
\begin{itemize}
    \item Information content of event should depend on its likelihood (low/high likelihood $\leftrightarrow$ high/low information)
    \item Independent events should have additive information: e.g. $I(2\times \text{Heads}) \overset{!}{=} 2\cdot I(\text{Heads})$
\end{itemize}

Define self-information of event $X=x$ as:
\begin{flalign*}
    I(X=x) := -\log{p(X=x)} && 
\end{flalign*}

\subsubsection*{Shannon entropy}
Quantification of uncertainty in probability distribution:
\begin{flalign*}
    H(X) := \E[X \sim p]{I(x)} &&
\end{flalign*}
$\rightarrow$ Expectation value of self-information

\subsubsection*{Kullback-Leibler (KL) divergence}
Measure of how different distributions $P(X)$ and $Q(X)$ over same variable $X$ are:
\begin{flalign*}
    \dkl{P}{Q} := \E[X \sim P]{\log{\frac{P(x)}{Q(x)}}} &&
\end{flalign*}
Properties:\\
$\dkl{P}{Q} \geq 0, \ \dkl{P}{Q} = 0 \iff P(X) = Q(X)$ \\ 
$\rightarrow$ concept of distance between distributions, however not symmetric so it makes difference which divergence is minimized \\
Proof: %https://stats.stackexchange.com/a/335201
\begin{flalign*}
    &\forall a > 0: \quad \log{a} \leq a - 1 &&\\
    & -\dkl{p}{q} = \sum_x p(x) \log{\frac{q(x)}{p(x)}} \leq \sum_x p(x) \br{\frac{q(x)}{p(x)} - 1} = \sum_x q(x) - \sum_x p(x) = 1 - 1 = 0 &&\\
    &\iff \dkl{p}{q} \geq 0
    \intertext{Since $\log{a} < a - 1$ for $a\neq 1$:}
    & -\dkl{p}{q} = \sum_x p(x) \br{\frac{q(x)}{p(x)} - 1} = 0 \iff \forall x: \quad p(x) = q(x) &&
\end{flalign*}

\subsubsection*{Cross-Entropy}
\begin{flalign*}
    H(P,Q) = -\E[X\sim p]{\log{Q(x)}} &&
\end{flalign*}
Minimizing cross-entropy with respect to $Q$ equivalent to minimizing KL divergence, since variation of $Q$ does not affect omitted term

\section*{Numerical Computation}
\subsubsection*{Overflow and Underflow}
Discrete representation of real numbers can lead to accumulation of rounding error resulting in algorithm failure \\
Underflow: number near zero rounded to zero $\rightarrow$ problem: zero division, $\log 0$
Overflow: large numbers approximated to $\pm\infty$ $\rightarrow$ problem: NaN in further arithmetic

\subsubsection*{Poor Conditioning}
Conditioning: how rapidly does function change for small variation in inputs $\rightarrow$ possibly large changes due to rounding error

\subsubsection*{Gradent-Based Optimization}
Problem in machine / deep learning: \\
functions high-dimensional, often many local minima, saddle points surrounded by very flat regions \\
difficult to limit function space used
\begin{flalign*}
    & \text{Jacobian} \ J_{i,j} := \frac{\partial f(\v{x})_i}{\partial x_j} && \\
    &\rightarrow \text{Jacobian of scalar function is transpose of its gradient}&&\\
    & \text{Hessian} \ H_{i,j} := \frac{\partial^2 f(\v{x})}{\partial x_i \partial x_j} \quad &&\\
    &\rightarrow \text{Jacobian of gradient of a scalar function} &&
\end{flalign*}
Interchangeability of partial derivatives $\implies \; \v{H}$ symmetric, can be decomposed in basis of eigenvectors \\
Eigenvalue = second derivative in direction of corresponding eigenvector

\begin{flalign*}
    \v{H} \; \begin{cases*}
        \text{positive definite} &$\implies$ local minimum \\
        \text{negative definite} &$\implies$ local maximum \\
        \text{pos. and neg. eigenvalues} &$\implies$ saddle point \\
        \text{at least one zero eigenvalue} &$\implies$ inconclusive in that direction \\
    \end{cases*} 
\end{flalign*}
Large differences in magnitude of Hessian eigenvalues ($\rightarrow$ curvature) problematic due to overshooting with gradient descent
\begin{flalign*}
    f(\v{x}) \approx f(\v{x}_0) + (\v{x} - \v{x}_0)^T \v{g} + \frac{1}{2} (\v{x} - \v{x}_0)^T \v{H} (\v{x} - \v{x}_0) &&
\end{flalign*}
can be used to estimate optimal step size $\epsilon^*$ for gradient descent step with $\v{x} = \v{x}_0 - \epsilon \v{g}$ \\
\\
Optimization algorithms using only gradient $\rightarrow$ first-order optimization algorithms, using Hessian second-order \\
Convex optimization: most successful field of specialized optimization, problems in deep learning difficult to formulate as convex optimization, if at all only used in subroutines

\subsubsection*{Constrained Optimization}
Limit set of domain of optimization to set of feasible points $\S$ e.g. norm constraint\\
Approaches:
\begin{itemize}
    \item modify gradient descent taking constraint into account (e.g. projection into $\S$)
    \item design unconstrained problem whose solution can be converted into solution of original, e.g. for $\norm{\v{x}} \overset{!}{=} 1$ optimize $g(\theta) = f([\cos(\theta), \sin(\theta)]^T)$
    \item Karush-Kuhn-Tucker (KKT) approach:
    \begin{flalign*}
       & \S := \set{\v{x}}{\forall i, g_i(\v{x})=0 \ \text{and} \ \forall j, h_j(\v{x}) \leq 0} &&\\
       & L(\v{x}, \v{\lambda}. \v{\alpha}) := f(\v{x}) + \sum_{i} \lambda_i g_i(\v{x}) + \sum_j \alpha_j h_j(\v{x}) \qquad \text{(Generalized Lagrangian)} &&\\
       & \text{Constrained minimization $\rightarrow$ unconstrained optimization of $L$:} \ \min_{\v{x}}  \max_{\v{\lambda}} \max_{\v{\alpha}, \alpha_j \geq 0} L(\v{x}, \v{\lambda}, \v{\alpha}) &&\\
       & \text{Constraint violated} \implies \max_{\v{\lambda}} \max_{\v{\alpha}, \alpha_j \geq 0} L(\v{x}, \v{\lambda}, \v{\alpha}) = \infty &&\\
       & \text{Constraints satisfied} \implies \max_{\v{\lambda}} \max_{\v{\alpha}, \alpha_j \geq 0} L(\v{x}, \v{\lambda}, \v{\alpha}) = f(\v{x}) \ \text{with} \ \forall j:  h_j(\v{x}) = 0 \ \text{or} \ \alpha_j = 0&& 
    \end{flalign*}
\end{itemize}
\section*{Machine Learning Basics}
\subsubsection*{Definitions}

Broad definition of machine learning: "computer program is said to learn from experience [...] with respect to some class of tasks $T$ if its performance at tasks in $T$ [...] improves with experience" \\
\\
Form of representation of object is very important \\
Pieces of information included in representation of object known as features \\
e.g. Image - Representation: Array of Pixels, Features: Individual pixel values \\
General machine learning: discover mapping from representation to output \\
Representation learning: also learn the "optimal" representation of data \\
Example of representation learning: Autoencoders \\
Encoder function: converts data to different representation \\
Decoder function: converts new representation back to original format \\
Task: preserve as much information, while providing additional nice properties e.g. reduced size in new representation \\
Deep learning tackles representation learning by introducing representations that are expressed in terms of other, simpler representations \\
e.g. Image: Pixels \arrow Edges \arrow Contours \arrow Object parts \arrow Object identity \\
\\
Tasks usually described in terms of how system should process an example \\
Example: collection (usually vector) of features that has been measured, representing object that system should process \\
\\

\subsubsection*{Common tasks}
\begin{itemize}
    \item Classification of $k$ categories: produce function $\func{f}{\R^n}{R^k}$ \\
    Output: probability vector $\norm{\v{y}} = 1$ (can also be one-hot)\\
    Variation: missing inputs in $\v{x}$
    \item Regression: predict numerical value given some input: $\func{f}{\R^n}{R^m}$
    \item Structured output: output is vector with important relationships between elements e.g. output 
    token codes in language processing
    \item Probability density estimation: explicitly learn function $\func{\p{model}}{\R^n}{\R}$ on space examples were drawn from \\
    most above tasks already require implicit capture of structure of probability distribution
\end{itemize}

\subsubsection*{Types of experience / learning}
\begin{itemize}
    \item Unsupervised: experience dataset, learning useful properties of dataset structure \\
    e.g. probability density $p(\v{x})$ or clustering dataset
    \item Supervised: experience dataset of labeled examples $\br{\v{x}, \v{y}}$, learn to estimate $p\c{\v{y}}{\v{x}}$
    \item Semi-supervised: some examples have label/target, some not
    \item Multi-instance: collection of examples labeled but not each example
    \item Reinforcement: interaction with environment, feedback loop between system and experiences
\end{itemize}
Not formally defined terms, for example:
\begin{flalign*}
    &\text{Unsupervised problem can be split into supervised learning problems:} \ p(\v{x}) = \prod_{i} p\c{x_i}{x_1, \ldots, x_{i-1}} &&\\
    &\text{Supervised learning of $p\c{\v{y}}{\v{x}}$ can be replaced by unsupervised learning of $p(\v{x}, \v{y})$ and using:} &&\\
    & p\c{\v{y}}{\v{x}} = \frac{p(\v{x}, \v{y})}{\sum_{\v{y}} p(\v{x}, \v{y})}&&
\end{flalign*}

\subsubsection*{Capacity, Overfitting and Underfitting}
Generalization: capacity to perform well on previously unseen inputs \\
During learning: reduce training error \arrow error on training set \\
Difference between machine learning and simple optimization:
goal is also low generalization error \arrow expected error on new input \\
Estimate generalization error with error on test set, set of examples not used for training \\
\\
to be able to estimate performance on test set without observing it make assumptions about data-generating process - i.i.d. assumptions:
\begin{itemize}
    \item examples independent from each other
    \item training set and test set identically distributed
\end{itemize}
\arrow data generation can be described by distribution over single example $\p{data}(\v{x})$ \\
\\
Terminology:\\
Capacity: ability of model to fit function to be learned \\
Underfitting: model cannot obtain low enough training error, usually capacity too low \\
Overfitting: gap between training and generalization error too large, can be caused by too high capacity for task
\arrow more parameters than examples or "memorization" of training data \\
Best performance with capacity appropriate for true complexity of task and amount of training data \\
Learning theory: upper bound for discrepancy between training and generalization error grows with capacity, shrinks with number of training examples \\
\\
Even ideal model knowing true probability distribution will have some error due to
\begin{itemize}
    \item Noise in distribution
    \item mapping $\v{x} \rightarrow \v{y}$ may be stochastic (supervised learning)
    \item $\v{y}$ may depend on other variables not included in $\v{x}$
\end{itemize}
\arrow Bayes error

\subsubsection*{No Free Lunch Theorem}
averaged over all possible data-generating distributions every classification algorithm has same error rate when classifying unobserved data \\
\arrow no algorithm is universally any better than any other \\
\arrow assumptions about data-generating distribution for specific task \arrow design learning algorithm that performs well on assumed distribution

\subsubsection*{Hyperparameters and Validation Sets}
Parameters that control algorithms behavior, which are not adapted by learning algorithm itself \\
e.g. not appropriate to learn parameter on training set (e.g. model capacity, since algorithm would always overfit) \\
\arrow Validation set separate from training and test set \\
Reduce generalization error on validation set after training by optimizing hyperparameters \\
Option to use all examples by cross-validation, e.g.: $k$-fold cross-validation:
\begin{itemize}
    \item split dataset into $k$ nonoverlapping subsets
    \item on iteration $i$ use set $i$ as test set, rest as training set
    \item average test error and training result across iterations
\end{itemize}

\subsubsection*{Estimators, Bias and Variance}
\begin{flalign*}
    & \text{Given i.i.d. data points} \set{x_1, \ldots, x_m}{} &&\\
    & \text{Point estimator of parameter $\theta$:} \ \est{\theta} = g(x_1, \ldots, x_m) &&\\
    & \rightarrow \ \text{Function estimator $\hat{f}$ = Point estimator in function space}&&\\
    & \rightarrow \ \text{Estimators are functions over the random variables $x_i$} &&\\
    & \text{Bias:} \ \bias{\est{\theta}} = \E[x\sim \p{data}]{\est{\theta}} - \theta &&\\
    & \text{Variance:} \ \var{\est{\theta}} = \var[x_i\sim \p{data}]{\est{\theta}} &&\\
    & \text{Mean Sqared Error:} \ \text{MSE}\left(\est{\theta}\right) = \E{\br{\est{\theta} - \theta}^2} = \bias{\est{\theta}}^2 + \var{\est{\theta}} &&\\
    & \textbf{Common estimators}: &&\\
    & \text{sample mean:} \ \est{\mu} := \frac{1}{m} \sum_{i=1}^{m} x_i &&\\
    & \text{sample variance:} \ \est{\sigma}^2 := \frac{1}{m} \sum_{i=1}^{m} \br{x_i - \est{\mu}}^2 &&\\
    & \text{unbiased sample variance:} \ \tilde{\sigma}^2_m := \frac{1}{m-1} \sum_{i=1}^{m} \br{x_i - \est{\mu}}^2 &&\\
\end{flalign*}

Bias measures expected deviation of estimator from true value \\
Variance measures deviation from estimator value that sampling is likely to cause \\
MSE measure overall expected deviation between estimator and true value \\
When bias and variance meaningful components of generalization error, increasing capacity tends to increase variance and decrease bias

\begin{flalign*}
    \text{Consistency} \ :\iff \text{plim}_{m \to \infty} \est{\theta} = \theta \ \iff  P\br{\abs{\est{\theta}-\theta} > \epsilon} \to 0 \ \text{as} \ m \to \infty &&
\end{flalign*}

\subsubsection*{Maximum Likelihood Estimation}
Given set of $m$ examples ${x_1, \ldots, x_m}$ drawn from data-generating distribution $\p{data}(x)$ \\
$\p{model}(x; \theta)$ family of probability distributions over same space as $\p{data}(x)$, indexed by $\theta$ \\
Maximum likelihood estimator for $\theta$ defined as:
\begin{flalign*}
    & \theta_{ML} := \argmax_\theta \p{model}(x_i; \theta) = \argmax_\theta \prod_{i=1}^{m} \p{model}(x_i; \theta) &&\\
    \intertext{Redefine using monotonicity of log to avoid inconvenient product (e.g. underflow) and rescale with $1/m$:}
    &\theta_{ML} := \argmax_\theta \frac{1}{m} \sum_{i=1}^{m} \log{\p{model}(x_i; \theta)} 
    = \argmax_\theta \mathlarger{\int}\br{\frac{1}{m}\sum_{i=1}^{m} \delta\br{x - x_i}} \log{\p{model}(x; \theta)} \, dx
    &&\\
    & = \argmax_\theta \E[x\sim \hat{p}_\text{data}]{\log{\p{model}(x; \theta)}} = \argmin_\theta \E[x\sim \hat{p}_\text{data}]{-\log{\p{model}(x; \theta)}} &&\\
    & \dkl{\hat{p}_\text{data}}{\p{model}} = \E[\hat{p}_\text{data}]{\log{\hat{p}_\text{data}}} - \E[\hat{p}_\text{data}]{\log{\p{model}}} &&
\end{flalign*}
$\implies \, \theta_{ML}$ minimizes dissimilarity between empirical distribution of training set $\hat{p}_\text{data}$ and model distribution $\p{model}$ by 
minimization of KL divergence which corresponds to minimizing cross-entropy between distributions

\subsubsection*{Conditional Log-Likelihood}
Generalization of ML to learn conditional probability $\p{model}\c{y}{x; \theta}$ to predict $y$ given $x$ and i.i.d labeled examples $\br{y_i, x_i}$:
\begin{flalign*}
    \theta_{ML} = \argmax_\theta \prod_{i} \p{model}\c{y_i}{x_i; \theta} \rightarrow \theta_{ML} = \argmax_\theta \sum_{i} \log{\p{model}\c{y_i}{x_i; \theta}}
\end{flalign*}
\arrow $x_i$ are parameters in probability distribution, however not optimized since given in form of examples

\subsubsection*{Properties of Maximum Likelihood}
convergence (consistency) under conditions:
\begin{itemize}
    \item true distribution $\p{data}$ lies within model family $\p{model}(\cdot;\theta)$ \\
    Otherwise no estimator can recover $\p{data}$
    \item true distribution $\p{data}$ must correspond to exactly one value of $\theta$
\end{itemize}
It can be shown that for large $m$ no consistent estimator has lower MSE than ML estimator \\
\arrow preferred estimator to use for machine learning


\subsubsection*{Support Vector Machines}
%https://en.wikipedia.org/wiki/Support_vector_machine
%% TODO
goal: construct optimal hyperplane in high- or infinite-dimensional space, useful e.g. for classification \\
classes often not linearly separable (i.e. by a plane) in original space \\
solution \arrow kernel trick: map original space to much higher dimensional space, making solution easier in that space \\
to optimize computation ensure that dot products of input data can be computed in terms of variables in original space \\
define dot products in terms of kernel function $k(x,y)$

\subsubsection*{Stochastic Gradient Descent}
Extension of gradent descent, powers nearly all of deep learning \\
Cost function often sum over training examples of per-example loss function \\
\arrow Computational cost of gradient descent step is $O(m)$ \\
SGD: gradient is itself an estimator, may be approximated using small set of samples ("minibatch") with fixed size $m^\prime$ \\
Number of steps required to reach convergence usually increases with $m$, as $m \to \infty$ model will converge to best possible error before SGD has sampled every example
\arrow Computational cost of SGD step is $O(1)$ as function of $m$

\subsection*{Components of Machine Learning Algorithm}
\begin{itemize}
    \item specification of dataset
    \item cost function $J$, most commonly negative log-likelihood (NLL) \\
    \arrow minimizing causes maximum likelihood estimation
    \item optimization procedure, often SGD
    \item model specification $\p{model}$
\end{itemize}

\subsubsection*{Challenges Motivating Deep Learning}
\begin{itemize}
    \item Curse of Dimensionality: number of possible configurations gets much larger than number of examples, 
    many traditional machine learning algorithms assume output should be approximately that of nearest training point \\
    \arrow implicit assumption of smoothness $f(x) \approx f(x + \epsilon)$
    \item Deep learning introduces additional (explicit and implicit) assumptions (priors) which can help to reduce generalization error
    \item Manifold learning: assumption: most of $\R^n$ invalid inputs, interesting inputs / outputs only occur on collection of manifolds \\
    Arguments for assumption in context of image, sound and text processing tasks:
    \begin{itemize}
        \item Probability distribution is highly concentrated, uniform noise essentially never resembles structured input
        \item Similar examples connected to each other by applying transformations to traverse manifold (informal example of transformation in image space: 
        gradually dim/brighten lights, rotate objects etc.)
        \item multiple manifolds likely involved (e.g. manifold of human face imges, cat face images)
    \end{itemize}
\end{itemize}

\section*{Deep Feedforward Networks}
\subsubsection*{Definitions}
also called multilayer perceptrons (MLP) \\
quentessential deep learning model \\
goal: learn best value of parameters $\theta$ such that $f(x; \theta)$ is best approximation of some function $f^*(x)$ \\
no feedback connections where ouptut fed back into model (recurrent neural networks) \\
network aspect: can be described by directed acycylic graph of function compositions \\
most commonly chain $f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$ where $f^{(i)}$ called $i$-th layer of network \\
length of chain \arrow depth of model \arrow origin of term "deep learning" \\
examples $(y_i, x_i)$ where $f^*(x_i) \simeq y_i$ specify what output $f(x_i; \theta)$ should produce \\
training data does not show desired output for intermediate layers \arrow "hidden layers" \\
hidden layers typically vector valued, dimensionality determines width of model \\
"neural" networks because architecture loosely inpired by neuroscience \\
vector elements play role analogous to neuron, can be seen as unit that receives input from many 
other units and computes own activation value \\
choice of function used for computation of activation value also loosely guided by biological neurons \\
goal is not to model brain, modern research guided by mathematical and engineering disciplines \\
feedforward networks: "function approximation machines designed to achieve statistical generalization, ocassionally drawing
some insight from what we know about the brain , rather than models of brain function" %p.164

%maybe: derivation from kernel trick
\subsubsection*{Gradient-Based Learning}
Nonlinearity of function useful property, otherwise e.g. XOR cannot be learned \\
causes most loss function to become nonconvex \arrow training using iterative, gradient-based optimizers 
that drive cost function to low value \arrow no convergence guarantees, sensitive to initial parameters \\
forps MLPs: initialize weights to small random values, biases to zero or small positive values \\
training algorithm almost always based on sgd or improvements of sgd


\subsubsection*{Cost Functions}
in most cases parametric model defines $p\c{y}{x; \theta}$ and use principle of maximum likelihood \\
\arrow use cross-entropy/NLL between training data and model predictions as cost function:
\begin{flalign*}
    J(\theta) = - \E[x,y\sim \hat{p_\text{data}}]{\log{\p{model}\c{y}{x}}} &&
\end{flalign*}
\arrow specifying model $p\c{y}{x}$ automatically determines cost function $\log{p\c{y}{x}}$ \\
gradient of cost function must be large and predictable enough to serve as good guide for learning algorithm \\
problematic for activation functions that saturate (become very flat)

%ADAM: exponentially weighted average https://stats.stackexchange.com/a/286644

\end{document}